<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>laueimproc.nn.vae_spot_classifier API documentation</title>
<meta name="description" content="Classifier of laue spots using variational convolutive auto-encoder." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}#lunr-search{width:100%;font-size:1em;padding:6px 9px 5px 9px;border:1px solid silver}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>laueimproc.nn.vae_spot_classifier</code></h1>
</header>
<section id="section-intro">
<p>Classifier of laue spots using variational convolutive auto-encoder.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

&#34;&#34;&#34;Classifier of laue spots using variational convolutive auto-encoder.&#34;&#34;&#34;

import math
import numbers
import typing
import warnings

# from tqdm.autonotebook import tqdm
import torch


def _test_size(size: int) -&gt; bool:
    &#34;&#34;&#34;Return True if the size is ok.&#34;&#34;&#34;
    if 3 &lt;= size &lt;= 32:
        return True
    size, rest = divmod(size, 2)
    if rest:
        return False
    return _test_size(size)


class VAESpotClassifier(torch.nn.Module):
    &#34;&#34;&#34;A partialy convolutive variationel auto encoder used for unsupervised spot classification.

    Attributes
    ----------
    decoder : Decoder
        The decoder part, able to random draw and reconstitue an image from the encoder.
    device: torch.device
        The device of the model.
    encoder : Encoder
        The encoder part, able to transform an image into au gaussian law.
    latent_dim : int
        The dimension of the latent space.
    shape : tuple[int, int]
        The shape of the rois.
    space : float, default = 3.0
        The non penalized spreading area half size.
    &#34;&#34;&#34;

    def __init__(
        self,
        shape: typing.Container[numbers.Integral],
        latent_dim: numbers.Integral = 2,
        space: numbers.Real = 3.0,
        **kwargs,
    ):
        &#34;&#34;&#34;Initialise the model.

        Parameters
        ----------
        shape : tuple[int, int]
            Transmitted to ``laueimproc.nn.vae_spot_classifier.Encoder``
            and ``laueimproc.nn.vae_spot_classifier.Decoder``.
        latent_dim : int, default=2
            Transmitted to ``laueimproc.nn.vae_spot_classifier.Encoder``
            and ``laueimproc.nn.vae_spot_classifier.Decoder``.
        space : float, default=3.0
            The non penalized spreading area in the latent space.
            All the points with abs(p) &lt;= space are autorized.
            A small value condensate all the data, very continuous space but hard to split.
            In a other way, a large space split the clusters but the values betwean the clusters
            are not well defined.
        intensity_sensitive : boolean, default=True
            If set to False, the model will not consider the spots intensity,
            as they will be normalized to have a power of 1.
        scale_sensitive : boolean = True
            If set to False, the model will not consider the spots size,
            as they will be resized and reinterpolated to a constant shape.
        &#34;&#34;&#34;
        assert hasattr(shape, &#34;__iter__&#34;), shape.__class__.__name__
        shape = tuple(shape)
        assert len(shape) == 2, shape
        assert isinstance(shape[0], numbers.Integral) and isinstance(shape[1], numbers.Integral), \
            shape
        shape = (int(shape[0]), int(shape[1]))
        assert isinstance(latent_dim, numbers.Integral), latent_dim.__class__.__name__
        assert latent_dim &gt;= 1, latent_dim
        latent_dim = int(latent_dim)
        assert isinstance(space, numbers.Real), space.__class__.__name__
        assert space &gt; 0, space
        space = float(space)
        self._latent_dim = latent_dim
        self._shape = shape
        self._space = space
        self._sensitivity = {}
        self._sensitivity[&#34;intensity&#34;] = kwargs.get(&#34;intensity_sensitive&#34;, True)
        assert isinstance(self._sensitivity[&#34;intensity&#34;], bool), self._sensitivity[&#34;intensity&#34;]
        self._sensitivity[&#34;scale&#34;] = kwargs.get(&#34;scale_sensitive&#34;, True)
        assert isinstance(self._sensitivity[&#34;scale&#34;], bool), self._sensitivity[&#34;scale&#34;]

        self._normalization = None

        super().__init__()
        self.encoder = Encoder(self)
        self.decoder = Decoder(self)

    def dataaug(self, image: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply all the data augmentations on the image.

        Parameters
        ----------
        image : torch.Tensor
            The image of shape (h, w).

        Returns
        -------
        aug_batch : torch.Tensor
            The augmented stack of images of shape (n, 1, h&#39;, w&#39;).
        &#34;&#34;&#34;
        assert isinstance(image, torch.Tensor), image.__class__.__name__
        assert image.ndim == 2 and image.shape &gt;= (1, 1), image.shape

        # reshape or pad/crop
        if self._sensitivity[&#34;scale&#34;]:
            from laueimproc.nn.dataaug.scale import rescale
            image = rescale(image, self.shape, copy=False)
        else:
            from laueimproc.nn.dataaug.patch import patch
            image = patch(image, self.shape, copy=False)

        # intensity normalization
        if not self._sensitivity[&#34;intensity&#34;]:
            energy = torch.sqrt(torch.sum(image * image))
            image = image / (energy + torch.finfo(image.dtype).eps)

        return image

    @property
    def device(self) -&gt; torch.device:
        &#34;&#34;&#34;Return the device of the model.&#34;&#34;&#34;
        return next(self.parameters()).device

    def forward(self, data) -&gt; torch.Tensor:
        &#34;&#34;&#34;Encode, random draw and decode the image.

        Parameters
        ----------
        data : laueimproc.classes.base_diagram.BaseDiagram or torch.Tensor
            If a digram is provided, the spots are extract, data augmentation are applied,
            and the mean projection in the latent space is returned.
            If the input is a tensor, data augmentation are not applied.
            Return the autoencoded data, after having decoded a random latent space draw.

        Returns
        -------
        torch.Tensor
            The mean latent vector of shape (n, latent_dim) if the input is a Diagram.
            The generated image of shape (n, height, width) otherwise.
        &#34;&#34;&#34;
        from laueimproc.classes.base_diagram import BaseDiagram
        assert isinstance(data, (torch.Tensor, BaseDiagram)), data.__class__.__name__

        # case tensor
        if isinstance(data, torch.Tensor):
            if data.ndim == 2:
                return self.forward(data.unsqueeze(0)).squeeze(0)
            assert data.ndim == 3, data.shape
            mean, std = self.encoder(data.unsqueeze(1))
            sample = self.decoder.parametrize(mean, std) if self.training else mean
            generated_image = self.decoder(sample)
            return generated_image.squeeze(1)

        # case diagram
        batch = torch.empty((len(data), *self.shape), dtype=torch.float32, device=self.device)
        for i, (roi, (height, width)) in enumerate(zip(data.rois, data.bboxes[:, 2:].tolist())):
            batch[i] = self.dataaug(roi[:height, :width])
        mean, _ = self.encoder(batch.unsqueeze(1))
        return mean

    @property
    def latent_dim(self) -&gt; int:
        &#34;&#34;&#34;Return the dimension of the latent space.&#34;&#34;&#34;
        return self._latent_dim

    def loss(self, batch: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Forward the data and compute the loss values.

        Parameters
        ----------
        batch : torch.Tensor
            The image stack of shape (n, h, w).

        Returns
        -------
        mse_loss : torch.Tensor
            The sum of the mean square error loss for each image in the batch, shape (1,).
        kld_loss : torch.Tensor
            Pretty close to the sum of the Kullback-Leibler divergence
            for each projection in the batch, shape (1,).
            It is not litteraly the Kullback-Leibler divergence because the peanality for the mean
            is less strict, the cost is 0 in the [-space, space] interval.
            The cost is minimum when var=1 and -space&lt;=mean&lt;=space.

        Notes
        -----
        * No verifications are performed for performance reason.
        * The reduction is sum and not mean because it ables to split the batch in several slices.
        &#34;&#34;&#34;
        assert isinstance(batch, torch.Tensor), batch.__class__.__name__
        assert batch.ndim == 3, batch.shape

        mean, std = self.encoder(batch.unsqueeze(1))
        if std is not None:
            var = std * std
            # real kld = sum(var - 1 - torch.log(var) + mean**2) / 2
            kld = torch.sum(
                var - 1 - torch.log(var)
                # + torch.nn.functional.relu(torch.abs(mean)-self.space)  # 0 because tanh(mean)
            )
            sample = self.decoder.parametrize(mean, std)
        else:
            kld = None
            sample = mean
        generated_batch = self.decoder(sample).squeeze(1)
        _, norm_std = self.normalization
        mse = torch.sum(torch.mean(((batch - generated_batch)/norm_std)**2, dim=(1, 2)))
        return (mse, kld)

    @property
    def normalization(self) -&gt; tuple[float, float]:
        &#34;&#34;&#34;Return the mean and the std of all the training data.&#34;&#34;&#34;
        if self._normalization is None:
            warnings.warn(&#34;call `scan_data` for allowing data normalization&#34;, RuntimeWarning)
            return 0.0, 1.0
        return self._normalization

    def plot_autoencode(self, axe_input, axe_output, spots: torch.Tensor):
        &#34;&#34;&#34;Encode and decode the images, plot the initial and regenerated images.

        Parameters
        ----------
        axe_input : matplotlib.axes.Axes
            The 2d empty axe ready to be filled by the input mosaic.
        axe_output : matplotlib.axes.Axes
            The 2d empty axe ready to be filled by the generated mosaic.
        spots : torch.Tensor
            The image stack of shape (n, h, w).
        &#34;&#34;&#34;
        from matplotlib.axes import Axes
        assert isinstance(axe_input, Axes), axe_input.__class__.__name__
        assert isinstance(axe_output, Axes), axe_output.__class__.__name__
        assert isinstance(spots, torch.Tensor), spots.__class__.__name__
        assert spots.ndim == 3 and spots.shape[-2:] == self.shape, spots.shape

        # find the grid dimension
        height = round(math.sqrt(len(spots)*self.shape[1]/self.shape[0]))
        width = round(math.sqrt(len(spots)*self.shape[0]/self.shape[1]))
        while height * width &lt; len(spots):
            if (height + 1) * width == len(spots):
                height += 1
            elif height * (width + 1) == len(spots):
                width += 1
            elif height &lt; width:  # in favor of square rather than rectangle
                height += 1
            else:
                width += 1

        # input mosaic
        mosaic_in = torch.empty(
            (height*self.shape[0], width*self.shape[1]),
            dtype=spots.dtype,
            device=spots.device,
        )
        for i in range(height):
            for j in range(width):
                idx = j + i*width
                pict = spots[idx] if idx &lt; len(spots) else 0
                mosaic_in[
                    i*self.shape[0]:(i+1)*self.shape[0], j*self.shape[1]:(j+1)*self.shape[1]
                ] = pict

        # output mosaic
        spots = self.forward(spots)
        mosaic_out = torch.empty(
            (height*self.shape[0], width*self.shape[1]),
            dtype=spots.dtype,
            device=spots.device,
        )
        for i in range(height):
            for j in range(width):
                idx = j + i*width
                pict = spots[idx] if idx &lt; len(spots) else 0
                mosaic_out[
                    i*self.shape[0]:(i+1)*self.shape[0], j*self.shape[1]:(j+1)*self.shape[1]
                ] = pict

        # plot
        vmin = min(float(mosaic_in.min()), float(mosaic_out.min()))
        vmax = max(float(mosaic_in.max()), float(mosaic_out.max()))
        axe_input.set_title(&#34;input images&#34;)
        axe_input.axis(&#34;off&#34;)
        axe_input.imshow(
            mosaic_in.numpy(force=True),
            aspect=&#34;equal&#34;,
            interpolation=None,  # antialiasing is True
            cmap=&#34;gray&#34;,
            vmin=vmin,
            vmax=vmax,
        )
        axe_output.set_title(&#34;output images&#34;)
        axe_output.axis(&#34;off&#34;)
        axe_output.imshow(
            mosaic_out.numpy(force=True),
            aspect=&#34;equal&#34;,
            interpolation=None,  # antialiasing is True
            cmap=&#34;gray&#34;,
            vmin=vmin,
            vmax=vmax,
        )

    def scan_data(self, spots_generator: torch.Tensor):
        &#34;&#34;&#34;Complete the data histogram to standardise data (centered and reduction).

        Parameters
        ----------
        spots_generator : iterable
            A generator of spot batch, each item has to be of shape (:, h, w).
        &#34;&#34;&#34;
        assert hasattr(spots_generator, &#34;__iter__&#34;), spots_generator.__class__.__name__

        bins = 10000
        tot_hist = torch.zeros(bins, dtype=torch.float64)
        # for batch in tqdm(spots_generator, desc=&#34;scan data repartition&#34;):
        for batch in spots_generator:
            assert isinstance(batch, torch.Tensor), batch.__class__.__name__
            assert batch.ndim == 3 and batch.shape[-2:] == self.shape, batch.shape
            hist, _ = torch.histogram(batch.to(torch.float64), bins=bins, range=(0.0, 1.0))
            tot_hist = tot_hist.to(hist.device)
            tot_hist += hist

        values = torch.linspace(1.0/(2*bins), 1.0 - 1.0/(2*bins), bins)
        mean = float((tot_hist * values).sum() / tot_hist.sum())
        std = float(torch.sqrt((tot_hist * (values - mean)**2).sum() / tot_hist.sum()))
        self._normalization = (mean, std)

    @property
    def shape(self) -&gt; tuple[int, int]:
        &#34;&#34;&#34;Return the shape of the images.&#34;&#34;&#34;
        return self._shape

    @property
    def space(self) -&gt; float:
        &#34;&#34;&#34;Return the non penalized spreading area half size.&#34;&#34;&#34;
        return self._space


class Decoder(torch.nn.Module):
    &#34;&#34;&#34;Decode the latent sample into a new image.

    Attributes
    ----------
    parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
        The main full auto encoder, containing this module.
    &#34;&#34;&#34;

    def __init__(self, parent: VAESpotClassifier):
        &#34;&#34;&#34;Initialise the decoder.

        Parameters
        ----------
        parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
            The main module.
        &#34;&#34;&#34;
        assert isinstance(parent, VAESpotClassifier), parent.__class__.__name__
        self._parent = (parent,)  # pack into tuple solve `cannot assign module before...`
        super().__init__()

        # size augmentation layers
        self.augmentation_layers = torch.nn.ModuleList()
        shape = parent.shape
        while min(shape) &gt; 32:
            self.augmentation_layers.append(
                torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(12, 12, kernel_size=4, stride=2, padding=1),
                    torch.nn.Dropout(0.1),
                    torch.nn.LeakyReLU(inplace=True),
                    # torch.nn.MaxUnpool2d(2, stride=2),
                )
            )
            shape = (shape[0]//2, shape[1]//2)

        area = shape[0] * shape[1]
        self.dense_layers = torch.nn.Sequential(
            torch.nn.Linear(parent.latent_dim, 1000),
            # torch.nn.BatchNorm1d(1000),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.2),

            # torch.nn.Linear(500, 1000),
            # # torch.nn.BatchNorm1d(1000),
            # torch.nn.LeakyReLU(inplace=True),
            # torch.nn.Dropout(0.2),

            torch.nn.Linear(1000, 12*area),
            torch.nn.BatchNorm1d(12*area),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.2),
        )
        self.convolutive_layers = torch.nn.Sequential(
            torch.nn.Conv2d(12, 1, kernel_size=3, stride=1, padding=1),
            # torch.nn.Tanh(),
        )

    def forward(self, sample: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Generate a new image from the samples.

        Parameters
        ----------
        sample : torch.Tensor
            The batch of the n samples, output of the ``Decoder.parametrize`` function.
            The size is (n, latent_dim)

        Returns
        -------
        image : torch.Tensor
            The generated image batch, of shape (n, 1, height, width)

        Notes
        -----
        No verifications are performed for performance reason.
        &#34;&#34;&#34;
        out = sample / self.parent.space  # to have reduced data in first layer
        out = self.dense_layers(out)
        shape = 2**len(self.augmentation_layers)
        shape = (self.parent.shape[0]//shape, self.parent.shape[1]//shape)
        out = torch.reshape(out, (len(out), 12, *shape))  # not -1 for empty tensors
        for augment in self.augmentation_layers:
            out = augment(out)
        out = self.convolutive_layers(out)
        norm_mean, norm_std = self.parent.normalization
        out = out * norm_std + norm_mean  # bijection operation of normalizatiion
        return out

    @staticmethod
    def parametrize(mean: torch.Tensor, std: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Perform a random draw according to the normal law N(mean, std**2).

        Parameters
        ----------
        mean : torch.Tensor
            The batch of the mean vectors, shape (n, latent_dim).
        std : torch.Tensor
            The batch of the diagonal sqrt(covariance) matrix, shape (n, latent_dim).

        Returns
        -------
        draw : torch.Tensor
            The batch of the random draw.

        Notes
        -----
        No verifications are performed for performance reason.
        &#34;&#34;&#34;
        sample = torch.randn_like(mean)  # torch.zeros_like(mean) for non variational
        sample = torch.mul(sample, std, out=(None if std.requires_grad else sample))
        sample = torch.add(
            sample, mean, out=(None if sample.requires_grad or mean.requires_grad else sample)
        )
        return sample

    @property
    def parent(self) -&gt; VAESpotClassifier:
        &#34;&#34;&#34;Return the parent module.&#34;&#34;&#34;
        return self._parent[0]

    def plot_map(
        self,
        axe,
        grid: typing.Union[numbers.Integral, tuple[numbers.Integral, numbers.Integral]] = 10,
    ):
        &#34;&#34;&#34;Generate and display spots from a regular sampling of latent space.

        Parameters
        ----------
        axe : matplotlib.axes.Axes
            The 2d empty axe ready to be filled.
        grid : int or tuple[int, int]
            Grid dimension in latent space. If only one number is supplied,
            the grid will have this dimension on all axes.
            The 2 coordinates corresponds respectively to the number of lines and columns.
        &#34;&#34;&#34;
        from matplotlib.axes import Axes

        assert self.parent.latent_dim == 2, &#34;can only plot in a 2d space&#34;
        assert isinstance(axe, Axes), axe.__class__.__name__
        if isinstance(grid, numbers.Integral):
            grid = (grid, grid)
        assert isinstance(grid, tuple), grid.__class__.__name__
        assert len(grid) == 2, len(grid)
        assert isinstance(grid[0], numbers.Integral) and isinstance(grid[1], numbers.Integral), grid
        assert grid &gt;= (1, 1), grid

        # fill figure metadata
        axe.set_title(f&#34;generated spots from latent regular grid of {grid[0]}x{grid[1]}&#34;)

        # generated data
        device = next(self.parameters()).device
        space = self.parent.space
        points = torch.meshgrid(
            torch.linspace(-space, space, grid[0], dtype=torch.float32, device=device),
            torch.linspace(-space, space, grid[1], dtype=torch.float32, device=device),
            indexing=&#34;ij&#34;,
        )
        points = (points[0].ravel(), points[1].ravel())
        points = torch.cat([points[0].unsqueeze(1), points[1].unsqueeze(1)], dim=1)
        predicted = self.forward(points)

        # draw data
        shape = self.parent.shape
        mosaic = torch.empty(
            (grid[0]*shape[0], grid[1]*shape[1]),
            dtype=torch.float32,
            device=device,
        )
        for i in range(grid[0]):
            for j in range(grid[1]):
                mosaic[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = (
                    predicted[i*grid[1]+j]
                )
        axe.imshow(
            mosaic.numpy(force=True).transpose(),
            aspect=mosaic.shape[0]/mosaic.shape[1],
            interpolation=None,  # antialiasing is True
            cmap=&#34;gray&#34;,
            origin=&#34;lower&#34;,
            extent=[-space, space, -space, space],
        )


class Encoder(torch.nn.Module):
    &#34;&#34;&#34;Encode an image into a gausian probality density.

    Attributes
    ----------
    parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
        The main full auto encoder, containing this module.
    &#34;&#34;&#34;

    def __init__(self, parent: VAESpotClassifier):
        &#34;&#34;&#34;Initialise the encoder.

        Parameters
        ----------
        parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
            The main module.
        &#34;&#34;&#34;
        assert isinstance(parent, VAESpotClassifier), parent.__class__.__name__
        self._parent = (parent,)  # pack into tuple solve `cannot assign module before...`
        super().__init__()

        # size reduction layers
        self.reduction_layers = torch.nn.ModuleList()
        shape = parent.shape
        while min(shape) &gt; 32:
            self.reduction_layers.append(
                torch.nn.Sequential(
                    torch.nn.Conv2d(12, 12, kernel_size=3, stride=1, padding=1),
                    torch.nn.Dropout(0.1),
                    torch.nn.MaxPool2d(2, stride=2),
                    torch.nn.LeakyReLU(inplace=True),
                )
            )
            shape = (shape[0]//2, shape[1]//2)

        # little fitures extraction layer
        self.convolutive_layers = torch.nn.Sequential(
            torch.nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=1),
            # torch.nn.BatchNorm2d(12),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.1),
        )

        # final dense layers
        area = shape[0] * shape[1]
        self.dense_layers = torch.nn.Sequential(
            torch.nn.Flatten(),

            torch.nn.Linear(12*area, 1000),
            torch.nn.BatchNorm1d(1000),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.2),

            # torch.nn.Linear(1000, 500),
            # # torch.nn.BatchNorm1d(500),
            # torch.nn.LeakyReLU(inplace=True),
            # torch.nn.Dropout(0.2),

        )
        self.prob_layers = torch.nn.ModuleList([
            torch.nn.Sequential(  # mean layer
                torch.nn.Linear(1000, parent.latent_dim),
                torch.nn.Tanh(),
            ),
            torch.nn.Sequential(  # log var layer
                torch.nn.Linear(1000, parent.latent_dim),
                torch.nn.LogSigmoid(),
            ),
        ])

    def forward(self, batch: torch.Tensor) -&gt; tuple[torch.Tensor, typing.Union[None, torch.Tensor]]:
        &#34;&#34;&#34;Extract the mean and the std for each images.

        Parameters
        ----------
        batch : torch.Tensor
            The stack of the n images, of shape (n, 1, height, width).

        Returns
        -------
        mean : torch.Tensor
            The mean (center of gaussians) for each image, shape (n, latent_dims).
        std : torch.Tensor
            The standard deviation (shape of gaussian) for each image, shape (n, latent_dims).

        Notes
        -----
        No verifications are performed for performance reason.
        If the model is in eval mode, it computes only the mean and gives the value None to the std.
        &#34;&#34;&#34;
        norm_mean, norm_std = self.parent.normalization
        batch = (batch - norm_mean) / norm_std  # centered and reduced

        # prefitures extraction
        inter_value = self.convolutive_layers(batch)

        # reduction
        for reducer in self.reduction_layers:
            inter_value = reducer(inter_value)

        # final layer
        inter_value = self.dense_layers(inter_value)
        mean = self.prob_layers[0](inter_value)
        mean = mean * self.parent.space  # to have reduced data in last layer
        if not self.training:
            return (mean, None)
        std = self.prob_layers[1](inter_value)  # not std but log(std**2)
        std = torch.exp(0.5 * std)  # not std but log(std)
        return (mean, std)

    @property
    def parent(self) -&gt; VAESpotClassifier:
        &#34;&#34;&#34;Return the parent module.&#34;&#34;&#34;
        return self._parent[0]

    def plot_latent(self, axe, spots_generator):
        &#34;&#34;&#34;Plot the 2d pca of the spots projected in the latent space.

        Parameters
        ----------
        axe : matplotlib.axes.Axes
            The 2d empty axe ready to be filled.
        spots_generator : iterable
            A generator of spot batch, each item has to be of shape (:, h, w).
        &#34;&#34;&#34;
        from matplotlib.axes import Axes
        assert isinstance(axe, Axes), axe.__class__.__name__
        assert hasattr(spots_generator, &#34;__iter__&#34;), spots_generator.__class__.__name__

        # eval model to get all points
        points = []
        for batch in spots_generator:
            assert isinstance(batch, torch.Tensor), batch.__class__.__name__
            assert batch.ndim == 3 and batch.shape[-2:] == self.parent.shape, batch.shape
            points.append(self.forward(batch.unsqueeze(1))[0])
        points = torch.cat(points)

        # projection with PCA
        points = points[:, :2]

        # plot
        axe.set_title(&#34;latent space projections&#34;)
        axe.axis(&#34;equal&#34;)
        space = self.parent.space
        axe.plot(
            [-space, space, space, -space, -space],
            [-space, -space, space, space, -space],
            color=&#34;black&#34;,
        )
        axe.scatter(*points.numpy(force=True).transpose())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.Decoder"><code class="flex name class">
<span>class <span class="ident">Decoder</span></span>
<span>(</span><span>parent: <a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Decode the latent sample into a new image.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>parent</code></strong> :&ensp;<code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></dt>
<dd>The main full auto encoder, containing this module.</dd>
</dl>
<p>Initialise the decoder.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>parent</code></strong> :&ensp;<code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></dt>
<dd>The main module.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Decoder(torch.nn.Module):
    &#34;&#34;&#34;Decode the latent sample into a new image.

    Attributes
    ----------
    parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
        The main full auto encoder, containing this module.
    &#34;&#34;&#34;

    def __init__(self, parent: VAESpotClassifier):
        &#34;&#34;&#34;Initialise the decoder.

        Parameters
        ----------
        parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
            The main module.
        &#34;&#34;&#34;
        assert isinstance(parent, VAESpotClassifier), parent.__class__.__name__
        self._parent = (parent,)  # pack into tuple solve `cannot assign module before...`
        super().__init__()

        # size augmentation layers
        self.augmentation_layers = torch.nn.ModuleList()
        shape = parent.shape
        while min(shape) &gt; 32:
            self.augmentation_layers.append(
                torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(12, 12, kernel_size=4, stride=2, padding=1),
                    torch.nn.Dropout(0.1),
                    torch.nn.LeakyReLU(inplace=True),
                    # torch.nn.MaxUnpool2d(2, stride=2),
                )
            )
            shape = (shape[0]//2, shape[1]//2)

        area = shape[0] * shape[1]
        self.dense_layers = torch.nn.Sequential(
            torch.nn.Linear(parent.latent_dim, 1000),
            # torch.nn.BatchNorm1d(1000),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.2),

            # torch.nn.Linear(500, 1000),
            # # torch.nn.BatchNorm1d(1000),
            # torch.nn.LeakyReLU(inplace=True),
            # torch.nn.Dropout(0.2),

            torch.nn.Linear(1000, 12*area),
            torch.nn.BatchNorm1d(12*area),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.2),
        )
        self.convolutive_layers = torch.nn.Sequential(
            torch.nn.Conv2d(12, 1, kernel_size=3, stride=1, padding=1),
            # torch.nn.Tanh(),
        )

    def forward(self, sample: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Generate a new image from the samples.

        Parameters
        ----------
        sample : torch.Tensor
            The batch of the n samples, output of the ``Decoder.parametrize`` function.
            The size is (n, latent_dim)

        Returns
        -------
        image : torch.Tensor
            The generated image batch, of shape (n, 1, height, width)

        Notes
        -----
        No verifications are performed for performance reason.
        &#34;&#34;&#34;
        out = sample / self.parent.space  # to have reduced data in first layer
        out = self.dense_layers(out)
        shape = 2**len(self.augmentation_layers)
        shape = (self.parent.shape[0]//shape, self.parent.shape[1]//shape)
        out = torch.reshape(out, (len(out), 12, *shape))  # not -1 for empty tensors
        for augment in self.augmentation_layers:
            out = augment(out)
        out = self.convolutive_layers(out)
        norm_mean, norm_std = self.parent.normalization
        out = out * norm_std + norm_mean  # bijection operation of normalizatiion
        return out

    @staticmethod
    def parametrize(mean: torch.Tensor, std: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Perform a random draw according to the normal law N(mean, std**2).

        Parameters
        ----------
        mean : torch.Tensor
            The batch of the mean vectors, shape (n, latent_dim).
        std : torch.Tensor
            The batch of the diagonal sqrt(covariance) matrix, shape (n, latent_dim).

        Returns
        -------
        draw : torch.Tensor
            The batch of the random draw.

        Notes
        -----
        No verifications are performed for performance reason.
        &#34;&#34;&#34;
        sample = torch.randn_like(mean)  # torch.zeros_like(mean) for non variational
        sample = torch.mul(sample, std, out=(None if std.requires_grad else sample))
        sample = torch.add(
            sample, mean, out=(None if sample.requires_grad or mean.requires_grad else sample)
        )
        return sample

    @property
    def parent(self) -&gt; VAESpotClassifier:
        &#34;&#34;&#34;Return the parent module.&#34;&#34;&#34;
        return self._parent[0]

    def plot_map(
        self,
        axe,
        grid: typing.Union[numbers.Integral, tuple[numbers.Integral, numbers.Integral]] = 10,
    ):
        &#34;&#34;&#34;Generate and display spots from a regular sampling of latent space.

        Parameters
        ----------
        axe : matplotlib.axes.Axes
            The 2d empty axe ready to be filled.
        grid : int or tuple[int, int]
            Grid dimension in latent space. If only one number is supplied,
            the grid will have this dimension on all axes.
            The 2 coordinates corresponds respectively to the number of lines and columns.
        &#34;&#34;&#34;
        from matplotlib.axes import Axes

        assert self.parent.latent_dim == 2, &#34;can only plot in a 2d space&#34;
        assert isinstance(axe, Axes), axe.__class__.__name__
        if isinstance(grid, numbers.Integral):
            grid = (grid, grid)
        assert isinstance(grid, tuple), grid.__class__.__name__
        assert len(grid) == 2, len(grid)
        assert isinstance(grid[0], numbers.Integral) and isinstance(grid[1], numbers.Integral), grid
        assert grid &gt;= (1, 1), grid

        # fill figure metadata
        axe.set_title(f&#34;generated spots from latent regular grid of {grid[0]}x{grid[1]}&#34;)

        # generated data
        device = next(self.parameters()).device
        space = self.parent.space
        points = torch.meshgrid(
            torch.linspace(-space, space, grid[0], dtype=torch.float32, device=device),
            torch.linspace(-space, space, grid[1], dtype=torch.float32, device=device),
            indexing=&#34;ij&#34;,
        )
        points = (points[0].ravel(), points[1].ravel())
        points = torch.cat([points[0].unsqueeze(1), points[1].unsqueeze(1)], dim=1)
        predicted = self.forward(points)

        # draw data
        shape = self.parent.shape
        mosaic = torch.empty(
            (grid[0]*shape[0], grid[1]*shape[1]),
            dtype=torch.float32,
            device=device,
        )
        for i in range(grid[0]):
            for j in range(grid[1]):
                mosaic[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = (
                    predicted[i*grid[1]+j]
                )
        axe.imshow(
            mosaic.numpy(force=True).transpose(),
            aspect=mosaic.shape[0]/mosaic.shape[1],
            interpolation=None,  # antialiasing is True
            cmap=&#34;gray&#34;,
            origin=&#34;lower&#34;,
            extent=[-space, space, -space, space],
        )</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.Decoder.parametrize"><code class="name flex">
<span>def <span class="ident">parametrize</span></span>(<span>mean: torch.Tensor, std: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Perform a random draw according to the normal law N(mean, std**2).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The batch of the mean vectors, shape (n, latent_dim).</dd>
<dt><strong><code>std</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The batch of the diagonal sqrt(covariance) matrix, shape (n, latent_dim).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>draw</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The batch of the random draw.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>No verifications are performed for performance reason.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def parametrize(mean: torch.Tensor, std: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Perform a random draw according to the normal law N(mean, std**2).

    Parameters
    ----------
    mean : torch.Tensor
        The batch of the mean vectors, shape (n, latent_dim).
    std : torch.Tensor
        The batch of the diagonal sqrt(covariance) matrix, shape (n, latent_dim).

    Returns
    -------
    draw : torch.Tensor
        The batch of the random draw.

    Notes
    -----
    No verifications are performed for performance reason.
    &#34;&#34;&#34;
    sample = torch.randn_like(mean)  # torch.zeros_like(mean) for non variational
    sample = torch.mul(sample, std, out=(None if std.requires_grad else sample))
    sample = torch.add(
        sample, mean, out=(None if sample.requires_grad or mean.requires_grad else sample)
    )
    return sample</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.Decoder.parent"><code class="name">var <span class="ident">parent</span> : <a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></dt>
<dd>
<div class="desc"><p>Return the parent module.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def parent(self) -&gt; VAESpotClassifier:
    &#34;&#34;&#34;Return the parent module.&#34;&#34;&#34;
    return self._parent[0]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.Decoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, sample: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Generate a new image from the samples.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The batch of the n samples, output of the <code><a title="laueimproc.nn.vae_spot_classifier.Decoder.parametrize" href="#laueimproc.nn.vae_spot_classifier.Decoder.parametrize">Decoder.parametrize()</a></code> function.
The size is (n, latent_dim)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The generated image batch, of shape (n, 1, height, width)</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>No verifications are performed for performance reason.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, sample: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Generate a new image from the samples.

    Parameters
    ----------
    sample : torch.Tensor
        The batch of the n samples, output of the ``Decoder.parametrize`` function.
        The size is (n, latent_dim)

    Returns
    -------
    image : torch.Tensor
        The generated image batch, of shape (n, 1, height, width)

    Notes
    -----
    No verifications are performed for performance reason.
    &#34;&#34;&#34;
    out = sample / self.parent.space  # to have reduced data in first layer
    out = self.dense_layers(out)
    shape = 2**len(self.augmentation_layers)
    shape = (self.parent.shape[0]//shape, self.parent.shape[1]//shape)
    out = torch.reshape(out, (len(out), 12, *shape))  # not -1 for empty tensors
    for augment in self.augmentation_layers:
        out = augment(out)
    out = self.convolutive_layers(out)
    norm_mean, norm_std = self.parent.normalization
    out = out * norm_std + norm_mean  # bijection operation of normalizatiion
    return out</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.Decoder.plot_map"><code class="name flex">
<span>def <span class="ident">plot_map</span></span>(<span>self, axe, grid: Union[numbers.Integral, tuple[numbers.Integral, numbers.Integral]] = 10)</span>
</code></dt>
<dd>
<div class="desc"><p>Generate and display spots from a regular sampling of latent space.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>axe</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The 2d empty axe ready to be filled.</dd>
<dt><strong><code>grid</code></strong> :&ensp;<code>int</code> or <code>tuple[int, int]</code></dt>
<dd>Grid dimension in latent space. If only one number is supplied,
the grid will have this dimension on all axes.
The 2 coordinates corresponds respectively to the number of lines and columns.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_map(
    self,
    axe,
    grid: typing.Union[numbers.Integral, tuple[numbers.Integral, numbers.Integral]] = 10,
):
    &#34;&#34;&#34;Generate and display spots from a regular sampling of latent space.

    Parameters
    ----------
    axe : matplotlib.axes.Axes
        The 2d empty axe ready to be filled.
    grid : int or tuple[int, int]
        Grid dimension in latent space. If only one number is supplied,
        the grid will have this dimension on all axes.
        The 2 coordinates corresponds respectively to the number of lines and columns.
    &#34;&#34;&#34;
    from matplotlib.axes import Axes

    assert self.parent.latent_dim == 2, &#34;can only plot in a 2d space&#34;
    assert isinstance(axe, Axes), axe.__class__.__name__
    if isinstance(grid, numbers.Integral):
        grid = (grid, grid)
    assert isinstance(grid, tuple), grid.__class__.__name__
    assert len(grid) == 2, len(grid)
    assert isinstance(grid[0], numbers.Integral) and isinstance(grid[1], numbers.Integral), grid
    assert grid &gt;= (1, 1), grid

    # fill figure metadata
    axe.set_title(f&#34;generated spots from latent regular grid of {grid[0]}x{grid[1]}&#34;)

    # generated data
    device = next(self.parameters()).device
    space = self.parent.space
    points = torch.meshgrid(
        torch.linspace(-space, space, grid[0], dtype=torch.float32, device=device),
        torch.linspace(-space, space, grid[1], dtype=torch.float32, device=device),
        indexing=&#34;ij&#34;,
    )
    points = (points[0].ravel(), points[1].ravel())
    points = torch.cat([points[0].unsqueeze(1), points[1].unsqueeze(1)], dim=1)
    predicted = self.forward(points)

    # draw data
    shape = self.parent.shape
    mosaic = torch.empty(
        (grid[0]*shape[0], grid[1]*shape[1]),
        dtype=torch.float32,
        device=device,
    )
    for i in range(grid[0]):
        for j in range(grid[1]):
            mosaic[i*shape[0]:(i+1)*shape[0], j*shape[1]:(j+1)*shape[1]] = (
                predicted[i*grid[1]+j]
            )
    axe.imshow(
        mosaic.numpy(force=True).transpose(),
        aspect=mosaic.shape[0]/mosaic.shape[1],
        interpolation=None,  # antialiasing is True
        cmap=&#34;gray&#34;,
        origin=&#34;lower&#34;,
        extent=[-space, space, -space, space],
    )</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.Encoder"><code class="flex name class">
<span>class <span class="ident">Encoder</span></span>
<span>(</span><span>parent: <a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Encode an image into a gausian probality density.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>parent</code></strong> :&ensp;<code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></dt>
<dd>The main full auto encoder, containing this module.</dd>
</dl>
<p>Initialise the encoder.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>parent</code></strong> :&ensp;<code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></dt>
<dd>The main module.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Encoder(torch.nn.Module):
    &#34;&#34;&#34;Encode an image into a gausian probality density.

    Attributes
    ----------
    parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
        The main full auto encoder, containing this module.
    &#34;&#34;&#34;

    def __init__(self, parent: VAESpotClassifier):
        &#34;&#34;&#34;Initialise the encoder.

        Parameters
        ----------
        parent : laueimproc.nn.vae_spot_classifier.VAESpotClassifier
            The main module.
        &#34;&#34;&#34;
        assert isinstance(parent, VAESpotClassifier), parent.__class__.__name__
        self._parent = (parent,)  # pack into tuple solve `cannot assign module before...`
        super().__init__()

        # size reduction layers
        self.reduction_layers = torch.nn.ModuleList()
        shape = parent.shape
        while min(shape) &gt; 32:
            self.reduction_layers.append(
                torch.nn.Sequential(
                    torch.nn.Conv2d(12, 12, kernel_size=3, stride=1, padding=1),
                    torch.nn.Dropout(0.1),
                    torch.nn.MaxPool2d(2, stride=2),
                    torch.nn.LeakyReLU(inplace=True),
                )
            )
            shape = (shape[0]//2, shape[1]//2)

        # little fitures extraction layer
        self.convolutive_layers = torch.nn.Sequential(
            torch.nn.Conv2d(1, 12, kernel_size=3, stride=1, padding=1),
            # torch.nn.BatchNorm2d(12),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.1),
        )

        # final dense layers
        area = shape[0] * shape[1]
        self.dense_layers = torch.nn.Sequential(
            torch.nn.Flatten(),

            torch.nn.Linear(12*area, 1000),
            torch.nn.BatchNorm1d(1000),
            torch.nn.LeakyReLU(inplace=True),
            torch.nn.Dropout(0.2),

            # torch.nn.Linear(1000, 500),
            # # torch.nn.BatchNorm1d(500),
            # torch.nn.LeakyReLU(inplace=True),
            # torch.nn.Dropout(0.2),

        )
        self.prob_layers = torch.nn.ModuleList([
            torch.nn.Sequential(  # mean layer
                torch.nn.Linear(1000, parent.latent_dim),
                torch.nn.Tanh(),
            ),
            torch.nn.Sequential(  # log var layer
                torch.nn.Linear(1000, parent.latent_dim),
                torch.nn.LogSigmoid(),
            ),
        ])

    def forward(self, batch: torch.Tensor) -&gt; tuple[torch.Tensor, typing.Union[None, torch.Tensor]]:
        &#34;&#34;&#34;Extract the mean and the std for each images.

        Parameters
        ----------
        batch : torch.Tensor
            The stack of the n images, of shape (n, 1, height, width).

        Returns
        -------
        mean : torch.Tensor
            The mean (center of gaussians) for each image, shape (n, latent_dims).
        std : torch.Tensor
            The standard deviation (shape of gaussian) for each image, shape (n, latent_dims).

        Notes
        -----
        No verifications are performed for performance reason.
        If the model is in eval mode, it computes only the mean and gives the value None to the std.
        &#34;&#34;&#34;
        norm_mean, norm_std = self.parent.normalization
        batch = (batch - norm_mean) / norm_std  # centered and reduced

        # prefitures extraction
        inter_value = self.convolutive_layers(batch)

        # reduction
        for reducer in self.reduction_layers:
            inter_value = reducer(inter_value)

        # final layer
        inter_value = self.dense_layers(inter_value)
        mean = self.prob_layers[0](inter_value)
        mean = mean * self.parent.space  # to have reduced data in last layer
        if not self.training:
            return (mean, None)
        std = self.prob_layers[1](inter_value)  # not std but log(std**2)
        std = torch.exp(0.5 * std)  # not std but log(std)
        return (mean, std)

    @property
    def parent(self) -&gt; VAESpotClassifier:
        &#34;&#34;&#34;Return the parent module.&#34;&#34;&#34;
        return self._parent[0]

    def plot_latent(self, axe, spots_generator):
        &#34;&#34;&#34;Plot the 2d pca of the spots projected in the latent space.

        Parameters
        ----------
        axe : matplotlib.axes.Axes
            The 2d empty axe ready to be filled.
        spots_generator : iterable
            A generator of spot batch, each item has to be of shape (:, h, w).
        &#34;&#34;&#34;
        from matplotlib.axes import Axes
        assert isinstance(axe, Axes), axe.__class__.__name__
        assert hasattr(spots_generator, &#34;__iter__&#34;), spots_generator.__class__.__name__

        # eval model to get all points
        points = []
        for batch in spots_generator:
            assert isinstance(batch, torch.Tensor), batch.__class__.__name__
            assert batch.ndim == 3 and batch.shape[-2:] == self.parent.shape, batch.shape
            points.append(self.forward(batch.unsqueeze(1))[0])
        points = torch.cat(points)

        # projection with PCA
        points = points[:, :2]

        # plot
        axe.set_title(&#34;latent space projections&#34;)
        axe.axis(&#34;equal&#34;)
        space = self.parent.space
        axe.plot(
            [-space, space, space, -space, -space],
            [-space, -space, space, space, -space],
            color=&#34;black&#34;,
        )
        axe.scatter(*points.numpy(force=True).transpose())</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.Encoder.parent"><code class="name">var <span class="ident">parent</span> : <a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></dt>
<dd>
<div class="desc"><p>Return the parent module.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def parent(self) -&gt; VAESpotClassifier:
    &#34;&#34;&#34;Return the parent module.&#34;&#34;&#34;
    return self._parent[0]</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.Encoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, batch: torch.Tensor) ‑> tuple[torch.Tensor, typing.Optional[torch.Tensor]]</span>
</code></dt>
<dd>
<div class="desc"><p>Extract the mean and the std for each images.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The stack of the n images, of shape (n, 1, height, width).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mean</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The mean (center of gaussians) for each image, shape (n, latent_dims).</dd>
<dt><strong><code>std</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The standard deviation (shape of gaussian) for each image, shape (n, latent_dims).</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>No verifications are performed for performance reason.
If the model is in eval mode, it computes only the mean and gives the value None to the std.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, batch: torch.Tensor) -&gt; tuple[torch.Tensor, typing.Union[None, torch.Tensor]]:
    &#34;&#34;&#34;Extract the mean and the std for each images.

    Parameters
    ----------
    batch : torch.Tensor
        The stack of the n images, of shape (n, 1, height, width).

    Returns
    -------
    mean : torch.Tensor
        The mean (center of gaussians) for each image, shape (n, latent_dims).
    std : torch.Tensor
        The standard deviation (shape of gaussian) for each image, shape (n, latent_dims).

    Notes
    -----
    No verifications are performed for performance reason.
    If the model is in eval mode, it computes only the mean and gives the value None to the std.
    &#34;&#34;&#34;
    norm_mean, norm_std = self.parent.normalization
    batch = (batch - norm_mean) / norm_std  # centered and reduced

    # prefitures extraction
    inter_value = self.convolutive_layers(batch)

    # reduction
    for reducer in self.reduction_layers:
        inter_value = reducer(inter_value)

    # final layer
    inter_value = self.dense_layers(inter_value)
    mean = self.prob_layers[0](inter_value)
    mean = mean * self.parent.space  # to have reduced data in last layer
    if not self.training:
        return (mean, None)
    std = self.prob_layers[1](inter_value)  # not std but log(std**2)
    std = torch.exp(0.5 * std)  # not std but log(std)
    return (mean, std)</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.Encoder.plot_latent"><code class="name flex">
<span>def <span class="ident">plot_latent</span></span>(<span>self, axe, spots_generator)</span>
</code></dt>
<dd>
<div class="desc"><p>Plot the 2d pca of the spots projected in the latent space.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>axe</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The 2d empty axe ready to be filled.</dd>
<dt><strong><code>spots_generator</code></strong> :&ensp;<code>iterable</code></dt>
<dd>A generator of spot batch, each item has to be of shape (:, h, w).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_latent(self, axe, spots_generator):
    &#34;&#34;&#34;Plot the 2d pca of the spots projected in the latent space.

    Parameters
    ----------
    axe : matplotlib.axes.Axes
        The 2d empty axe ready to be filled.
    spots_generator : iterable
        A generator of spot batch, each item has to be of shape (:, h, w).
    &#34;&#34;&#34;
    from matplotlib.axes import Axes
    assert isinstance(axe, Axes), axe.__class__.__name__
    assert hasattr(spots_generator, &#34;__iter__&#34;), spots_generator.__class__.__name__

    # eval model to get all points
    points = []
    for batch in spots_generator:
        assert isinstance(batch, torch.Tensor), batch.__class__.__name__
        assert batch.ndim == 3 and batch.shape[-2:] == self.parent.shape, batch.shape
        points.append(self.forward(batch.unsqueeze(1))[0])
    points = torch.cat(points)

    # projection with PCA
    points = points[:, :2]

    # plot
    axe.set_title(&#34;latent space projections&#34;)
    axe.axis(&#34;equal&#34;)
    space = self.parent.space
    axe.plot(
        [-space, space, space, -space, -space],
        [-space, -space, space, space, -space],
        color=&#34;black&#34;,
    )
    axe.scatter(*points.numpy(force=True).transpose())</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier"><code class="flex name class">
<span>class <span class="ident">VAESpotClassifier</span></span>
<span>(</span><span>shape: Container[numbers.Integral], latent_dim: numbers.Integral = 2, space: numbers.Real = 3.0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A partialy convolutive variationel auto encoder used for unsupervised spot classification.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>decoder</code></strong> :&ensp;<code><a title="laueimproc.nn.vae_spot_classifier.Decoder" href="#laueimproc.nn.vae_spot_classifier.Decoder">Decoder</a></code></dt>
<dd>The decoder part, able to random draw and reconstitue an image from the encoder.</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>The device of the model.</dd>
<dt><strong><code>encoder</code></strong> :&ensp;<code><a title="laueimproc.nn.vae_spot_classifier.Encoder" href="#laueimproc.nn.vae_spot_classifier.Encoder">Encoder</a></code></dt>
<dd>The encoder part, able to transform an image into au gaussian law.</dd>
<dt><strong><code>latent_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>The dimension of the latent space.</dd>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple[int, int]</code></dt>
<dd>The shape of the rois.</dd>
<dt><strong><code>space</code></strong> :&ensp;<code>float</code>, default <code>= 3.0</code></dt>
<dd>The non penalized spreading area half size.</dd>
</dl>
<p>Initialise the model.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>shape</code></strong> :&ensp;<code>tuple[int, int]</code></dt>
<dd>Transmitted to <code><a title="laueimproc.nn.vae_spot_classifier.Encoder" href="#laueimproc.nn.vae_spot_classifier.Encoder">Encoder</a></code>
and <code><a title="laueimproc.nn.vae_spot_classifier.Decoder" href="#laueimproc.nn.vae_spot_classifier.Decoder">Decoder</a></code>.</dd>
<dt><strong><code>latent_dim</code></strong> :&ensp;<code>int</code>, default=<code>2</code></dt>
<dd>Transmitted to <code><a title="laueimproc.nn.vae_spot_classifier.Encoder" href="#laueimproc.nn.vae_spot_classifier.Encoder">Encoder</a></code>
and <code><a title="laueimproc.nn.vae_spot_classifier.Decoder" href="#laueimproc.nn.vae_spot_classifier.Decoder">Decoder</a></code>.</dd>
<dt><strong><code>space</code></strong> :&ensp;<code>float</code>, default=<code>3.0</code></dt>
<dd>The non penalized spreading area in the latent space.
All the points with abs(p) &lt;= space are autorized.
A small value condensate all the data, very continuous space but hard to split.
In a other way, a large space split the clusters but the values betwean the clusters
are not well defined.</dd>
<dt><strong><code>intensity_sensitive</code></strong> :&ensp;<code>boolean</code>, default=<code>True</code></dt>
<dd>If set to False, the model will not consider the spots intensity,
as they will be normalized to have a power of 1.</dd>
<dt><strong><code>scale_sensitive</code></strong> :&ensp;<code>boolean = True</code></dt>
<dd>If set to False, the model will not consider the spots size,
as they will be resized and reinterpolated to a constant shape.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VAESpotClassifier(torch.nn.Module):
    &#34;&#34;&#34;A partialy convolutive variationel auto encoder used for unsupervised spot classification.

    Attributes
    ----------
    decoder : Decoder
        The decoder part, able to random draw and reconstitue an image from the encoder.
    device: torch.device
        The device of the model.
    encoder : Encoder
        The encoder part, able to transform an image into au gaussian law.
    latent_dim : int
        The dimension of the latent space.
    shape : tuple[int, int]
        The shape of the rois.
    space : float, default = 3.0
        The non penalized spreading area half size.
    &#34;&#34;&#34;

    def __init__(
        self,
        shape: typing.Container[numbers.Integral],
        latent_dim: numbers.Integral = 2,
        space: numbers.Real = 3.0,
        **kwargs,
    ):
        &#34;&#34;&#34;Initialise the model.

        Parameters
        ----------
        shape : tuple[int, int]
            Transmitted to ``laueimproc.nn.vae_spot_classifier.Encoder``
            and ``laueimproc.nn.vae_spot_classifier.Decoder``.
        latent_dim : int, default=2
            Transmitted to ``laueimproc.nn.vae_spot_classifier.Encoder``
            and ``laueimproc.nn.vae_spot_classifier.Decoder``.
        space : float, default=3.0
            The non penalized spreading area in the latent space.
            All the points with abs(p) &lt;= space are autorized.
            A small value condensate all the data, very continuous space but hard to split.
            In a other way, a large space split the clusters but the values betwean the clusters
            are not well defined.
        intensity_sensitive : boolean, default=True
            If set to False, the model will not consider the spots intensity,
            as they will be normalized to have a power of 1.
        scale_sensitive : boolean = True
            If set to False, the model will not consider the spots size,
            as they will be resized and reinterpolated to a constant shape.
        &#34;&#34;&#34;
        assert hasattr(shape, &#34;__iter__&#34;), shape.__class__.__name__
        shape = tuple(shape)
        assert len(shape) == 2, shape
        assert isinstance(shape[0], numbers.Integral) and isinstance(shape[1], numbers.Integral), \
            shape
        shape = (int(shape[0]), int(shape[1]))
        assert isinstance(latent_dim, numbers.Integral), latent_dim.__class__.__name__
        assert latent_dim &gt;= 1, latent_dim
        latent_dim = int(latent_dim)
        assert isinstance(space, numbers.Real), space.__class__.__name__
        assert space &gt; 0, space
        space = float(space)
        self._latent_dim = latent_dim
        self._shape = shape
        self._space = space
        self._sensitivity = {}
        self._sensitivity[&#34;intensity&#34;] = kwargs.get(&#34;intensity_sensitive&#34;, True)
        assert isinstance(self._sensitivity[&#34;intensity&#34;], bool), self._sensitivity[&#34;intensity&#34;]
        self._sensitivity[&#34;scale&#34;] = kwargs.get(&#34;scale_sensitive&#34;, True)
        assert isinstance(self._sensitivity[&#34;scale&#34;], bool), self._sensitivity[&#34;scale&#34;]

        self._normalization = None

        super().__init__()
        self.encoder = Encoder(self)
        self.decoder = Decoder(self)

    def dataaug(self, image: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply all the data augmentations on the image.

        Parameters
        ----------
        image : torch.Tensor
            The image of shape (h, w).

        Returns
        -------
        aug_batch : torch.Tensor
            The augmented stack of images of shape (n, 1, h&#39;, w&#39;).
        &#34;&#34;&#34;
        assert isinstance(image, torch.Tensor), image.__class__.__name__
        assert image.ndim == 2 and image.shape &gt;= (1, 1), image.shape

        # reshape or pad/crop
        if self._sensitivity[&#34;scale&#34;]:
            from laueimproc.nn.dataaug.scale import rescale
            image = rescale(image, self.shape, copy=False)
        else:
            from laueimproc.nn.dataaug.patch import patch
            image = patch(image, self.shape, copy=False)

        # intensity normalization
        if not self._sensitivity[&#34;intensity&#34;]:
            energy = torch.sqrt(torch.sum(image * image))
            image = image / (energy + torch.finfo(image.dtype).eps)

        return image

    @property
    def device(self) -&gt; torch.device:
        &#34;&#34;&#34;Return the device of the model.&#34;&#34;&#34;
        return next(self.parameters()).device

    def forward(self, data) -&gt; torch.Tensor:
        &#34;&#34;&#34;Encode, random draw and decode the image.

        Parameters
        ----------
        data : laueimproc.classes.base_diagram.BaseDiagram or torch.Tensor
            If a digram is provided, the spots are extract, data augmentation are applied,
            and the mean projection in the latent space is returned.
            If the input is a tensor, data augmentation are not applied.
            Return the autoencoded data, after having decoded a random latent space draw.

        Returns
        -------
        torch.Tensor
            The mean latent vector of shape (n, latent_dim) if the input is a Diagram.
            The generated image of shape (n, height, width) otherwise.
        &#34;&#34;&#34;
        from laueimproc.classes.base_diagram import BaseDiagram
        assert isinstance(data, (torch.Tensor, BaseDiagram)), data.__class__.__name__

        # case tensor
        if isinstance(data, torch.Tensor):
            if data.ndim == 2:
                return self.forward(data.unsqueeze(0)).squeeze(0)
            assert data.ndim == 3, data.shape
            mean, std = self.encoder(data.unsqueeze(1))
            sample = self.decoder.parametrize(mean, std) if self.training else mean
            generated_image = self.decoder(sample)
            return generated_image.squeeze(1)

        # case diagram
        batch = torch.empty((len(data), *self.shape), dtype=torch.float32, device=self.device)
        for i, (roi, (height, width)) in enumerate(zip(data.rois, data.bboxes[:, 2:].tolist())):
            batch[i] = self.dataaug(roi[:height, :width])
        mean, _ = self.encoder(batch.unsqueeze(1))
        return mean

    @property
    def latent_dim(self) -&gt; int:
        &#34;&#34;&#34;Return the dimension of the latent space.&#34;&#34;&#34;
        return self._latent_dim

    def loss(self, batch: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:
        &#34;&#34;&#34;Forward the data and compute the loss values.

        Parameters
        ----------
        batch : torch.Tensor
            The image stack of shape (n, h, w).

        Returns
        -------
        mse_loss : torch.Tensor
            The sum of the mean square error loss for each image in the batch, shape (1,).
        kld_loss : torch.Tensor
            Pretty close to the sum of the Kullback-Leibler divergence
            for each projection in the batch, shape (1,).
            It is not litteraly the Kullback-Leibler divergence because the peanality for the mean
            is less strict, the cost is 0 in the [-space, space] interval.
            The cost is minimum when var=1 and -space&lt;=mean&lt;=space.

        Notes
        -----
        * No verifications are performed for performance reason.
        * The reduction is sum and not mean because it ables to split the batch in several slices.
        &#34;&#34;&#34;
        assert isinstance(batch, torch.Tensor), batch.__class__.__name__
        assert batch.ndim == 3, batch.shape

        mean, std = self.encoder(batch.unsqueeze(1))
        if std is not None:
            var = std * std
            # real kld = sum(var - 1 - torch.log(var) + mean**2) / 2
            kld = torch.sum(
                var - 1 - torch.log(var)
                # + torch.nn.functional.relu(torch.abs(mean)-self.space)  # 0 because tanh(mean)
            )
            sample = self.decoder.parametrize(mean, std)
        else:
            kld = None
            sample = mean
        generated_batch = self.decoder(sample).squeeze(1)
        _, norm_std = self.normalization
        mse = torch.sum(torch.mean(((batch - generated_batch)/norm_std)**2, dim=(1, 2)))
        return (mse, kld)

    @property
    def normalization(self) -&gt; tuple[float, float]:
        &#34;&#34;&#34;Return the mean and the std of all the training data.&#34;&#34;&#34;
        if self._normalization is None:
            warnings.warn(&#34;call `scan_data` for allowing data normalization&#34;, RuntimeWarning)
            return 0.0, 1.0
        return self._normalization

    def plot_autoencode(self, axe_input, axe_output, spots: torch.Tensor):
        &#34;&#34;&#34;Encode and decode the images, plot the initial and regenerated images.

        Parameters
        ----------
        axe_input : matplotlib.axes.Axes
            The 2d empty axe ready to be filled by the input mosaic.
        axe_output : matplotlib.axes.Axes
            The 2d empty axe ready to be filled by the generated mosaic.
        spots : torch.Tensor
            The image stack of shape (n, h, w).
        &#34;&#34;&#34;
        from matplotlib.axes import Axes
        assert isinstance(axe_input, Axes), axe_input.__class__.__name__
        assert isinstance(axe_output, Axes), axe_output.__class__.__name__
        assert isinstance(spots, torch.Tensor), spots.__class__.__name__
        assert spots.ndim == 3 and spots.shape[-2:] == self.shape, spots.shape

        # find the grid dimension
        height = round(math.sqrt(len(spots)*self.shape[1]/self.shape[0]))
        width = round(math.sqrt(len(spots)*self.shape[0]/self.shape[1]))
        while height * width &lt; len(spots):
            if (height + 1) * width == len(spots):
                height += 1
            elif height * (width + 1) == len(spots):
                width += 1
            elif height &lt; width:  # in favor of square rather than rectangle
                height += 1
            else:
                width += 1

        # input mosaic
        mosaic_in = torch.empty(
            (height*self.shape[0], width*self.shape[1]),
            dtype=spots.dtype,
            device=spots.device,
        )
        for i in range(height):
            for j in range(width):
                idx = j + i*width
                pict = spots[idx] if idx &lt; len(spots) else 0
                mosaic_in[
                    i*self.shape[0]:(i+1)*self.shape[0], j*self.shape[1]:(j+1)*self.shape[1]
                ] = pict

        # output mosaic
        spots = self.forward(spots)
        mosaic_out = torch.empty(
            (height*self.shape[0], width*self.shape[1]),
            dtype=spots.dtype,
            device=spots.device,
        )
        for i in range(height):
            for j in range(width):
                idx = j + i*width
                pict = spots[idx] if idx &lt; len(spots) else 0
                mosaic_out[
                    i*self.shape[0]:(i+1)*self.shape[0], j*self.shape[1]:(j+1)*self.shape[1]
                ] = pict

        # plot
        vmin = min(float(mosaic_in.min()), float(mosaic_out.min()))
        vmax = max(float(mosaic_in.max()), float(mosaic_out.max()))
        axe_input.set_title(&#34;input images&#34;)
        axe_input.axis(&#34;off&#34;)
        axe_input.imshow(
            mosaic_in.numpy(force=True),
            aspect=&#34;equal&#34;,
            interpolation=None,  # antialiasing is True
            cmap=&#34;gray&#34;,
            vmin=vmin,
            vmax=vmax,
        )
        axe_output.set_title(&#34;output images&#34;)
        axe_output.axis(&#34;off&#34;)
        axe_output.imshow(
            mosaic_out.numpy(force=True),
            aspect=&#34;equal&#34;,
            interpolation=None,  # antialiasing is True
            cmap=&#34;gray&#34;,
            vmin=vmin,
            vmax=vmax,
        )

    def scan_data(self, spots_generator: torch.Tensor):
        &#34;&#34;&#34;Complete the data histogram to standardise data (centered and reduction).

        Parameters
        ----------
        spots_generator : iterable
            A generator of spot batch, each item has to be of shape (:, h, w).
        &#34;&#34;&#34;
        assert hasattr(spots_generator, &#34;__iter__&#34;), spots_generator.__class__.__name__

        bins = 10000
        tot_hist = torch.zeros(bins, dtype=torch.float64)
        # for batch in tqdm(spots_generator, desc=&#34;scan data repartition&#34;):
        for batch in spots_generator:
            assert isinstance(batch, torch.Tensor), batch.__class__.__name__
            assert batch.ndim == 3 and batch.shape[-2:] == self.shape, batch.shape
            hist, _ = torch.histogram(batch.to(torch.float64), bins=bins, range=(0.0, 1.0))
            tot_hist = tot_hist.to(hist.device)
            tot_hist += hist

        values = torch.linspace(1.0/(2*bins), 1.0 - 1.0/(2*bins), bins)
        mean = float((tot_hist * values).sum() / tot_hist.sum())
        std = float(torch.sqrt((tot_hist * (values - mean)**2).sum() / tot_hist.sum()))
        self._normalization = (mean, std)

    @property
    def shape(self) -&gt; tuple[int, int]:
        &#34;&#34;&#34;Return the shape of the images.&#34;&#34;&#34;
        return self._shape

    @property
    def space(self) -&gt; float:
        &#34;&#34;&#34;Return the non penalized spreading area half size.&#34;&#34;&#34;
        return self._space</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.device"><code class="name">var <span class="ident">device</span> : torch.device</code></dt>
<dd>
<div class="desc"><p>Return the device of the model.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def device(self) -&gt; torch.device:
    &#34;&#34;&#34;Return the device of the model.&#34;&#34;&#34;
    return next(self.parameters()).device</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.latent_dim"><code class="name">var <span class="ident">latent_dim</span> : int</code></dt>
<dd>
<div class="desc"><p>Return the dimension of the latent space.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def latent_dim(self) -&gt; int:
    &#34;&#34;&#34;Return the dimension of the latent space.&#34;&#34;&#34;
    return self._latent_dim</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.normalization"><code class="name">var <span class="ident">normalization</span> : tuple[float, float]</code></dt>
<dd>
<div class="desc"><p>Return the mean and the std of all the training data.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def normalization(self) -&gt; tuple[float, float]:
    &#34;&#34;&#34;Return the mean and the std of all the training data.&#34;&#34;&#34;
    if self._normalization is None:
        warnings.warn(&#34;call `scan_data` for allowing data normalization&#34;, RuntimeWarning)
        return 0.0, 1.0
    return self._normalization</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.shape"><code class="name">var <span class="ident">shape</span> : tuple[int, int]</code></dt>
<dd>
<div class="desc"><p>Return the shape of the images.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def shape(self) -&gt; tuple[int, int]:
    &#34;&#34;&#34;Return the shape of the images.&#34;&#34;&#34;
    return self._shape</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.space"><code class="name">var <span class="ident">space</span> : float</code></dt>
<dd>
<div class="desc"><p>Return the non penalized spreading area half size.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def space(self) -&gt; float:
    &#34;&#34;&#34;Return the non penalized spreading area half size.&#34;&#34;&#34;
    return self._space</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.dataaug"><code class="name flex">
<span>def <span class="ident">dataaug</span></span>(<span>self, image: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Apply all the data augmentations on the image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>image</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The image of shape (h, w).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>aug_batch</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The augmented stack of images of shape (n, 1, h', w').</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dataaug(self, image: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Apply all the data augmentations on the image.

    Parameters
    ----------
    image : torch.Tensor
        The image of shape (h, w).

    Returns
    -------
    aug_batch : torch.Tensor
        The augmented stack of images of shape (n, 1, h&#39;, w&#39;).
    &#34;&#34;&#34;
    assert isinstance(image, torch.Tensor), image.__class__.__name__
    assert image.ndim == 2 and image.shape &gt;= (1, 1), image.shape

    # reshape or pad/crop
    if self._sensitivity[&#34;scale&#34;]:
        from laueimproc.nn.dataaug.scale import rescale
        image = rescale(image, self.shape, copy=False)
    else:
        from laueimproc.nn.dataaug.patch import patch
        image = patch(image, self.shape, copy=False)

    # intensity normalization
    if not self._sensitivity[&#34;intensity&#34;]:
        energy = torch.sqrt(torch.sum(image * image))
        image = image / (energy + torch.finfo(image.dtype).eps)

    return image</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, data) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Encode, random draw and decode the image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code><a title="laueimproc.classes.base_diagram.BaseDiagram" href="../classes/base_diagram.html#laueimproc.classes.base_diagram.BaseDiagram">BaseDiagram</a></code> or <code>torch.Tensor</code></dt>
<dd>If a digram is provided, the spots are extract, data augmentation are applied,
and the mean projection in the latent space is returned.
If the input is a tensor, data augmentation are not applied.
Return the autoencoded data, after having decoded a random latent space draw.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>torch.Tensor</code></dt>
<dd>The mean latent vector of shape (n, latent_dim) if the input is a Diagram.
The generated image of shape (n, height, width) otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, data) -&gt; torch.Tensor:
    &#34;&#34;&#34;Encode, random draw and decode the image.

    Parameters
    ----------
    data : laueimproc.classes.base_diagram.BaseDiagram or torch.Tensor
        If a digram is provided, the spots are extract, data augmentation are applied,
        and the mean projection in the latent space is returned.
        If the input is a tensor, data augmentation are not applied.
        Return the autoencoded data, after having decoded a random latent space draw.

    Returns
    -------
    torch.Tensor
        The mean latent vector of shape (n, latent_dim) if the input is a Diagram.
        The generated image of shape (n, height, width) otherwise.
    &#34;&#34;&#34;
    from laueimproc.classes.base_diagram import BaseDiagram
    assert isinstance(data, (torch.Tensor, BaseDiagram)), data.__class__.__name__

    # case tensor
    if isinstance(data, torch.Tensor):
        if data.ndim == 2:
            return self.forward(data.unsqueeze(0)).squeeze(0)
        assert data.ndim == 3, data.shape
        mean, std = self.encoder(data.unsqueeze(1))
        sample = self.decoder.parametrize(mean, std) if self.training else mean
        generated_image = self.decoder(sample)
        return generated_image.squeeze(1)

    # case diagram
    batch = torch.empty((len(data), *self.shape), dtype=torch.float32, device=self.device)
    for i, (roi, (height, width)) in enumerate(zip(data.rois, data.bboxes[:, 2:].tolist())):
        batch[i] = self.dataaug(roi[:height, :width])
    mean, _ = self.encoder(batch.unsqueeze(1))
    return mean</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.loss"><code class="name flex">
<span>def <span class="ident">loss</span></span>(<span>self, batch: torch.Tensor) ‑> tuple[torch.Tensor, torch.Tensor]</span>
</code></dt>
<dd>
<div class="desc"><p>Forward the data and compute the loss values.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The image stack of shape (n, h, w).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>mse_loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The sum of the mean square error loss for each image in the batch, shape (1,).</dd>
<dt><strong><code>kld_loss</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>Pretty close to the sum of the Kullback-Leibler divergence
for each projection in the batch, shape (1,).
It is not litteraly the Kullback-Leibler divergence because the peanality for the mean
is less strict, the cost is 0 in the [-space, space] interval.
The cost is minimum when var=1 and -space&lt;=mean&lt;=space.</dd>
</dl>
<h2 id="notes">Notes</h2>
<ul>
<li>No verifications are performed for performance reason.</li>
<li>The reduction is sum and not mean because it ables to split the batch in several slices.</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def loss(self, batch: torch.Tensor) -&gt; tuple[torch.Tensor, torch.Tensor]:
    &#34;&#34;&#34;Forward the data and compute the loss values.

    Parameters
    ----------
    batch : torch.Tensor
        The image stack of shape (n, h, w).

    Returns
    -------
    mse_loss : torch.Tensor
        The sum of the mean square error loss for each image in the batch, shape (1,).
    kld_loss : torch.Tensor
        Pretty close to the sum of the Kullback-Leibler divergence
        for each projection in the batch, shape (1,).
        It is not litteraly the Kullback-Leibler divergence because the peanality for the mean
        is less strict, the cost is 0 in the [-space, space] interval.
        The cost is minimum when var=1 and -space&lt;=mean&lt;=space.

    Notes
    -----
    * No verifications are performed for performance reason.
    * The reduction is sum and not mean because it ables to split the batch in several slices.
    &#34;&#34;&#34;
    assert isinstance(batch, torch.Tensor), batch.__class__.__name__
    assert batch.ndim == 3, batch.shape

    mean, std = self.encoder(batch.unsqueeze(1))
    if std is not None:
        var = std * std
        # real kld = sum(var - 1 - torch.log(var) + mean**2) / 2
        kld = torch.sum(
            var - 1 - torch.log(var)
            # + torch.nn.functional.relu(torch.abs(mean)-self.space)  # 0 because tanh(mean)
        )
        sample = self.decoder.parametrize(mean, std)
    else:
        kld = None
        sample = mean
    generated_batch = self.decoder(sample).squeeze(1)
    _, norm_std = self.normalization
    mse = torch.sum(torch.mean(((batch - generated_batch)/norm_std)**2, dim=(1, 2)))
    return (mse, kld)</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.plot_autoencode"><code class="name flex">
<span>def <span class="ident">plot_autoencode</span></span>(<span>self, axe_input, axe_output, spots: torch.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Encode and decode the images, plot the initial and regenerated images.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>axe_input</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The 2d empty axe ready to be filled by the input mosaic.</dd>
<dt><strong><code>axe_output</code></strong> :&ensp;<code>matplotlib.axes.Axes</code></dt>
<dd>The 2d empty axe ready to be filled by the generated mosaic.</dd>
<dt><strong><code>spots</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The image stack of shape (n, h, w).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_autoencode(self, axe_input, axe_output, spots: torch.Tensor):
    &#34;&#34;&#34;Encode and decode the images, plot the initial and regenerated images.

    Parameters
    ----------
    axe_input : matplotlib.axes.Axes
        The 2d empty axe ready to be filled by the input mosaic.
    axe_output : matplotlib.axes.Axes
        The 2d empty axe ready to be filled by the generated mosaic.
    spots : torch.Tensor
        The image stack of shape (n, h, w).
    &#34;&#34;&#34;
    from matplotlib.axes import Axes
    assert isinstance(axe_input, Axes), axe_input.__class__.__name__
    assert isinstance(axe_output, Axes), axe_output.__class__.__name__
    assert isinstance(spots, torch.Tensor), spots.__class__.__name__
    assert spots.ndim == 3 and spots.shape[-2:] == self.shape, spots.shape

    # find the grid dimension
    height = round(math.sqrt(len(spots)*self.shape[1]/self.shape[0]))
    width = round(math.sqrt(len(spots)*self.shape[0]/self.shape[1]))
    while height * width &lt; len(spots):
        if (height + 1) * width == len(spots):
            height += 1
        elif height * (width + 1) == len(spots):
            width += 1
        elif height &lt; width:  # in favor of square rather than rectangle
            height += 1
        else:
            width += 1

    # input mosaic
    mosaic_in = torch.empty(
        (height*self.shape[0], width*self.shape[1]),
        dtype=spots.dtype,
        device=spots.device,
    )
    for i in range(height):
        for j in range(width):
            idx = j + i*width
            pict = spots[idx] if idx &lt; len(spots) else 0
            mosaic_in[
                i*self.shape[0]:(i+1)*self.shape[0], j*self.shape[1]:(j+1)*self.shape[1]
            ] = pict

    # output mosaic
    spots = self.forward(spots)
    mosaic_out = torch.empty(
        (height*self.shape[0], width*self.shape[1]),
        dtype=spots.dtype,
        device=spots.device,
    )
    for i in range(height):
        for j in range(width):
            idx = j + i*width
            pict = spots[idx] if idx &lt; len(spots) else 0
            mosaic_out[
                i*self.shape[0]:(i+1)*self.shape[0], j*self.shape[1]:(j+1)*self.shape[1]
            ] = pict

    # plot
    vmin = min(float(mosaic_in.min()), float(mosaic_out.min()))
    vmax = max(float(mosaic_in.max()), float(mosaic_out.max()))
    axe_input.set_title(&#34;input images&#34;)
    axe_input.axis(&#34;off&#34;)
    axe_input.imshow(
        mosaic_in.numpy(force=True),
        aspect=&#34;equal&#34;,
        interpolation=None,  # antialiasing is True
        cmap=&#34;gray&#34;,
        vmin=vmin,
        vmax=vmax,
    )
    axe_output.set_title(&#34;output images&#34;)
    axe_output.axis(&#34;off&#34;)
    axe_output.imshow(
        mosaic_out.numpy(force=True),
        aspect=&#34;equal&#34;,
        interpolation=None,  # antialiasing is True
        cmap=&#34;gray&#34;,
        vmin=vmin,
        vmax=vmax,
    )</code></pre>
</details>
</dd>
<dt id="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.scan_data"><code class="name flex">
<span>def <span class="ident">scan_data</span></span>(<span>self, spots_generator: torch.Tensor)</span>
</code></dt>
<dd>
<div class="desc"><p>Complete the data histogram to standardise data (centered and reduction).</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spots_generator</code></strong> :&ensp;<code>iterable</code></dt>
<dd>A generator of spot batch, each item has to be of shape (:, h, w).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scan_data(self, spots_generator: torch.Tensor):
    &#34;&#34;&#34;Complete the data histogram to standardise data (centered and reduction).

    Parameters
    ----------
    spots_generator : iterable
        A generator of spot batch, each item has to be of shape (:, h, w).
    &#34;&#34;&#34;
    assert hasattr(spots_generator, &#34;__iter__&#34;), spots_generator.__class__.__name__

    bins = 10000
    tot_hist = torch.zeros(bins, dtype=torch.float64)
    # for batch in tqdm(spots_generator, desc=&#34;scan data repartition&#34;):
    for batch in spots_generator:
        assert isinstance(batch, torch.Tensor), batch.__class__.__name__
        assert batch.ndim == 3 and batch.shape[-2:] == self.shape, batch.shape
        hist, _ = torch.histogram(batch.to(torch.float64), bins=bins, range=(0.0, 1.0))
        tot_hist = tot_hist.to(hist.device)
        tot_hist += hist

    values = torch.linspace(1.0/(2*bins), 1.0 - 1.0/(2*bins), bins)
    mean = float((tot_hist * values).sum() / tot_hist.sum())
    std = float(torch.sqrt((tot_hist * (values - mean)**2).sum() / tot_hist.sum()))
    self._normalization = (mean, std)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<form>
<input id="lunr-search" name="q" placeholder="🔎 Search ..." aria-label="Search"
disabled minlength="2">
</form>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.css" integrity="sha512-j1u8eUJ4f23xPPxwOrLUPQaCD2dwzNqqmDDcWS4deWsMv2ohLqmXXuP3hU7g8TyzbMSakP/mMqoNBYWj8AEIFg==" crossorigin>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tingle/0.15.3/tingle.min.js" integrity="sha512-plGUER9JkeEWPPqQBE4sdLqBoQug5Ap+BCGMc7bJ8BXkm+VVj6QzkpBz5Yv2yPkkq+cqg9IpkBaGCas6uDbW8g==" crossorigin></script>
<style>
.modal-dialog iframe {
width: 100vw;
height: calc(100vh - 80px);
}
@media screen and (min-width: 700px) {
.modal-dialog iframe {
width: 70vw;
height: 80vh;
}
}
.modal-dialog .tingle-modal-box {width: auto;}
.modal-dialog .tingle-modal-box__content {padding: 0;}
</style>
<script>
const input = document.getElementById('lunr-search');
input.disabled = false;
input.form.addEventListener('submit', (ev) => {
ev.preventDefault();
const url = new URL(window.location);
url.searchParams.set('q', input.value);
history.replaceState({}, null, url.toString());
search(input.value);
});
const query = new URL(window.location).searchParams.get('q');
if (query)
search(query);
function search(query) {
const url = '../../doc-search.html#' + encodeURIComponent(query);
new tingle.modal({
cssClass: ['modal-dialog'],
onClose: () => {
const url = new URL(window.location);
url.searchParams.delete('q');
history.replaceState({}, null, url.toString());
setTimeout(() => input.focus(), 100);
}
}).setContent('<iframe src="' + url + '"></iframe>').open();
}
</script>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="laueimproc.nn" href="index.html">laueimproc.nn</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="laueimproc.nn.vae_spot_classifier.Decoder" href="#laueimproc.nn.vae_spot_classifier.Decoder">Decoder</a></code></h4>
<ul class="">
<li><code><a title="laueimproc.nn.vae_spot_classifier.Decoder.forward" href="#laueimproc.nn.vae_spot_classifier.Decoder.forward">forward</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.Decoder.parametrize" href="#laueimproc.nn.vae_spot_classifier.Decoder.parametrize">parametrize</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.Decoder.parent" href="#laueimproc.nn.vae_spot_classifier.Decoder.parent">parent</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.Decoder.plot_map" href="#laueimproc.nn.vae_spot_classifier.Decoder.plot_map">plot_map</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laueimproc.nn.vae_spot_classifier.Encoder" href="#laueimproc.nn.vae_spot_classifier.Encoder">Encoder</a></code></h4>
<ul class="">
<li><code><a title="laueimproc.nn.vae_spot_classifier.Encoder.forward" href="#laueimproc.nn.vae_spot_classifier.Encoder.forward">forward</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.Encoder.parent" href="#laueimproc.nn.vae_spot_classifier.Encoder.parent">parent</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.Encoder.plot_latent" href="#laueimproc.nn.vae_spot_classifier.Encoder.plot_latent">plot_latent</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier">VAESpotClassifier</a></code></h4>
<ul class="two-column">
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.dataaug" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.dataaug">dataaug</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.device" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.device">device</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.forward" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.forward">forward</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.latent_dim" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.latent_dim">latent_dim</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.loss" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.loss">loss</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.normalization" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.normalization">normalization</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.plot_autoencode" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.plot_autoencode">plot_autoencode</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.scan_data" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.scan_data">scan_data</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.shape" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.shape">shape</a></code></li>
<li><code><a title="laueimproc.nn.vae_spot_classifier.VAESpotClassifier.space" href="#laueimproc.nn.vae_spot_classifier.VAESpotClassifier.space">space</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>