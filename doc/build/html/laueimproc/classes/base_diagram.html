<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>laueimproc.classes.base_diagram API documentation</title>
<meta name="description" content="Define the pytonic structure of a basic Diagram." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>laueimproc.classes.base_diagram</code></h1>
</header>
<section id="section-intro">
<p>Define the pytonic structure of a basic Diagram.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

&#34;&#34;&#34;Define the pytonic structure of a basic Diagram.&#34;&#34;&#34;

import hashlib
import math
import numbers
import pathlib
import pickle
import re
import sys
import threading
import typing
import warnings

from matplotlib.axes import Axes
from matplotlib.figure import Figure
import numpy as np
import torch

from laueimproc.common import bytes2human
from laueimproc.improc.peaks_search import peaks_search
from laueimproc.io.read import read_image, to_floattensor
from laueimproc.opti.cache import auto_cache, getsizeof
from laueimproc.opti.comp import compress_rois, decompress_rois
from laueimproc.opti.manager import DiagramManager
from .spot import Spot



class BaseDiagram:
    &#34;&#34;&#34;A Basic diagram with the fondamental structure.

    Attributes
    ----------
    bboxes : torch.Tensor or None
        The tensor of the bounding boxes (anchor_i, anchor_j, height, width)
        for each spots, of shape (n, 4) (readonly).
        Return None until spots are initialized.
    centers : torch.Tensor or None
        The tensor of the centers for each roi, of shape (n, 2) (readonly).
        The tensor is of type int, so if the size is odd, the middle is rounded down.
        Return None until spots are initialized.
    file : pathlib.Path or None
        The absolute file path to the image if provided, None otherwise (readonly).
    history : list[str]
        The actions performed on the Diagram from the initialisation (readonly).
    image : torch.Tensor
        The complete brut image of the diagram (readonly).
    rois : torch.Tensor or None
        The tensor of the regions of interest for each spots (readonly).
        For writing, use `self.spots = ...`.
        Return None until spots are initialized. The shape is (n, h, w).
    spots : list[laueimproc.classes.spot.Spot] or None
        All the spots contained in this diagram (read and write).
        Return None until spots are initialized.
    &#34;&#34;&#34;

    def __init__(
        self,
        data: typing.Union[np.ndarray, torch.Tensor, str, bytes, pathlib.Path],
        *,
        _check: bool = True,
    ):
        &#34;&#34;&#34;Create a new diagram with appropriated metadata.

        Parameters
        ----------
        data : pathlike or arraylike
            The filename or the array/tensor use as a diagram.
            For memory management, it is better to provide a pathlike rather than an array.
        &#34;&#34;&#34;
        # declaration
        self._cache: dict[str] = {}  # contains the optional cached data
        self._cache_lock = threading.Lock()  # make the cache acces thread safe
        self._file_or_data: typing.Union[pathlib.Path, torch.Tensor]  # the path to the image file
        self._find_spots_kwargs: typing.Optional[dict] = None  # the kwargs
        self._history: list[str] = []  # the history of the actions performed
        self._rois: typing.Union[None, torch.Tensor, bytes] = None  # rois are undeletable
        self._rois_lock = threading.Lock()  # make the rois compression thread safe
        self._spots: typing.Optional[list[Spot]] = None  # the spots of the diagram

        # initialisation
        if isinstance(data, (str, bytes, pathlib.Path)):
            self._file_or_data = data
            if _check:
                self._file_or_data = pathlib.Path(self._file_or_data).expanduser().resolve()
                assert self._file_or_data.is_file(), self._file_or_data
        else:
            warnings.warn(&#34;please provide a path rather than an array&#34;, RuntimeWarning)
            self._file_or_data = to_floattensor(data)
            assert self._file_or_data.ndim == 2, self._file_or_data.shape

        # track
        DiagramManager().add_diagram(self)

    def __getstate__(self, cache: bool = False):
        &#34;&#34;&#34;Make the object pickleable.&#34;&#34;&#34;
        if self._spots is None:
            spots_no_diagram = None
        else:
            spots_no_diagram = [
                Spot.__new__(Spot).__setstate__(s.__getstate__(cache=cache))
                for s in self._spots
            ]
            for spot in spots_no_diagram:
                spot._diagram = None  # to avoid cyclic reference
        with self._rois_lock:
            with self._cache_lock:
                if cache:
                    return (
                        self._file_or_data,
                        self._find_spots_kwargs,
                        self._history,
                        self._rois,
                        spots_no_diagram,
                        self._cache.copy(),
                    )
            return (
                self._file_or_data,
                self._find_spots_kwargs,
                self._history,
                self._rois,
                spots_no_diagram,
            )

    def __setstate__(self, state: tuple):
        &#34;&#34;&#34;Fill the internal attributes.

        Usefull for pickle.

        Notes
        -----
        * No verification is made because the user is not supposed to call this method.
        * Return self by ease.
        &#34;&#34;&#34;
        (
            self._file_or_data,
            self._find_spots_kwargs,
            self._history,
            self._rois,
            self._spots,
        ) = state[:5]
        if self._spots is not None:
            for spot in self._spots:
                spot._diagram = self
        self._cache = state[5] if len(state) == 6 else {}
        self._cache_lock = threading.Lock()
        self._rois_lock = threading.Lock()
        DiagramManager().add_diagram(self)

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;Return a nice sumary of the history of this diagram.&#34;&#34;&#34;

        # title
        if isinstance(self._file_or_data, pathlib.Path):
            text = f&#34;Diagram from {self._file_or_data.name}:&#34;
        else:
            text = f&#34;Diagram from Tensor of id {id(self._file_or_data)}:&#34;

        # history
        if self.spots:
            text += &#34;\n    History:&#34;
            for i, history in enumerate(self._history):
                text += f&#34;\n        {i+1}. {history}&#34;
        else:
            text += &#34;\n    History empty, please initialize the spots `self.find_spots()`.&#34;

        # stats
        text += &#34;\n    Current state:&#34;
        text += f&#34;\n        * id, state: {id(self)}, {self.state}&#34;
        if self.is_init():
            text += f&#34;\n        * nbr spots: {len(self.get_spots(_copy=False))}&#34;
        with self._cache_lock, self._rois_lock:
            size = sys.getsizeof(self) + sum(getsizeof(e) for e in self.__dict__.values())
        text += f&#34;\n        * total mem: {bytes2human(size)}&#34;
        return text

    def _find_spots(self, **kwargs):
        &#34;&#34;&#34;Real version of `find_spots`.&#34;&#34;&#34;
        # peaks search
        rois, bboxes = peaks_search(self.image, **kwargs)

        # cast into spots objects
        self._rois = rois  # not lock because autoblock
        self._spots = [
            Spot.__new__(Spot).__setstate__((self, index, bbox))
            for index, bbox in enumerate(bboxes.tolist())
        ]
        self._history = [f&#34;{len(self._spots)} spots from self.find_spots(...)&#34;]

    def _set_spots_from_anchors_rois(
        self, anchors: torch.Tensor, rois: list[torch.Tensor], _check: bool = True
    ):
        &#34;&#34;&#34;Set the new spots from anchors and region of interest.

        Parameters
        ----------
        bboxes : np.ndarray[int]
            The tensor of the bounding boxes of the spots.
        &#34;&#34;&#34;
        if anchors.shape[0]:
            bboxes = torch.tensor(
                [(i, j, *roi.shape) for (i, j), roi in zip(anchors.tolist(), rois)], dtype=int
            )
        else:
            bboxes = torch.empty((0, 4), dtype=int)
        self._set_spots_from_bboxes(bboxes, _check=_check)
        image = self.image
        with self._rois_lock:
            self._rois = torch.zeros( # zeros 2 times faster than empty + fill 0
                (
                    len(rois),
                    torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                    torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                ),
                dtype=image.dtype,
                device=image.device,
            )
            for index, (roi, (height, width)) in enumerate(zip(rois, bboxes[:, 2:].tolist())):
                self._rois[index, :height, :width] = roi
        self._history = [f&#34;{len(self._spots)} spots from external anchors and rois&#34;]
        self._find_spots_kwargs = None

    def _set_spots_from_bboxes(self, bboxes: torch.Tensor, _check: bool = True) -&gt; None:
        &#34;&#34;&#34;Set the new spots from bboxes, in a cleared diagram.

        Parameters
        ----------
        bboxes : np.ndarray[int]
            The tensor of the bounding boxes of the spots.
        &#34;&#34;&#34;
        if _check:
            image = self.image
            selection = bboxes[:, 0] &lt; 0  # overflow on left
            selection = torch.logical_or(selection, bboxes[:, 1] &lt; 0, out=selection)  # on top
            selection = torch.logical_or(  # on right
                selection, bboxes[:, 0] + bboxes[:, 2] &gt; image.shape[0], out=selection
            )
            selection = torch.logical_or(  # on bottom
                selection, bboxes[:, 0] + bboxes[:, 2] &gt; image.shape[0], out=selection
            )
            if nbr := torch.sum(selection.to(int)).item():
                warnings.warn(f&#34;{nbr} bboxes protrude the image, they are removed&#34;, RuntimeWarning)
                bboxes = bboxes[~selection]  # del all overflow bboxes
        self._spots = [
            Spot.__new__(Spot).__setstate__((self, index, (i, j, h, w)))
            for index, (i, j, h, w) in enumerate(bboxes.tolist())
        ]
        with self._rois_lock:
            self._rois = None
        self._history = [f&#34;{len(self._spots)} spots from external bboxes&#34;]
        self._find_spots_kwargs = None

    def _set_spots_from_spots(self, new_spots: list[Spot], _check: bool = True) -&gt; None:
        &#34;&#34;&#34;Set the new spots, in a cleared diagram (not cleard for self._rois).

        Parameters
        ----------
        new_spots : list[Spot]
            All the new spots to set, they can to comes from any diagram.
        &#34;&#34;&#34;
        # verification
        image = self.image
        if _check:  # very slow
            new_spots_ = [
                s for s in new_spots
                if (
                    s.bbox[0] &gt;= 0  # overflow on left
                    and s.bbox[1] &gt;= 0   # on top
                    and s.bbox[0]+s.bbox[2] &lt;= image.shape[0]  # on right
                    and s.bbox[1]+s.bbox[3] &lt;= image.shape[1]  # on bottom
                )
            ]
            if nbr := len(new_spots) - len(new_spots_):
                warnings.warn(f&#34;{nbr} spots protrude the image, they are removed&#34;, RuntimeWarning)
                new_spots = new_spots_

        # set new spots
        rois = [  # extract rois before change index and reset self._roi in case of ref to self
            s.roi for s in new_spots
        ]
        for index, spot in enumerate(new_spots):
            spot._diagram, spot._index = self, index
        self._spots = new_spots
        bboxes = self.bboxes  # reachable because self._spots is defined
        with self._rois_lock:
            self._rois = torch.zeros(  # zeros 2 times faster than empty + fill 0
                (
                    bboxes.shape[0],
                    torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                    torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                ),
                dtype=image.dtype,
                device=image.device,
            )
            for index, (roi, (height, width)) in enumerate(zip(rois, bboxes[:, 2:].tolist())):
                self._rois[index, :height, :width] = roi
        self._history = [f&#34;{len(self._spots)} spots from external spots&#34;]
        self._find_spots_kwargs = None

    @property
    def bboxes(self) -&gt; typing.Union[None, torch.Tensor]:
        &#34;&#34;&#34;Return the tensor of the bounding boxes (anchor_i, anchor_j, height, width).&#34;&#34;&#34;
        @auto_cache
        def _compute_bboxes(self) -&gt; torch.Tensor:
            &#34;&#34;&#34;Helper for `self.bboxes`.&#34;&#34;&#34;
            if self.spots:
                return torch.tensor([s.bbox for s in self.spots], dtype=int)
            return torch.empty((0, 4), dtype=int)
        if not self.is_init():
            return None
        return _compute_bboxes(self)

    @property
    def centers(self) -&gt; typing.Union[None, torch.Tensor]:  # very fast -&gt; no cache
        &#34;&#34;&#34;Return the tensor of the centers for each roi.&#34;&#34;&#34;
        if not self.is_init():
            return None
        if self.spots:
            bboxes = self.bboxes
            return bboxes[:, :2] + bboxes[:, 2:]//2
        return torch.empty((0, 2), dtype=int)

    def clone(self, deep: bool = True, cache: bool = True):
        &#34;&#34;&#34;Instanciate a new identical diagram.

        Parameters
        ----------
        deep : boolean, default=True
            If True, the memory of the new created diagram object is totally
            independ of this object (slow but safe). Otherwise, `image` and `rois` attributes
            and cached big data share the same memory view (Tensor). So modifying one of these
            attributes in one diagram will modify the same attribute in the other diagram.
            It if realy faster but not safe.
        cache : boolean, default=True
            Copy the cache into the new diagram if True (default), or live the cache empty if False.
        &#34;&#34;&#34;
        assert isinstance(deep, bool), deep.__class__.__name__
        assert isinstance(cache, bool), cache.__class__.__name__

        state = self.__getstate__(cache=cache)
        if deep:
            state = pickle.loads(pickle.dumps(state))
        new_diagram = self.__class__.__new__(self.__class__)  # create a new diagram
        new_diagram.__setstate__(state)  # initialise (fill) the new diagram
        return new_diagram

    def compress(self, size: numbers.Real = math.inf, *, _levels: set[int] = None) -&gt; int:
        &#34;&#34;&#34;Delete or compress attributes and elements in the cache.

        Paremeters
        ----------
        size : int
            The quantity of bytes to remove from the cache.

        Returns
        -------
        removed : int
            The number of bytes removed from the cache.
        &#34;&#34;&#34;
        # verifiactions
        assert isinstance(size, numbers.Real), size.__class__.__name__
        assert size &gt; 0, size

        _levels = _levels or {0, 1, 2}

        # declaration
        removed = 0

        # delete obsolete cache
        if 0 in _levels:
            pattern = r&#34;(?P&lt;state&gt;[0-9a-f]{32})\.\w+\([0-9a-f]{32}\)&#34;
            state = self.state
            with self._cache_lock:
                for key in list(self._cache):  # copy keys for pop
                    if (match := re.search(pattern, key)) is None:
                        continue
                    if match[&#34;state&#34;] != state:
                        removed += sys.getsizeof(key) + getsizeof(self._cache.pop(key))

        # delete valid cache
        if 1 in _levels:
            with self._cache_lock:
                size_to_key = {getsizeof(v): k for k, v in self._cache.items()}
                for item_size in sorted(size_to_key, reverse=True):  # delete biggest elements first
                    key = size_to_key[item_size]
                    removed += sys.getsizeof(key) + getsizeof(self._cache.pop(key))
                    if removed &gt;= size:
                        return removed

        # compress undeletable cache
        if 2 in _levels:
            with self._rois_lock:
                if isinstance(self._rois, torch.Tensor):
                    removed += getsizeof(self._rois)
                    self._rois = compress_rois(self._rois)
                    removed -= sys.getsizeof(self._rois)
                    if removed &gt;= size:
                        return removed

        return removed

    @property
    def file(self) -&gt; typing.Union[None, pathlib.Path]:
        &#34;&#34;&#34;Return the absolute file path to the image, if it is provided.&#34;&#34;&#34;
        if isinstance(self._file_or_data, pathlib.Path):
            return self._file_or_data
        return None

    def filter_spots(
        self, indexs: typing.Container, msg: str = &#34;general filter&#34;, *, inplace: bool = False
    ):
        &#34;&#34;&#34;Keep only the given spots, delete the rest.

        This method can be used for filtering or sorting spots.

        Parameters
        ----------
        indexs : arraylike
            The list of the indexs of the spots to keep
            or the boolean vector with True for keeping the spot, False otherwise like a mask.
        msg : str
            The message to happend to the history.
        inplace : boolean, default=True
            If True, modify the diagram self (no copy) and return a reference to self.
            If False, first clone the diagram, then apply the selection on the new diagram,
            It create a checkpoint (real copy) (but it is slowler).

        Returns
        -------
        filtered_diagram : BaseDiagram
            Return self if inplace is True or a modified clone of self otherwise.
        &#34;&#34;&#34;
        # verifications and cast
        if not self.is_init():
            raise RuntimeWarning(
                &#34;you must to initialize the spots (`self.find_spots()`) before to filter it&#34;
            )
        assert hasattr(indexs, &#34;__iter__&#34;), indexs.__class__.__name__
        assert isinstance(msg, str), msg.__class__.__name__
        assert isinstance(inplace, bool), inplace.__class__.__name__
        indexs = torch.as_tensor(indexs)
        indexs = torch.squeeze(indexs)
        assert indexs.ndim == 1, f&#34;only a 1d vector is accepted, shape is {indexs.shape}&#34;
        if indexs.dtype is torch.bool:  # case mask -&gt; convert into index list
            assert indexs.shape[0] == len(self.spots), (
                &#34;the mask has to have the same length as the number of spots, &#34;
                f&#34;there are {len(self.spots)} spots and mask is of len {indexs.shape[0]}&#34;
            )
            indexs = torch.arange(len(self.spots))[indexs]  # bool -&gt; indexs
        else:
            # assert len(set(indexs.tolist())) == indexs.shape[0], \  # very slow !
            #     &#34;each index must be unique, but some of them appear more than once&#34;
            assert not indexs.shape[0] or torch.min(indexs).item() &gt;= 0, \
                &#34;negative index is not allowed&#34;
            assert not indexs.shape[0] or torch.max(indexs).item() &lt; len(self.spots), \
                &#34;some indexes are out of range&#34;

        # manage inplace
        nb_spots = len(self.spots)
        if not inplace:
            self = self.clone()  # pylint: disable=W0642

        # update history, it has to be done before changing state to be catched by signature
        self._history.append(f&#34;{nb_spots} to {len(indexs)} spots: {msg}&#34;)

        # filter the spots
        self._spots = [self._spots[new_index] for new_index in indexs.tolist()]
        for new_index, spot in enumerate(self._spots):
            spot._index = new_index  # pylint: disable=W0212

        # filter the rois
        rois = self.rois[indexs]
        if len(indexs):
            bboxes = self.bboxes
            height = torch.max(bboxes[:, 2]).item()
            width = torch.max(bboxes[:, 3]).item()
            rois = rois[:, :height, :width]
        with self._rois_lock:
            self._rois = rois

        return None if inplace else self

    def find_spots(self, **kwargs) -&gt; None:
        &#34;&#34;&#34;Search all the spots in this diagram, store the result into the `spots` attribute.

        Parameters
        ----------
        kwargs : dict
            Transmitted to ``laueimproc.improc.peaks_search.peaks_search``.
        &#34;&#34;&#34;
        self._find_spots_kwargs = kwargs
        self._history = [&#34;x spots from self.find_spots(...)&#34;]
        with self._rois_lock:
            self._rois = None
        self._spots = None

    def get_spots(self, *, _copy: bool = True) -&gt; list[Spot]:
        &#34;&#34;&#34;Return the spots in a unordered set container.&#34;&#34;&#34;
        if self._spots is None and self._find_spots_kwargs is not None:
            self._find_spots(**self._find_spots_kwargs)
        if self._spots is None:  # case spots are never been searched
            return None
        # copy is slow but it is a strong protection against the user
        return self._spots.copy() if _copy else self._spots

    @property
    def history(self) -&gt; list[str]:
        &#34;&#34;&#34;Return the actions performed on the Diagram from the initialisation.&#34;&#34;&#34;
        if not self.is_init():
            return []
        self.get_spots(_copy=False)  # update hystory (first step init)
        return self._history.copy()  # copy for user protection

    def is_init(self) -&gt; bool:
        &#34;&#34;&#34;Return True if the diagram has been initialized.&#34;&#34;&#34;
        return self._spots is not None or self._find_spots_kwargs is not None

    @property
    def image(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return the complete image of the diagram.&#34;&#34;&#34;
        with self._cache_lock:
            if &#34;image&#34; not in self._cache:  # no auto ache because it is state invariant
                self._cache[&#34;image&#34;] = (
                    read_image(self._file_or_data)
                    if isinstance(self._file_or_data, pathlib.Path) else
                    self._file_or_data
                )
            return self._cache[&#34;image&#34;]

    def plot(
        self,
        disp: typing.Optional[typing.Union[Figure, Axes]] = None,
        vmin: typing.Optional[numbers.Real] = None,
        vmax: typing.Optional[numbers.Real] = None,
    ) -&gt; Axes:
        &#34;&#34;&#34;Prepare for display the diagram and the spots.

        Parameters
        ----------
        disp : matplotlib.figure.Figure or matplotlib.axes.Axes
            The matplotlib figure to complete.
        vmin : float, optional
            The minimum intensity ploted.
        vmax : float, optional
            The maximum intensity ploted.

        Notes
        -----
        It doesn&#39;t create the figure and call show.
        Use `self.show()` to Display the diagram from scratch.
        &#34;&#34;&#34;
        assert disp is None or isinstance(disp, (Figure, Axes))
        image = self.image
        if vmin is None:
            vmin = torch.min(image).item()
        assert vmin is None or isinstance(vmin, numbers.Real), vmin.__class__.__name__
        if vmax is None:
            vmax = torch.mean(image).item() + 5.0*torch.std(image).item()
        assert isinstance(vmax, numbers.Real), vmax.__class__.__name__

        # fill figure metadata
        axes = disp  # is gonna changed
        disp = disp or Figure(layout=&#34;tight&#34;)
        if isinstance(disp, Figure):
            if isinstance(self._file_or_data, pathlib.Path):
                disp.suptitle(f&#34;Diagram {self._file_or_data.name}&#34;)
            else:
                disp.suptitle(f&#34;Diagram from Tensor of id {id(self._file_or_data)}&#34;)
            axes = disp.add_subplot()

        # fill axes
        axes.set_ylabel(&#34;i (first axis)&#34;)
        axes.set_xlabel(&#34;j (second axis)&#34;)
        axes.imshow(
            image.numpy(force=True).transpose(),
            aspect=&#34;equal&#34;,
            extent=(0, self.image.shape[1], self.image.shape[0], 0),  # origin to corner of pxl
            interpolation=None,  # antialiasing is True
            norm=&#34;log&#34;,
            cmap=&#34;gray&#34;,
            vmin=vmin,
            vmax=vmax,
        )
        if self.spots:
            bboxes = self.bboxes.numpy(force=True)
            axes.plot(
                np.vstack((
                    bboxes[:, 0],
                    bboxes[:, 0]+bboxes[:, 2],
                    bboxes[:, 0]+bboxes[:, 2],
                    bboxes[:, 0],
                    bboxes[:, 0],
                )),
                np.vstack((
                    bboxes[:, 1],
                    bboxes[:, 1],
                    bboxes[:, 1]+bboxes[:, 3],
                    bboxes[:, 1]+bboxes[:, 3],
                    bboxes[:, 1],
                )),
                color=&#34;blue&#34;,
                scalex=False,
                scaley=False,
            )
        return axes

    @property
    def rois(self) -&gt; typing.Union[None, torch.Tensor]:
        &#34;&#34;&#34;Return the tensor of the rois of the spots.&#34;&#34;&#34;
        if not self.is_init():
            return None
        with self._rois_lock:
            if self._rois is None:
                image = self.image
                bboxes = self.bboxes
                self._rois = torch.zeros(  # zeros 2 times faster than empty + fill 0
                    (
                        bboxes.shape[0],
                        torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                        torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                    ),
                    dtype=image.dtype,
                    device=image.device,
                )
                for index, (i, j, height, width) in enumerate(bboxes.tolist()):
                    self._rois[index, :height, :width] = image[i:i+height, j:j+width]
            if isinstance(self._rois, bytes):
                self._rois = decompress_rois(self._rois)  # take 6 ms in average
                # return decompress_rois(self._rois)  # better no store decompress for sparse memory
            return self._rois

    def set_spots(self, new_spots: typing.Container) -&gt; None:
        &#34;&#34;&#34;Set the new spots as the current spots, reset the history and the cache.

        Paremeters
        ----------
        new_spots : typing.Container
            * Can be an iterable (list, tuple, set, ...) of `Spot` instances.
            * Can be an arraylike of bounding boxes n * (anchor_i, anchor_j, height, width)
            * Can be a tuple of anchors (arraylike of shape n * (anchor_i, anchor_i))
                and rois (iterable of images patches).
            * Can be None for reset all (same as calling the `reset` method)
        &#34;&#34;&#34;
        assert hasattr(new_spots, &#34;__iter__&#34;), new_spots.__class__.__name__

        # clear history and internal state (self._rois is cleared later)
        with self._cache_lock:
            self._cache = {}
        self._history = []
        self._spots = None

        # case tensor
        if isinstance(new_spots, np.ndarray):
            new_spots = new_spots.from_numpy(new_spots)
        if isinstance(new_spots, torch.Tensor):
            if new_spots.dtype.is_floating_point:
                new_spots = new_spots.to(int)
            if new_spots.ndim == 2 and new_spots.shape[1] == 4:  # case from bounding boxes
                return self._set_spots_from_bboxes(new_spots)

            raise NotImplementedError(
                f&#34;impossible to set new spots from an array of shape {new_spots.shape}, &#34;
                &#34;if has to be of shape (n, 4) for bounding boxes&#34;
            )

        # preparation of data
        if not isinstance(new_spots, list):
            new_spots = list(new_spots)  # freeze the order

        # case from Spot instances
        cls = {s.__class__ for s in new_spots}
        if not cls or (len(cls) == 1 and next(iter(cls)) is Spot):  # catch empty case
            return self._set_spots_from_spots(new_spots)  # reset self._rois

        # case from anchors, rois
        if (
            len(new_spots) == 2
            and isinstance(new_spots[0], (list, tuple, np.ndarray, torch.Tensor))
            and isinstance(new_spots[1], (list, tuple))
            and len(new_spots[0]) == len(new_spots[1])
        ):
            if not isinstance(new_spots[0], torch.Tensor):  # no torch.as_array memory leak
                new_spots[0] = torch.as_tensor(new_spots[0], dtype=int)
            anchors = new_spots[0]
            rois = [to_floattensor(roi) for roi in new_spots[1]]
            return self._set_spots_from_anchors_rois(anchors, rois)

        # case tensor (recursive delegation)
        if len(cls) == 1 and issubclass(next(iter(cls)), (list, tuple)):
            return self.set_spots(torch.tensor(new_spots, dtype=int))

        raise NotImplementedError(
            f&#34;impossible to set new spots from {new_spots}, &#34;
            &#34;it has to be a container of Spot instance &#34;
            &#34;or a container of bounding boxes&#34;
        )

    @property
    def spots(self) -&gt; typing.Union[list[Spot]]:
        &#34;&#34;&#34;Alias to the `get_spots`.&#34;&#34;&#34;
        return self.get_spots()

    @spots.setter
    def spots(self, new_spots: typing.Container):
        &#34;&#34;&#34;Alias to ``set_spots``.&#34;&#34;&#34;
        self.set_spots(new_spots)

    @property
    def state(self) -&gt; str:
        &#34;&#34;&#34;Return a hash of the diagram.

        If two diagrams gots the same state, it means they are the same.
        The hash take in consideration the internal state of the diagram.
        The retruned value is a hexadecimal strinf of length 32
        &#34;&#34;&#34;
        hasher = hashlib.md5(usedforsecurity=False)
        if isinstance(self._file_or_data, pathlib.Path):
            hasher.update(str(self._file_or_data.name).encode())
        else:
            hasher.update(id(self._file_or_data).to_bytes(8, &#34;big&#34;))
        hasher.update(str(self._find_spots_kwargs).encode())
        hasher.update(&#34;\n&#34;.join(self._history[1:]).encode())
        return hasher.hexdigest()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="laueimproc.classes.base_diagram.BaseDiagram"><code class="flex name class">
<span>class <span class="ident">BaseDiagram</span></span>
<span>(</span><span>data: Union[numpy.ndarray, torch.Tensor, str, bytes, pathlib.Path])</span>
</code></dt>
<dd>
<div class="desc"><p>A Basic diagram with the fondamental structure.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>bboxes</code></strong> :&ensp;<code>torch.Tensor</code> or <code>None</code></dt>
<dd>The tensor of the bounding boxes (anchor_i, anchor_j, height, width)
for each spots, of shape (n, 4) (readonly).
Return None until spots are initialized.</dd>
<dt><strong><code>centers</code></strong> :&ensp;<code>torch.Tensor</code> or <code>None</code></dt>
<dd>The tensor of the centers for each roi, of shape (n, 2) (readonly).
The tensor is of type int, so if the size is odd, the middle is rounded down.
Return None until spots are initialized.</dd>
<dt><strong><code>file</code></strong> :&ensp;<code>pathlib.Path</code> or <code>None</code></dt>
<dd>The absolute file path to the image if provided, None otherwise (readonly).</dd>
<dt><strong><code>history</code></strong> :&ensp;<code>list[str]</code></dt>
<dd>The actions performed on the Diagram from the initialisation (readonly).</dd>
<dt><strong><code>image</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The complete brut image of the diagram (readonly).</dd>
<dt><strong><code>rois</code></strong> :&ensp;<code>torch.Tensor</code> or <code>None</code></dt>
<dd>The tensor of the regions of interest for each spots (readonly).
For writing, use <code>self.spots = ...</code>.
Return None until spots are initialized. The shape is (n, h, w).</dd>
<dt><strong><code>spots</code></strong> :&ensp;<code>list[<a title="laueimproc.classes.spot.Spot" href="spot.html#laueimproc.classes.spot.Spot">Spot</a>]</code> or <code>None</code></dt>
<dd>All the spots contained in this diagram (read and write).
Return None until spots are initialized.</dd>
</dl>
<p>Create a new diagram with appropriated metadata.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong> :&ensp;<code>pathlike</code> or <code>arraylike</code></dt>
<dd>The filename or the array/tensor use as a diagram.
For memory management, it is better to provide a pathlike rather than an array.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseDiagram:
    &#34;&#34;&#34;A Basic diagram with the fondamental structure.

    Attributes
    ----------
    bboxes : torch.Tensor or None
        The tensor of the bounding boxes (anchor_i, anchor_j, height, width)
        for each spots, of shape (n, 4) (readonly).
        Return None until spots are initialized.
    centers : torch.Tensor or None
        The tensor of the centers for each roi, of shape (n, 2) (readonly).
        The tensor is of type int, so if the size is odd, the middle is rounded down.
        Return None until spots are initialized.
    file : pathlib.Path or None
        The absolute file path to the image if provided, None otherwise (readonly).
    history : list[str]
        The actions performed on the Diagram from the initialisation (readonly).
    image : torch.Tensor
        The complete brut image of the diagram (readonly).
    rois : torch.Tensor or None
        The tensor of the regions of interest for each spots (readonly).
        For writing, use `self.spots = ...`.
        Return None until spots are initialized. The shape is (n, h, w).
    spots : list[laueimproc.classes.spot.Spot] or None
        All the spots contained in this diagram (read and write).
        Return None until spots are initialized.
    &#34;&#34;&#34;

    def __init__(
        self,
        data: typing.Union[np.ndarray, torch.Tensor, str, bytes, pathlib.Path],
        *,
        _check: bool = True,
    ):
        &#34;&#34;&#34;Create a new diagram with appropriated metadata.

        Parameters
        ----------
        data : pathlike or arraylike
            The filename or the array/tensor use as a diagram.
            For memory management, it is better to provide a pathlike rather than an array.
        &#34;&#34;&#34;
        # declaration
        self._cache: dict[str] = {}  # contains the optional cached data
        self._cache_lock = threading.Lock()  # make the cache acces thread safe
        self._file_or_data: typing.Union[pathlib.Path, torch.Tensor]  # the path to the image file
        self._find_spots_kwargs: typing.Optional[dict] = None  # the kwargs
        self._history: list[str] = []  # the history of the actions performed
        self._rois: typing.Union[None, torch.Tensor, bytes] = None  # rois are undeletable
        self._rois_lock = threading.Lock()  # make the rois compression thread safe
        self._spots: typing.Optional[list[Spot]] = None  # the spots of the diagram

        # initialisation
        if isinstance(data, (str, bytes, pathlib.Path)):
            self._file_or_data = data
            if _check:
                self._file_or_data = pathlib.Path(self._file_or_data).expanduser().resolve()
                assert self._file_or_data.is_file(), self._file_or_data
        else:
            warnings.warn(&#34;please provide a path rather than an array&#34;, RuntimeWarning)
            self._file_or_data = to_floattensor(data)
            assert self._file_or_data.ndim == 2, self._file_or_data.shape

        # track
        DiagramManager().add_diagram(self)

    def __getstate__(self, cache: bool = False):
        &#34;&#34;&#34;Make the object pickleable.&#34;&#34;&#34;
        if self._spots is None:
            spots_no_diagram = None
        else:
            spots_no_diagram = [
                Spot.__new__(Spot).__setstate__(s.__getstate__(cache=cache))
                for s in self._spots
            ]
            for spot in spots_no_diagram:
                spot._diagram = None  # to avoid cyclic reference
        with self._rois_lock:
            with self._cache_lock:
                if cache:
                    return (
                        self._file_or_data,
                        self._find_spots_kwargs,
                        self._history,
                        self._rois,
                        spots_no_diagram,
                        self._cache.copy(),
                    )
            return (
                self._file_or_data,
                self._find_spots_kwargs,
                self._history,
                self._rois,
                spots_no_diagram,
            )

    def __setstate__(self, state: tuple):
        &#34;&#34;&#34;Fill the internal attributes.

        Usefull for pickle.

        Notes
        -----
        * No verification is made because the user is not supposed to call this method.
        * Return self by ease.
        &#34;&#34;&#34;
        (
            self._file_or_data,
            self._find_spots_kwargs,
            self._history,
            self._rois,
            self._spots,
        ) = state[:5]
        if self._spots is not None:
            for spot in self._spots:
                spot._diagram = self
        self._cache = state[5] if len(state) == 6 else {}
        self._cache_lock = threading.Lock()
        self._rois_lock = threading.Lock()
        DiagramManager().add_diagram(self)

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;Return a nice sumary of the history of this diagram.&#34;&#34;&#34;

        # title
        if isinstance(self._file_or_data, pathlib.Path):
            text = f&#34;Diagram from {self._file_or_data.name}:&#34;
        else:
            text = f&#34;Diagram from Tensor of id {id(self._file_or_data)}:&#34;

        # history
        if self.spots:
            text += &#34;\n    History:&#34;
            for i, history in enumerate(self._history):
                text += f&#34;\n        {i+1}. {history}&#34;
        else:
            text += &#34;\n    History empty, please initialize the spots `self.find_spots()`.&#34;

        # stats
        text += &#34;\n    Current state:&#34;
        text += f&#34;\n        * id, state: {id(self)}, {self.state}&#34;
        if self.is_init():
            text += f&#34;\n        * nbr spots: {len(self.get_spots(_copy=False))}&#34;
        with self._cache_lock, self._rois_lock:
            size = sys.getsizeof(self) + sum(getsizeof(e) for e in self.__dict__.values())
        text += f&#34;\n        * total mem: {bytes2human(size)}&#34;
        return text

    def _find_spots(self, **kwargs):
        &#34;&#34;&#34;Real version of `find_spots`.&#34;&#34;&#34;
        # peaks search
        rois, bboxes = peaks_search(self.image, **kwargs)

        # cast into spots objects
        self._rois = rois  # not lock because autoblock
        self._spots = [
            Spot.__new__(Spot).__setstate__((self, index, bbox))
            for index, bbox in enumerate(bboxes.tolist())
        ]
        self._history = [f&#34;{len(self._spots)} spots from self.find_spots(...)&#34;]

    def _set_spots_from_anchors_rois(
        self, anchors: torch.Tensor, rois: list[torch.Tensor], _check: bool = True
    ):
        &#34;&#34;&#34;Set the new spots from anchors and region of interest.

        Parameters
        ----------
        bboxes : np.ndarray[int]
            The tensor of the bounding boxes of the spots.
        &#34;&#34;&#34;
        if anchors.shape[0]:
            bboxes = torch.tensor(
                [(i, j, *roi.shape) for (i, j), roi in zip(anchors.tolist(), rois)], dtype=int
            )
        else:
            bboxes = torch.empty((0, 4), dtype=int)
        self._set_spots_from_bboxes(bboxes, _check=_check)
        image = self.image
        with self._rois_lock:
            self._rois = torch.zeros( # zeros 2 times faster than empty + fill 0
                (
                    len(rois),
                    torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                    torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                ),
                dtype=image.dtype,
                device=image.device,
            )
            for index, (roi, (height, width)) in enumerate(zip(rois, bboxes[:, 2:].tolist())):
                self._rois[index, :height, :width] = roi
        self._history = [f&#34;{len(self._spots)} spots from external anchors and rois&#34;]
        self._find_spots_kwargs = None

    def _set_spots_from_bboxes(self, bboxes: torch.Tensor, _check: bool = True) -&gt; None:
        &#34;&#34;&#34;Set the new spots from bboxes, in a cleared diagram.

        Parameters
        ----------
        bboxes : np.ndarray[int]
            The tensor of the bounding boxes of the spots.
        &#34;&#34;&#34;
        if _check:
            image = self.image
            selection = bboxes[:, 0] &lt; 0  # overflow on left
            selection = torch.logical_or(selection, bboxes[:, 1] &lt; 0, out=selection)  # on top
            selection = torch.logical_or(  # on right
                selection, bboxes[:, 0] + bboxes[:, 2] &gt; image.shape[0], out=selection
            )
            selection = torch.logical_or(  # on bottom
                selection, bboxes[:, 0] + bboxes[:, 2] &gt; image.shape[0], out=selection
            )
            if nbr := torch.sum(selection.to(int)).item():
                warnings.warn(f&#34;{nbr} bboxes protrude the image, they are removed&#34;, RuntimeWarning)
                bboxes = bboxes[~selection]  # del all overflow bboxes
        self._spots = [
            Spot.__new__(Spot).__setstate__((self, index, (i, j, h, w)))
            for index, (i, j, h, w) in enumerate(bboxes.tolist())
        ]
        with self._rois_lock:
            self._rois = None
        self._history = [f&#34;{len(self._spots)} spots from external bboxes&#34;]
        self._find_spots_kwargs = None

    def _set_spots_from_spots(self, new_spots: list[Spot], _check: bool = True) -&gt; None:
        &#34;&#34;&#34;Set the new spots, in a cleared diagram (not cleard for self._rois).

        Parameters
        ----------
        new_spots : list[Spot]
            All the new spots to set, they can to comes from any diagram.
        &#34;&#34;&#34;
        # verification
        image = self.image
        if _check:  # very slow
            new_spots_ = [
                s for s in new_spots
                if (
                    s.bbox[0] &gt;= 0  # overflow on left
                    and s.bbox[1] &gt;= 0   # on top
                    and s.bbox[0]+s.bbox[2] &lt;= image.shape[0]  # on right
                    and s.bbox[1]+s.bbox[3] &lt;= image.shape[1]  # on bottom
                )
            ]
            if nbr := len(new_spots) - len(new_spots_):
                warnings.warn(f&#34;{nbr} spots protrude the image, they are removed&#34;, RuntimeWarning)
                new_spots = new_spots_

        # set new spots
        rois = [  # extract rois before change index and reset self._roi in case of ref to self
            s.roi for s in new_spots
        ]
        for index, spot in enumerate(new_spots):
            spot._diagram, spot._index = self, index
        self._spots = new_spots
        bboxes = self.bboxes  # reachable because self._spots is defined
        with self._rois_lock:
            self._rois = torch.zeros(  # zeros 2 times faster than empty + fill 0
                (
                    bboxes.shape[0],
                    torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                    torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                ),
                dtype=image.dtype,
                device=image.device,
            )
            for index, (roi, (height, width)) in enumerate(zip(rois, bboxes[:, 2:].tolist())):
                self._rois[index, :height, :width] = roi
        self._history = [f&#34;{len(self._spots)} spots from external spots&#34;]
        self._find_spots_kwargs = None

    @property
    def bboxes(self) -&gt; typing.Union[None, torch.Tensor]:
        &#34;&#34;&#34;Return the tensor of the bounding boxes (anchor_i, anchor_j, height, width).&#34;&#34;&#34;
        @auto_cache
        def _compute_bboxes(self) -&gt; torch.Tensor:
            &#34;&#34;&#34;Helper for `self.bboxes`.&#34;&#34;&#34;
            if self.spots:
                return torch.tensor([s.bbox for s in self.spots], dtype=int)
            return torch.empty((0, 4), dtype=int)
        if not self.is_init():
            return None
        return _compute_bboxes(self)

    @property
    def centers(self) -&gt; typing.Union[None, torch.Tensor]:  # very fast -&gt; no cache
        &#34;&#34;&#34;Return the tensor of the centers for each roi.&#34;&#34;&#34;
        if not self.is_init():
            return None
        if self.spots:
            bboxes = self.bboxes
            return bboxes[:, :2] + bboxes[:, 2:]//2
        return torch.empty((0, 2), dtype=int)

    def clone(self, deep: bool = True, cache: bool = True):
        &#34;&#34;&#34;Instanciate a new identical diagram.

        Parameters
        ----------
        deep : boolean, default=True
            If True, the memory of the new created diagram object is totally
            independ of this object (slow but safe). Otherwise, `image` and `rois` attributes
            and cached big data share the same memory view (Tensor). So modifying one of these
            attributes in one diagram will modify the same attribute in the other diagram.
            It if realy faster but not safe.
        cache : boolean, default=True
            Copy the cache into the new diagram if True (default), or live the cache empty if False.
        &#34;&#34;&#34;
        assert isinstance(deep, bool), deep.__class__.__name__
        assert isinstance(cache, bool), cache.__class__.__name__

        state = self.__getstate__(cache=cache)
        if deep:
            state = pickle.loads(pickle.dumps(state))
        new_diagram = self.__class__.__new__(self.__class__)  # create a new diagram
        new_diagram.__setstate__(state)  # initialise (fill) the new diagram
        return new_diagram

    def compress(self, size: numbers.Real = math.inf, *, _levels: set[int] = None) -&gt; int:
        &#34;&#34;&#34;Delete or compress attributes and elements in the cache.

        Paremeters
        ----------
        size : int
            The quantity of bytes to remove from the cache.

        Returns
        -------
        removed : int
            The number of bytes removed from the cache.
        &#34;&#34;&#34;
        # verifiactions
        assert isinstance(size, numbers.Real), size.__class__.__name__
        assert size &gt; 0, size

        _levels = _levels or {0, 1, 2}

        # declaration
        removed = 0

        # delete obsolete cache
        if 0 in _levels:
            pattern = r&#34;(?P&lt;state&gt;[0-9a-f]{32})\.\w+\([0-9a-f]{32}\)&#34;
            state = self.state
            with self._cache_lock:
                for key in list(self._cache):  # copy keys for pop
                    if (match := re.search(pattern, key)) is None:
                        continue
                    if match[&#34;state&#34;] != state:
                        removed += sys.getsizeof(key) + getsizeof(self._cache.pop(key))

        # delete valid cache
        if 1 in _levels:
            with self._cache_lock:
                size_to_key = {getsizeof(v): k for k, v in self._cache.items()}
                for item_size in sorted(size_to_key, reverse=True):  # delete biggest elements first
                    key = size_to_key[item_size]
                    removed += sys.getsizeof(key) + getsizeof(self._cache.pop(key))
                    if removed &gt;= size:
                        return removed

        # compress undeletable cache
        if 2 in _levels:
            with self._rois_lock:
                if isinstance(self._rois, torch.Tensor):
                    removed += getsizeof(self._rois)
                    self._rois = compress_rois(self._rois)
                    removed -= sys.getsizeof(self._rois)
                    if removed &gt;= size:
                        return removed

        return removed

    @property
    def file(self) -&gt; typing.Union[None, pathlib.Path]:
        &#34;&#34;&#34;Return the absolute file path to the image, if it is provided.&#34;&#34;&#34;
        if isinstance(self._file_or_data, pathlib.Path):
            return self._file_or_data
        return None

    def filter_spots(
        self, indexs: typing.Container, msg: str = &#34;general filter&#34;, *, inplace: bool = False
    ):
        &#34;&#34;&#34;Keep only the given spots, delete the rest.

        This method can be used for filtering or sorting spots.

        Parameters
        ----------
        indexs : arraylike
            The list of the indexs of the spots to keep
            or the boolean vector with True for keeping the spot, False otherwise like a mask.
        msg : str
            The message to happend to the history.
        inplace : boolean, default=True
            If True, modify the diagram self (no copy) and return a reference to self.
            If False, first clone the diagram, then apply the selection on the new diagram,
            It create a checkpoint (real copy) (but it is slowler).

        Returns
        -------
        filtered_diagram : BaseDiagram
            Return self if inplace is True or a modified clone of self otherwise.
        &#34;&#34;&#34;
        # verifications and cast
        if not self.is_init():
            raise RuntimeWarning(
                &#34;you must to initialize the spots (`self.find_spots()`) before to filter it&#34;
            )
        assert hasattr(indexs, &#34;__iter__&#34;), indexs.__class__.__name__
        assert isinstance(msg, str), msg.__class__.__name__
        assert isinstance(inplace, bool), inplace.__class__.__name__
        indexs = torch.as_tensor(indexs)
        indexs = torch.squeeze(indexs)
        assert indexs.ndim == 1, f&#34;only a 1d vector is accepted, shape is {indexs.shape}&#34;
        if indexs.dtype is torch.bool:  # case mask -&gt; convert into index list
            assert indexs.shape[0] == len(self.spots), (
                &#34;the mask has to have the same length as the number of spots, &#34;
                f&#34;there are {len(self.spots)} spots and mask is of len {indexs.shape[0]}&#34;
            )
            indexs = torch.arange(len(self.spots))[indexs]  # bool -&gt; indexs
        else:
            # assert len(set(indexs.tolist())) == indexs.shape[0], \  # very slow !
            #     &#34;each index must be unique, but some of them appear more than once&#34;
            assert not indexs.shape[0] or torch.min(indexs).item() &gt;= 0, \
                &#34;negative index is not allowed&#34;
            assert not indexs.shape[0] or torch.max(indexs).item() &lt; len(self.spots), \
                &#34;some indexes are out of range&#34;

        # manage inplace
        nb_spots = len(self.spots)
        if not inplace:
            self = self.clone()  # pylint: disable=W0642

        # update history, it has to be done before changing state to be catched by signature
        self._history.append(f&#34;{nb_spots} to {len(indexs)} spots: {msg}&#34;)

        # filter the spots
        self._spots = [self._spots[new_index] for new_index in indexs.tolist()]
        for new_index, spot in enumerate(self._spots):
            spot._index = new_index  # pylint: disable=W0212

        # filter the rois
        rois = self.rois[indexs]
        if len(indexs):
            bboxes = self.bboxes
            height = torch.max(bboxes[:, 2]).item()
            width = torch.max(bboxes[:, 3]).item()
            rois = rois[:, :height, :width]
        with self._rois_lock:
            self._rois = rois

        return None if inplace else self

    def find_spots(self, **kwargs) -&gt; None:
        &#34;&#34;&#34;Search all the spots in this diagram, store the result into the `spots` attribute.

        Parameters
        ----------
        kwargs : dict
            Transmitted to ``laueimproc.improc.peaks_search.peaks_search``.
        &#34;&#34;&#34;
        self._find_spots_kwargs = kwargs
        self._history = [&#34;x spots from self.find_spots(...)&#34;]
        with self._rois_lock:
            self._rois = None
        self._spots = None

    def get_spots(self, *, _copy: bool = True) -&gt; list[Spot]:
        &#34;&#34;&#34;Return the spots in a unordered set container.&#34;&#34;&#34;
        if self._spots is None and self._find_spots_kwargs is not None:
            self._find_spots(**self._find_spots_kwargs)
        if self._spots is None:  # case spots are never been searched
            return None
        # copy is slow but it is a strong protection against the user
        return self._spots.copy() if _copy else self._spots

    @property
    def history(self) -&gt; list[str]:
        &#34;&#34;&#34;Return the actions performed on the Diagram from the initialisation.&#34;&#34;&#34;
        if not self.is_init():
            return []
        self.get_spots(_copy=False)  # update hystory (first step init)
        return self._history.copy()  # copy for user protection

    def is_init(self) -&gt; bool:
        &#34;&#34;&#34;Return True if the diagram has been initialized.&#34;&#34;&#34;
        return self._spots is not None or self._find_spots_kwargs is not None

    @property
    def image(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Return the complete image of the diagram.&#34;&#34;&#34;
        with self._cache_lock:
            if &#34;image&#34; not in self._cache:  # no auto ache because it is state invariant
                self._cache[&#34;image&#34;] = (
                    read_image(self._file_or_data)
                    if isinstance(self._file_or_data, pathlib.Path) else
                    self._file_or_data
                )
            return self._cache[&#34;image&#34;]

    def plot(
        self,
        disp: typing.Optional[typing.Union[Figure, Axes]] = None,
        vmin: typing.Optional[numbers.Real] = None,
        vmax: typing.Optional[numbers.Real] = None,
    ) -&gt; Axes:
        &#34;&#34;&#34;Prepare for display the diagram and the spots.

        Parameters
        ----------
        disp : matplotlib.figure.Figure or matplotlib.axes.Axes
            The matplotlib figure to complete.
        vmin : float, optional
            The minimum intensity ploted.
        vmax : float, optional
            The maximum intensity ploted.

        Notes
        -----
        It doesn&#39;t create the figure and call show.
        Use `self.show()` to Display the diagram from scratch.
        &#34;&#34;&#34;
        assert disp is None or isinstance(disp, (Figure, Axes))
        image = self.image
        if vmin is None:
            vmin = torch.min(image).item()
        assert vmin is None or isinstance(vmin, numbers.Real), vmin.__class__.__name__
        if vmax is None:
            vmax = torch.mean(image).item() + 5.0*torch.std(image).item()
        assert isinstance(vmax, numbers.Real), vmax.__class__.__name__

        # fill figure metadata
        axes = disp  # is gonna changed
        disp = disp or Figure(layout=&#34;tight&#34;)
        if isinstance(disp, Figure):
            if isinstance(self._file_or_data, pathlib.Path):
                disp.suptitle(f&#34;Diagram {self._file_or_data.name}&#34;)
            else:
                disp.suptitle(f&#34;Diagram from Tensor of id {id(self._file_or_data)}&#34;)
            axes = disp.add_subplot()

        # fill axes
        axes.set_ylabel(&#34;i (first axis)&#34;)
        axes.set_xlabel(&#34;j (second axis)&#34;)
        axes.imshow(
            image.numpy(force=True).transpose(),
            aspect=&#34;equal&#34;,
            extent=(0, self.image.shape[1], self.image.shape[0], 0),  # origin to corner of pxl
            interpolation=None,  # antialiasing is True
            norm=&#34;log&#34;,
            cmap=&#34;gray&#34;,
            vmin=vmin,
            vmax=vmax,
        )
        if self.spots:
            bboxes = self.bboxes.numpy(force=True)
            axes.plot(
                np.vstack((
                    bboxes[:, 0],
                    bboxes[:, 0]+bboxes[:, 2],
                    bboxes[:, 0]+bboxes[:, 2],
                    bboxes[:, 0],
                    bboxes[:, 0],
                )),
                np.vstack((
                    bboxes[:, 1],
                    bboxes[:, 1],
                    bboxes[:, 1]+bboxes[:, 3],
                    bboxes[:, 1]+bboxes[:, 3],
                    bboxes[:, 1],
                )),
                color=&#34;blue&#34;,
                scalex=False,
                scaley=False,
            )
        return axes

    @property
    def rois(self) -&gt; typing.Union[None, torch.Tensor]:
        &#34;&#34;&#34;Return the tensor of the rois of the spots.&#34;&#34;&#34;
        if not self.is_init():
            return None
        with self._rois_lock:
            if self._rois is None:
                image = self.image
                bboxes = self.bboxes
                self._rois = torch.zeros(  # zeros 2 times faster than empty + fill 0
                    (
                        bboxes.shape[0],
                        torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                        torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                    ),
                    dtype=image.dtype,
                    device=image.device,
                )
                for index, (i, j, height, width) in enumerate(bboxes.tolist()):
                    self._rois[index, :height, :width] = image[i:i+height, j:j+width]
            if isinstance(self._rois, bytes):
                self._rois = decompress_rois(self._rois)  # take 6 ms in average
                # return decompress_rois(self._rois)  # better no store decompress for sparse memory
            return self._rois

    def set_spots(self, new_spots: typing.Container) -&gt; None:
        &#34;&#34;&#34;Set the new spots as the current spots, reset the history and the cache.

        Paremeters
        ----------
        new_spots : typing.Container
            * Can be an iterable (list, tuple, set, ...) of `Spot` instances.
            * Can be an arraylike of bounding boxes n * (anchor_i, anchor_j, height, width)
            * Can be a tuple of anchors (arraylike of shape n * (anchor_i, anchor_i))
                and rois (iterable of images patches).
            * Can be None for reset all (same as calling the `reset` method)
        &#34;&#34;&#34;
        assert hasattr(new_spots, &#34;__iter__&#34;), new_spots.__class__.__name__

        # clear history and internal state (self._rois is cleared later)
        with self._cache_lock:
            self._cache = {}
        self._history = []
        self._spots = None

        # case tensor
        if isinstance(new_spots, np.ndarray):
            new_spots = new_spots.from_numpy(new_spots)
        if isinstance(new_spots, torch.Tensor):
            if new_spots.dtype.is_floating_point:
                new_spots = new_spots.to(int)
            if new_spots.ndim == 2 and new_spots.shape[1] == 4:  # case from bounding boxes
                return self._set_spots_from_bboxes(new_spots)

            raise NotImplementedError(
                f&#34;impossible to set new spots from an array of shape {new_spots.shape}, &#34;
                &#34;if has to be of shape (n, 4) for bounding boxes&#34;
            )

        # preparation of data
        if not isinstance(new_spots, list):
            new_spots = list(new_spots)  # freeze the order

        # case from Spot instances
        cls = {s.__class__ for s in new_spots}
        if not cls or (len(cls) == 1 and next(iter(cls)) is Spot):  # catch empty case
            return self._set_spots_from_spots(new_spots)  # reset self._rois

        # case from anchors, rois
        if (
            len(new_spots) == 2
            and isinstance(new_spots[0], (list, tuple, np.ndarray, torch.Tensor))
            and isinstance(new_spots[1], (list, tuple))
            and len(new_spots[0]) == len(new_spots[1])
        ):
            if not isinstance(new_spots[0], torch.Tensor):  # no torch.as_array memory leak
                new_spots[0] = torch.as_tensor(new_spots[0], dtype=int)
            anchors = new_spots[0]
            rois = [to_floattensor(roi) for roi in new_spots[1]]
            return self._set_spots_from_anchors_rois(anchors, rois)

        # case tensor (recursive delegation)
        if len(cls) == 1 and issubclass(next(iter(cls)), (list, tuple)):
            return self.set_spots(torch.tensor(new_spots, dtype=int))

        raise NotImplementedError(
            f&#34;impossible to set new spots from {new_spots}, &#34;
            &#34;it has to be a container of Spot instance &#34;
            &#34;or a container of bounding boxes&#34;
        )

    @property
    def spots(self) -&gt; typing.Union[list[Spot]]:
        &#34;&#34;&#34;Alias to the `get_spots`.&#34;&#34;&#34;
        return self.get_spots()

    @spots.setter
    def spots(self, new_spots: typing.Container):
        &#34;&#34;&#34;Alias to ``set_spots``.&#34;&#34;&#34;
        self.set_spots(new_spots)

    @property
    def state(self) -&gt; str:
        &#34;&#34;&#34;Return a hash of the diagram.

        If two diagrams gots the same state, it means they are the same.
        The hash take in consideration the internal state of the diagram.
        The retruned value is a hexadecimal strinf of length 32
        &#34;&#34;&#34;
        hasher = hashlib.md5(usedforsecurity=False)
        if isinstance(self._file_or_data, pathlib.Path):
            hasher.update(str(self._file_or_data.name).encode())
        else:
            hasher.update(id(self._file_or_data).to_bytes(8, &#34;big&#34;))
        hasher.update(str(self._find_spots_kwargs).encode())
        hasher.update(&#34;\n&#34;.join(self._history[1:]).encode())
        return hasher.hexdigest()</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="laueimproc.classes.diagram.Diagram" href="diagram.html#laueimproc.classes.diagram.Diagram">Diagram</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.bboxes"><code class="name">var <span class="ident">bboxes</span> : Optional[None]</code></dt>
<dd>
<div class="desc"><p>Return the tensor of the bounding boxes (anchor_i, anchor_j, height, width).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def bboxes(self) -&gt; typing.Union[None, torch.Tensor]:
    &#34;&#34;&#34;Return the tensor of the bounding boxes (anchor_i, anchor_j, height, width).&#34;&#34;&#34;
    @auto_cache
    def _compute_bboxes(self) -&gt; torch.Tensor:
        &#34;&#34;&#34;Helper for `self.bboxes`.&#34;&#34;&#34;
        if self.spots:
            return torch.tensor([s.bbox for s in self.spots], dtype=int)
        return torch.empty((0, 4), dtype=int)
    if not self.is_init():
        return None
    return _compute_bboxes(self)</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.centers"><code class="name">var <span class="ident">centers</span> : Optional[None]</code></dt>
<dd>
<div class="desc"><p>Return the tensor of the centers for each roi.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def centers(self) -&gt; typing.Union[None, torch.Tensor]:  # very fast -&gt; no cache
    &#34;&#34;&#34;Return the tensor of the centers for each roi.&#34;&#34;&#34;
    if not self.is_init():
        return None
    if self.spots:
        bboxes = self.bboxes
        return bboxes[:, :2] + bboxes[:, 2:]//2
    return torch.empty((0, 2), dtype=int)</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.file"><code class="name">var <span class="ident">file</span> : Optional[None]</code></dt>
<dd>
<div class="desc"><p>Return the absolute file path to the image, if it is provided.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def file(self) -&gt; typing.Union[None, pathlib.Path]:
    &#34;&#34;&#34;Return the absolute file path to the image, if it is provided.&#34;&#34;&#34;
    if isinstance(self._file_or_data, pathlib.Path):
        return self._file_or_data
    return None</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.history"><code class="name">var <span class="ident">history</span> : list[str]</code></dt>
<dd>
<div class="desc"><p>Return the actions performed on the Diagram from the initialisation.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def history(self) -&gt; list[str]:
    &#34;&#34;&#34;Return the actions performed on the Diagram from the initialisation.&#34;&#34;&#34;
    if not self.is_init():
        return []
    self.get_spots(_copy=False)  # update hystory (first step init)
    return self._history.copy()  # copy for user protection</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.image"><code class="name">var <span class="ident">image</span> : torch.Tensor</code></dt>
<dd>
<div class="desc"><p>Return the complete image of the diagram.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def image(self) -&gt; torch.Tensor:
    &#34;&#34;&#34;Return the complete image of the diagram.&#34;&#34;&#34;
    with self._cache_lock:
        if &#34;image&#34; not in self._cache:  # no auto ache because it is state invariant
            self._cache[&#34;image&#34;] = (
                read_image(self._file_or_data)
                if isinstance(self._file_or_data, pathlib.Path) else
                self._file_or_data
            )
        return self._cache[&#34;image&#34;]</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.rois"><code class="name">var <span class="ident">rois</span> : Optional[None]</code></dt>
<dd>
<div class="desc"><p>Return the tensor of the rois of the spots.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def rois(self) -&gt; typing.Union[None, torch.Tensor]:
    &#34;&#34;&#34;Return the tensor of the rois of the spots.&#34;&#34;&#34;
    if not self.is_init():
        return None
    with self._rois_lock:
        if self._rois is None:
            image = self.image
            bboxes = self.bboxes
            self._rois = torch.zeros(  # zeros 2 times faster than empty + fill 0
                (
                    bboxes.shape[0],
                    torch.max(bboxes[:, 2]).item() if bboxes.shape[0] else 1,
                    torch.max(bboxes[:, 3]).item() if bboxes.shape[0] else 1,
                ),
                dtype=image.dtype,
                device=image.device,
            )
            for index, (i, j, height, width) in enumerate(bboxes.tolist()):
                self._rois[index, :height, :width] = image[i:i+height, j:j+width]
        if isinstance(self._rois, bytes):
            self._rois = decompress_rois(self._rois)  # take 6 ms in average
            # return decompress_rois(self._rois)  # better no store decompress for sparse memory
        return self._rois</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.spots"><code class="name">var <span class="ident">spots</span> : list[<a title="laueimproc.classes.spot.Spot" href="spot.html#laueimproc.classes.spot.Spot">Spot</a>]</code></dt>
<dd>
<div class="desc"><p>Alias to the <code>get_spots</code>.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def spots(self) -&gt; typing.Union[list[Spot]]:
    &#34;&#34;&#34;Alias to the `get_spots`.&#34;&#34;&#34;
    return self.get_spots()</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.state"><code class="name">var <span class="ident">state</span> : str</code></dt>
<dd>
<div class="desc"><p>Return a hash of the diagram.</p>
<p>If two diagrams gots the same state, it means they are the same.
The hash take in consideration the internal state of the diagram.
The retruned value is a hexadecimal strinf of length 32</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def state(self) -&gt; str:
    &#34;&#34;&#34;Return a hash of the diagram.

    If two diagrams gots the same state, it means they are the same.
    The hash take in consideration the internal state of the diagram.
    The retruned value is a hexadecimal strinf of length 32
    &#34;&#34;&#34;
    hasher = hashlib.md5(usedforsecurity=False)
    if isinstance(self._file_or_data, pathlib.Path):
        hasher.update(str(self._file_or_data.name).encode())
    else:
        hasher.update(id(self._file_or_data).to_bytes(8, &#34;big&#34;))
    hasher.update(str(self._find_spots_kwargs).encode())
    hasher.update(&#34;\n&#34;.join(self._history[1:]).encode())
    return hasher.hexdigest()</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.clone"><code class="name flex">
<span>def <span class="ident">clone</span></span>(<span>self, deep: bool = True, cache: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><p>Instanciate a new identical diagram.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>deep</code></strong> :&ensp;<code>boolean</code>, default=<code>True</code></dt>
<dd>If True, the memory of the new created diagram object is totally
independ of this object (slow but safe). Otherwise, <code>image</code> and <code><a title="rois" href="../../rois.html">rois</a></code> attributes
and cached big data share the same memory view (Tensor). So modifying one of these
attributes in one diagram will modify the same attribute in the other diagram.
It if realy faster but not safe.</dd>
<dt><strong><code>cache</code></strong> :&ensp;<code>boolean</code>, default=<code>True</code></dt>
<dd>Copy the cache into the new diagram if True (default), or live the cache empty if False.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def clone(self, deep: bool = True, cache: bool = True):
    &#34;&#34;&#34;Instanciate a new identical diagram.

    Parameters
    ----------
    deep : boolean, default=True
        If True, the memory of the new created diagram object is totally
        independ of this object (slow but safe). Otherwise, `image` and `rois` attributes
        and cached big data share the same memory view (Tensor). So modifying one of these
        attributes in one diagram will modify the same attribute in the other diagram.
        It if realy faster but not safe.
    cache : boolean, default=True
        Copy the cache into the new diagram if True (default), or live the cache empty if False.
    &#34;&#34;&#34;
    assert isinstance(deep, bool), deep.__class__.__name__
    assert isinstance(cache, bool), cache.__class__.__name__

    state = self.__getstate__(cache=cache)
    if deep:
        state = pickle.loads(pickle.dumps(state))
    new_diagram = self.__class__.__new__(self.__class__)  # create a new diagram
    new_diagram.__setstate__(state)  # initialise (fill) the new diagram
    return new_diagram</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.compress"><code class="name flex">
<span>def <span class="ident">compress</span></span>(<span>self, size: numbers.Real = inf) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Delete or compress attributes and elements in the cache.</p>
<h2 id="paremeters">Paremeters</h2>
<p>size : int
The quantity of bytes to remove from the cache.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>removed</code></strong> :&ensp;<code>int</code></dt>
<dd>The number of bytes removed from the cache.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress(self, size: numbers.Real = math.inf, *, _levels: set[int] = None) -&gt; int:
    &#34;&#34;&#34;Delete or compress attributes and elements in the cache.

    Paremeters
    ----------
    size : int
        The quantity of bytes to remove from the cache.

    Returns
    -------
    removed : int
        The number of bytes removed from the cache.
    &#34;&#34;&#34;
    # verifiactions
    assert isinstance(size, numbers.Real), size.__class__.__name__
    assert size &gt; 0, size

    _levels = _levels or {0, 1, 2}

    # declaration
    removed = 0

    # delete obsolete cache
    if 0 in _levels:
        pattern = r&#34;(?P&lt;state&gt;[0-9a-f]{32})\.\w+\([0-9a-f]{32}\)&#34;
        state = self.state
        with self._cache_lock:
            for key in list(self._cache):  # copy keys for pop
                if (match := re.search(pattern, key)) is None:
                    continue
                if match[&#34;state&#34;] != state:
                    removed += sys.getsizeof(key) + getsizeof(self._cache.pop(key))

    # delete valid cache
    if 1 in _levels:
        with self._cache_lock:
            size_to_key = {getsizeof(v): k for k, v in self._cache.items()}
            for item_size in sorted(size_to_key, reverse=True):  # delete biggest elements first
                key = size_to_key[item_size]
                removed += sys.getsizeof(key) + getsizeof(self._cache.pop(key))
                if removed &gt;= size:
                    return removed

    # compress undeletable cache
    if 2 in _levels:
        with self._rois_lock:
            if isinstance(self._rois, torch.Tensor):
                removed += getsizeof(self._rois)
                self._rois = compress_rois(self._rois)
                removed -= sys.getsizeof(self._rois)
                if removed &gt;= size:
                    return removed

    return removed</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.filter_spots"><code class="name flex">
<span>def <span class="ident">filter_spots</span></span>(<span>self, indexs: Container, msg: str = 'general filter', *, inplace: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Keep only the given spots, delete the rest.</p>
<p>This method can be used for filtering or sorting spots.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>indexs</code></strong> :&ensp;<code>arraylike</code></dt>
<dd>The list of the indexs of the spots to keep
or the boolean vector with True for keeping the spot, False otherwise like a mask.</dd>
<dt><strong><code>msg</code></strong> :&ensp;<code>str</code></dt>
<dd>The message to happend to the history.</dd>
<dt><strong><code>inplace</code></strong> :&ensp;<code>boolean</code>, default=<code>True</code></dt>
<dd>If True, modify the diagram self (no copy) and return a reference to self.
If False, first clone the diagram, then apply the selection on the new diagram,
It create a checkpoint (real copy) (but it is slowler).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>filtered_diagram</code></strong> :&ensp;<code><a title="laueimproc.classes.base_diagram.BaseDiagram" href="#laueimproc.classes.base_diagram.BaseDiagram">BaseDiagram</a></code></dt>
<dd>Return self if inplace is True or a modified clone of self otherwise.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_spots(
    self, indexs: typing.Container, msg: str = &#34;general filter&#34;, *, inplace: bool = False
):
    &#34;&#34;&#34;Keep only the given spots, delete the rest.

    This method can be used for filtering or sorting spots.

    Parameters
    ----------
    indexs : arraylike
        The list of the indexs of the spots to keep
        or the boolean vector with True for keeping the spot, False otherwise like a mask.
    msg : str
        The message to happend to the history.
    inplace : boolean, default=True
        If True, modify the diagram self (no copy) and return a reference to self.
        If False, first clone the diagram, then apply the selection on the new diagram,
        It create a checkpoint (real copy) (but it is slowler).

    Returns
    -------
    filtered_diagram : BaseDiagram
        Return self if inplace is True or a modified clone of self otherwise.
    &#34;&#34;&#34;
    # verifications and cast
    if not self.is_init():
        raise RuntimeWarning(
            &#34;you must to initialize the spots (`self.find_spots()`) before to filter it&#34;
        )
    assert hasattr(indexs, &#34;__iter__&#34;), indexs.__class__.__name__
    assert isinstance(msg, str), msg.__class__.__name__
    assert isinstance(inplace, bool), inplace.__class__.__name__
    indexs = torch.as_tensor(indexs)
    indexs = torch.squeeze(indexs)
    assert indexs.ndim == 1, f&#34;only a 1d vector is accepted, shape is {indexs.shape}&#34;
    if indexs.dtype is torch.bool:  # case mask -&gt; convert into index list
        assert indexs.shape[0] == len(self.spots), (
            &#34;the mask has to have the same length as the number of spots, &#34;
            f&#34;there are {len(self.spots)} spots and mask is of len {indexs.shape[0]}&#34;
        )
        indexs = torch.arange(len(self.spots))[indexs]  # bool -&gt; indexs
    else:
        # assert len(set(indexs.tolist())) == indexs.shape[0], \  # very slow !
        #     &#34;each index must be unique, but some of them appear more than once&#34;
        assert not indexs.shape[0] or torch.min(indexs).item() &gt;= 0, \
            &#34;negative index is not allowed&#34;
        assert not indexs.shape[0] or torch.max(indexs).item() &lt; len(self.spots), \
            &#34;some indexes are out of range&#34;

    # manage inplace
    nb_spots = len(self.spots)
    if not inplace:
        self = self.clone()  # pylint: disable=W0642

    # update history, it has to be done before changing state to be catched by signature
    self._history.append(f&#34;{nb_spots} to {len(indexs)} spots: {msg}&#34;)

    # filter the spots
    self._spots = [self._spots[new_index] for new_index in indexs.tolist()]
    for new_index, spot in enumerate(self._spots):
        spot._index = new_index  # pylint: disable=W0212

    # filter the rois
    rois = self.rois[indexs]
    if len(indexs):
        bboxes = self.bboxes
        height = torch.max(bboxes[:, 2]).item()
        width = torch.max(bboxes[:, 3]).item()
        rois = rois[:, :height, :width]
    with self._rois_lock:
        self._rois = rois

    return None if inplace else self</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.find_spots"><code class="name flex">
<span>def <span class="ident">find_spots</span></span>(<span>self, **kwargs) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Search all the spots in this diagram, store the result into the <code>spots</code> attribute.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kwargs</code></strong> :&ensp;<code>dict</code></dt>
<dd>Transmitted to <code><a title="laueimproc.improc.peaks_search.peaks_search" href="../improc/peaks_search.html#laueimproc.improc.peaks_search.peaks_search">peaks_search()</a></code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_spots(self, **kwargs) -&gt; None:
    &#34;&#34;&#34;Search all the spots in this diagram, store the result into the `spots` attribute.

    Parameters
    ----------
    kwargs : dict
        Transmitted to ``laueimproc.improc.peaks_search.peaks_search``.
    &#34;&#34;&#34;
    self._find_spots_kwargs = kwargs
    self._history = [&#34;x spots from self.find_spots(...)&#34;]
    with self._rois_lock:
        self._rois = None
    self._spots = None</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.get_spots"><code class="name flex">
<span>def <span class="ident">get_spots</span></span>(<span>self) ‑> list[<a title="laueimproc.classes.spot.Spot" href="spot.html#laueimproc.classes.spot.Spot">Spot</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Return the spots in a unordered set container.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_spots(self, *, _copy: bool = True) -&gt; list[Spot]:
    &#34;&#34;&#34;Return the spots in a unordered set container.&#34;&#34;&#34;
    if self._spots is None and self._find_spots_kwargs is not None:
        self._find_spots(**self._find_spots_kwargs)
    if self._spots is None:  # case spots are never been searched
        return None
    # copy is slow but it is a strong protection against the user
    return self._spots.copy() if _copy else self._spots</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.is_init"><code class="name flex">
<span>def <span class="ident">is_init</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Return True if the diagram has been initialized.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_init(self) -&gt; bool:
    &#34;&#34;&#34;Return True if the diagram has been initialized.&#34;&#34;&#34;
    return self._spots is not None or self._find_spots_kwargs is not None</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, disp: Union[matplotlib.figure.Figure, matplotlib.axes._axes.Axes, ForwardRef(None)] = None, vmin: Optional[numbers.Real] = None, vmax: Optional[numbers.Real] = None) ‑> matplotlib.axes._axes.Axes</span>
</code></dt>
<dd>
<div class="desc"><p>Prepare for display the diagram and the spots.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>disp</code></strong> :&ensp;<code>matplotlib.figure.Figure</code> or <code>matplotlib.axes.Axes</code></dt>
<dd>The matplotlib figure to complete.</dd>
<dt><strong><code>vmin</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The minimum intensity ploted.</dd>
<dt><strong><code>vmax</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>The maximum intensity ploted.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>It doesn't create the figure and call show.
Use <code>self.show()</code> to Display the diagram from scratch.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(
    self,
    disp: typing.Optional[typing.Union[Figure, Axes]] = None,
    vmin: typing.Optional[numbers.Real] = None,
    vmax: typing.Optional[numbers.Real] = None,
) -&gt; Axes:
    &#34;&#34;&#34;Prepare for display the diagram and the spots.

    Parameters
    ----------
    disp : matplotlib.figure.Figure or matplotlib.axes.Axes
        The matplotlib figure to complete.
    vmin : float, optional
        The minimum intensity ploted.
    vmax : float, optional
        The maximum intensity ploted.

    Notes
    -----
    It doesn&#39;t create the figure and call show.
    Use `self.show()` to Display the diagram from scratch.
    &#34;&#34;&#34;
    assert disp is None or isinstance(disp, (Figure, Axes))
    image = self.image
    if vmin is None:
        vmin = torch.min(image).item()
    assert vmin is None or isinstance(vmin, numbers.Real), vmin.__class__.__name__
    if vmax is None:
        vmax = torch.mean(image).item() + 5.0*torch.std(image).item()
    assert isinstance(vmax, numbers.Real), vmax.__class__.__name__

    # fill figure metadata
    axes = disp  # is gonna changed
    disp = disp or Figure(layout=&#34;tight&#34;)
    if isinstance(disp, Figure):
        if isinstance(self._file_or_data, pathlib.Path):
            disp.suptitle(f&#34;Diagram {self._file_or_data.name}&#34;)
        else:
            disp.suptitle(f&#34;Diagram from Tensor of id {id(self._file_or_data)}&#34;)
        axes = disp.add_subplot()

    # fill axes
    axes.set_ylabel(&#34;i (first axis)&#34;)
    axes.set_xlabel(&#34;j (second axis)&#34;)
    axes.imshow(
        image.numpy(force=True).transpose(),
        aspect=&#34;equal&#34;,
        extent=(0, self.image.shape[1], self.image.shape[0], 0),  # origin to corner of pxl
        interpolation=None,  # antialiasing is True
        norm=&#34;log&#34;,
        cmap=&#34;gray&#34;,
        vmin=vmin,
        vmax=vmax,
    )
    if self.spots:
        bboxes = self.bboxes.numpy(force=True)
        axes.plot(
            np.vstack((
                bboxes[:, 0],
                bboxes[:, 0]+bboxes[:, 2],
                bboxes[:, 0]+bboxes[:, 2],
                bboxes[:, 0],
                bboxes[:, 0],
            )),
            np.vstack((
                bboxes[:, 1],
                bboxes[:, 1],
                bboxes[:, 1]+bboxes[:, 3],
                bboxes[:, 1]+bboxes[:, 3],
                bboxes[:, 1],
            )),
            color=&#34;blue&#34;,
            scalex=False,
            scaley=False,
        )
    return axes</code></pre>
</details>
</dd>
<dt id="laueimproc.classes.base_diagram.BaseDiagram.set_spots"><code class="name flex">
<span>def <span class="ident">set_spots</span></span>(<span>self, new_spots: Container) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Set the new spots as the current spots, reset the history and the cache.</p>
<h2 id="paremeters">Paremeters</h2>
<p>new_spots : typing.Container
* Can be an iterable (list, tuple, set, &hellip;) of <code>Spot</code> instances.
* Can be an arraylike of bounding boxes n * (anchor_i, anchor_j, height, width)
* Can be a tuple of anchors (arraylike of shape n * (anchor_i, anchor_i))
and rois (iterable of images patches).
* Can be None for reset all (same as calling the <code>reset</code> method)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_spots(self, new_spots: typing.Container) -&gt; None:
    &#34;&#34;&#34;Set the new spots as the current spots, reset the history and the cache.

    Paremeters
    ----------
    new_spots : typing.Container
        * Can be an iterable (list, tuple, set, ...) of `Spot` instances.
        * Can be an arraylike of bounding boxes n * (anchor_i, anchor_j, height, width)
        * Can be a tuple of anchors (arraylike of shape n * (anchor_i, anchor_i))
            and rois (iterable of images patches).
        * Can be None for reset all (same as calling the `reset` method)
    &#34;&#34;&#34;
    assert hasattr(new_spots, &#34;__iter__&#34;), new_spots.__class__.__name__

    # clear history and internal state (self._rois is cleared later)
    with self._cache_lock:
        self._cache = {}
    self._history = []
    self._spots = None

    # case tensor
    if isinstance(new_spots, np.ndarray):
        new_spots = new_spots.from_numpy(new_spots)
    if isinstance(new_spots, torch.Tensor):
        if new_spots.dtype.is_floating_point:
            new_spots = new_spots.to(int)
        if new_spots.ndim == 2 and new_spots.shape[1] == 4:  # case from bounding boxes
            return self._set_spots_from_bboxes(new_spots)

        raise NotImplementedError(
            f&#34;impossible to set new spots from an array of shape {new_spots.shape}, &#34;
            &#34;if has to be of shape (n, 4) for bounding boxes&#34;
        )

    # preparation of data
    if not isinstance(new_spots, list):
        new_spots = list(new_spots)  # freeze the order

    # case from Spot instances
    cls = {s.__class__ for s in new_spots}
    if not cls or (len(cls) == 1 and next(iter(cls)) is Spot):  # catch empty case
        return self._set_spots_from_spots(new_spots)  # reset self._rois

    # case from anchors, rois
    if (
        len(new_spots) == 2
        and isinstance(new_spots[0], (list, tuple, np.ndarray, torch.Tensor))
        and isinstance(new_spots[1], (list, tuple))
        and len(new_spots[0]) == len(new_spots[1])
    ):
        if not isinstance(new_spots[0], torch.Tensor):  # no torch.as_array memory leak
            new_spots[0] = torch.as_tensor(new_spots[0], dtype=int)
        anchors = new_spots[0]
        rois = [to_floattensor(roi) for roi in new_spots[1]]
        return self._set_spots_from_anchors_rois(anchors, rois)

    # case tensor (recursive delegation)
    if len(cls) == 1 and issubclass(next(iter(cls)), (list, tuple)):
        return self.set_spots(torch.tensor(new_spots, dtype=int))

    raise NotImplementedError(
        f&#34;impossible to set new spots from {new_spots}, &#34;
        &#34;it has to be a container of Spot instance &#34;
        &#34;or a container of bounding boxes&#34;
    )</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="laueimproc.classes" href="index.html">laueimproc.classes</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="laueimproc.classes.base_diagram.BaseDiagram" href="#laueimproc.classes.base_diagram.BaseDiagram">BaseDiagram</a></code></h4>
<ul class="two-column">
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.bboxes" href="#laueimproc.classes.base_diagram.BaseDiagram.bboxes">bboxes</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.centers" href="#laueimproc.classes.base_diagram.BaseDiagram.centers">centers</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.clone" href="#laueimproc.classes.base_diagram.BaseDiagram.clone">clone</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.compress" href="#laueimproc.classes.base_diagram.BaseDiagram.compress">compress</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.file" href="#laueimproc.classes.base_diagram.BaseDiagram.file">file</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.filter_spots" href="#laueimproc.classes.base_diagram.BaseDiagram.filter_spots">filter_spots</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.find_spots" href="#laueimproc.classes.base_diagram.BaseDiagram.find_spots">find_spots</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.get_spots" href="#laueimproc.classes.base_diagram.BaseDiagram.get_spots">get_spots</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.history" href="#laueimproc.classes.base_diagram.BaseDiagram.history">history</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.image" href="#laueimproc.classes.base_diagram.BaseDiagram.image">image</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.is_init" href="#laueimproc.classes.base_diagram.BaseDiagram.is_init">is_init</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.plot" href="#laueimproc.classes.base_diagram.BaseDiagram.plot">plot</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.rois" href="#laueimproc.classes.base_diagram.BaseDiagram.rois">rois</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.set_spots" href="#laueimproc.classes.base_diagram.BaseDiagram.set_spots">set_spots</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.spots" href="#laueimproc.classes.base_diagram.BaseDiagram.spots">spots</a></code></li>
<li><code><a title="laueimproc.classes.base_diagram.BaseDiagram.state" href="#laueimproc.classes.base_diagram.BaseDiagram.state">state</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>