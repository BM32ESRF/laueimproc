<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>laueimproc.io.comp_lm API documentation</title>
<meta name="description" content="Implement a convolutive generative variational auto-encoder neuronal network." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS_CHTML" integrity="sha256-kZafAc6mZvK3W3v1pHOcUix30OHQN6pU/NO2oFkqZVw=" crossorigin></script>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>laueimproc.io.comp_lm</code></h1>
</header>
<section id="section-intro">
<p>Implement a convolutive generative variational auto-encoder neuronal network.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python3

&#34;&#34;&#34;Implement a convolutive generative variational auto-encoder neuronal network.&#34;&#34;&#34;

import bz2
import pathlib
import pickle
import typing

import numpy as np
import torch
import tqdm

from laueimproc.classes.diagram import Diagram


class VariationalEncoder(torch.nn.Module):
    &#34;&#34;&#34;Projects images into a more compact space.

    Each patch of 320x320 pixels with a stride of 64 pixels
    is projected into a space of dimension 64, quantizable with 8 bits per component.
    &#34;&#34;&#34;

    def __init__(self):
        super().__init__()

        eta = 1.32  # (lat_dim/first_dim)**(1/nb_layers)

        self.pre = torch.nn.Sequential(
            torch.nn.Conv2d(1, 16, kernel_size=3, stride=1),
            torch.nn.ELU(),
        )
        self.encoder = torch.nn.Sequential(
            *(
                torch.nn.Sequential(
                    torch.nn.Conv2d(
                        round(16*eta**layer),
                        round(16*eta**(layer+1)),
                        kernel_size=6,
                        stride=2,
                        padding=2,
                        bias=False,
                    ),
                    torch.nn.BatchNorm2d(round(16*eta**(layer+1))),
                    torch.nn.ELU(),
                    torch.nn.Conv2d(
                        round(16*eta**(layer+1)),
                        round(16*eta**(layer+1)),
                        kernel_size=3,
                    ),
                    torch.nn.ELU(),
                )
                for layer in range(5)
            ),
        )
        self.post = torch.nn.Sequential(
            torch.nn.Conv2d(65, 64, kernel_size=3, padding=1),
            torch.nn.Sigmoid(),
        )

    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply the function on the images.

        Parameters
        ----------
        img : torch.Tensor
            The float image batch of shape (n, 1, h, w).
            With h and w &gt;= 160 + k*32, k &gt;= 0 integer.

        Returns
        -------
        lat : torch.Tensor
            The projection of the image in the latent space.
            New shape is (n, 256, h/32-4, w/32-4) with value in [0, 1].

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
        &gt;&gt;&gt; encoder = VariationalEncoder()
        &gt;&gt;&gt; encoder(torch.rand((5, 1, 160, 224))).shape
        torch.Size([5, 64, 1, 3])
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(img, torch.Tensor), img.__class__.__name__
        assert img.ndim == 4, img.shape
        assert img.shape[1] == 1, img.shape
        assert img.shape[2:] &gt;= (160, 160), img.shape
        assert img.shape[2] % 32 == 0, img.shape
        assert img.shape[3] % 32 == 0, img.shape
        assert img.dtype.is_floating_point, img.dtype

        mean = (
            torch.mean(img, dim=(2, 3), keepdim=True)
            .expand(-1, 1, img.shape[2]//32-4, img.shape[3]//32-4)
        )
        x = self.pre(img)
        x = self.encoder(x)
        lat = self.post(torch.cat((x, mean), dim=1))
        if self.training:
            lat = self.add_quantization_noise(lat)
        return lat

    @staticmethod
    def add_quantization_noise(lat: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Add a uniform noise in order to simulate the quantization into uint8.

        Parameters
        ----------
        lat : torch.Tensor
            The float lattent space of shape (n, 64, a, b) with value in range ]0, 1[.

        Returns
        -------
        noised_lat : torch.Tensor
            The input tensor with a aditive uniform noise U(-.5/255, .5/255).
            The finals values are clamped to stay in the range [0, 1].

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
        &gt;&gt;&gt; lat = torch.rand((5, 64, 1, 3))
        &gt;&gt;&gt; q_lat = VariationalEncoder.add_quantization_noise(lat)
        &gt;&gt;&gt; torch.all(abs(q_lat - lat) &lt;= 0.5/255)
        tensor(True)
        &gt;&gt;&gt; abs((q_lat - lat).mean().round(decimals=3))
        tensor(0.)
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(lat, torch.Tensor), lat.__class__.__name__
        assert lat.ndim == 4, lat.shape
        assert lat.shape[1] == 64, lat.shape
        assert lat.dtype.is_floating_point, lat.dtype

        noise = torch.rand_like(lat)/255
        noise -= 0.5/255
        out = lat + noise
        out = torch.clamp(out, min=0, max=1)
        return out


class Decoder(torch.nn.Module):
    &#34;&#34;&#34;Unfold the projected encoded images into the color space.&#34;&#34;&#34;

    def __init__(self):
        super().__init__()

        eta = 1.32

        self.pre = torch.nn.Sequential(
            torch.nn.ConstantPad2d(2, 0.5),
            torch.nn.Conv2d(66, 64, kernel_size=1),
            torch.nn.ELU(inplace=True),
        )
        self.decoder = torch.nn.Sequential(
            *(
                torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(
                        round(16*eta**layer),
                        round(16*eta**(layer-1)),
                        kernel_size=4,
                        stride=2,
                        padding=1,
                        bias=False,
                    ),
                    torch.nn.BatchNorm2d(round(16*eta**(layer-1))),
                    torch.nn.ELU(inplace=True),
                    torch.nn.Conv2d(
                        round(16*eta**(layer-1)),
                        round(16*eta**(layer-1)),
                        kernel_size=3,
                        padding=1,
                    ),
                    torch.nn.ELU(inplace=True),
                )
                for layer in range(5, 0, -1)
            ),
        )
        self.post = torch.nn.Sequential(
            torch.nn.Conv2d(16, 12, kernel_size=5, padding=2),
            torch.nn.ELU(inplace=True),
            torch.nn.Conv2d(12, 1, kernel_size=5, padding=2),
            torch.nn.Sigmoid(),
        )

    def forward(self, lat: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply the function on the latent images.

        Parameters
        ----------
        lat : torch.Tensor
            The projected image in the latent space of shape (n, 256, hl, wl).

        Returns
        -------
        img : torch.Tensor
            A close image in colorspace to the input image.
            It is as mutch bijective as possible than VariationalEncoder.
            New shape is (n, 256, 160+hl*32, 160+wl*32) with value in [0, 1].

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import Decoder
        &gt;&gt;&gt; decoder = Decoder()
        &gt;&gt;&gt; decoder(torch.rand((5, 64, 1, 3))).shape
        torch.Size([5, 1, 160, 224])
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(lat, torch.Tensor), lat.__class__.__name__
        assert lat.ndim == 4, lat.shape
        assert lat.shape[1] == 64, lat.shape
        assert lat.shape[2:] &gt;= (1, 1), lat.shape
        assert lat.dtype.is_floating_point, lat.dtype

        pos_h = torch.linspace(-1, 1, lat.shape[2], dtype=lat.dtype, device=lat.device)
        pos_w = torch.linspace(-1, 1, lat.shape[3], dtype=lat.dtype, device=lat.device)
        pos_h, pos_w = pos_h.reshape(1, 1, -1, 1), pos_w.reshape(1, 1, 1, -1)
        pos_h, pos_w = (
            pos_h.expand(len(lat), 1, *lat.shape[2:]),
            pos_w.expand(len(lat), 1, *lat.shape[2:]),
        )

        x = self.pre(torch.cat((lat, pos_h, pos_w), dim=1))
        x = self.decoder(x)
        out = self.post(x)
        return out


class LMCodec(torch.nn.Module):
    &#34;&#34;&#34;Encode and Decode Laue Max images.&#34;&#34;&#34;

    def __init__(self, weights: typing.Union[str, pathlib.Path]):
        &#34;&#34;&#34;Initialise the codec.

        Parameters
        ----------
        weights : pathlike
            The filename of the model data.
        &#34;&#34;&#34;
        assert isinstance(weights, (str, pathlib.Path)), weights.__class__.__name__
        weights = pathlib.Path(weights).expanduser().resolve()
        assert not weights.is_dir(), weights

        super().__init__()
        self.encoder = VariationalEncoder()
        self.decoder = Decoder()
        self._weights = weights

        if weights.is_file():
            checkpoint = torch.load(weights)
            self.encoder.load_state_dict(checkpoint[&#34;encoder&#34;])
            self.decoder.load_state_dict(checkpoint[&#34;decoder&#34;])
        else:
            torch.save(
                {&#34;encoder&#34;: self.encoder.state_dict(), &#34;decoder&#34;: self.decoder.state_dict()},
                weights,
            )

    def decode(
        self, data: typing.Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]
    ) -&gt; typing.Union[torch.Tensor, np.ndarray]:
        &#34;&#34;&#34;Decode le compressed content.

        Parameters
        ----------
        data
            The compact data representation.

        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
        &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;).eval()
        &gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
        &gt;&gt;&gt; data = codec.encode(img)
        &gt;&gt;&gt; decoded = codec.decode(data)
        &gt;&gt;&gt; (img == decoded).all()
        True
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        if isinstance(data, tuple):
            assert len(data) == 2, len(data)
            encoded, pad = data
            assert isinstance(encoded, torch.Tensor), encoded.__class__.__name__
            assert encoded.ndim == 4, encoded.shape
            assert encoded.shape[1] == 64, encoded.shape
            assert len(pad) == 4, len(pad)
            assert all(isinstance(p, int) for p in pad), pad
            padleft, padright, padtop, padbottom = pad
            img = self.decoder(encoded)
            img = img[:, :, padtop:img.shape[2]-padbottom, padleft:img.shape[3]-padright]
            return img

        assert isinstance(data, bytes), data.__class__.__name__
        data = bz2.decompress(data)
        lat_np, residu, (padleft, padright, padtop, padbottom) = pickle.loads(data)
        residu = (residu &gt;&gt; 1) * np.where(residu % 2, -1, 1)
        lat_torch = torch.from_numpy(lat_np).to(torch.float32) / 255
        pred_torch = self.decoder(lat_torch)
        pred_torch = (
            pred_torch
            [:, :, padtop:pred_torch.shape[2]-padbottom, padleft:pred_torch.shape[3]-padright]
        )
        pred_np = (pred_torch*65535 + 0.5).squeeze(0).squeeze(0).numpy(force=True).astype(np.int32)
        img_np = (residu + pred_np).astype(np.uint16)
        return img_np

    def encode(
        self, img: typing.Union[torch.Tensor, np.ndarray]
    ) -&gt; typing.Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]:
        &#34;&#34;&#34;Encode the image.

        Parameters
        ----------
        img : torch.Tensor or np.ndarray
            The 1 channel image to encode.

        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
        &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;).eval()
        &gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
        &gt;&gt;&gt; encoded = codec.encode(img)
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        if isinstance(img, torch.Tensor):
            assert img.ndim == 4, img.shape
            assert img.shape[1] == 1, img.shape

            # padding
            padtop = max(0, 128-img.shape[2])
            padtop += 32 - ((img.shape[2] + padtop) % 32)
            padleft = max(0, 1280-img.shape[3])
            padleft += 32 - ((img.shape[3] + padleft) % 32)
            padtop, padbottom = padtop//2, padtop - padtop//2
            padleft, padright = padleft//2, padleft - padleft//2
            img = torch.nn.functional.pad(img, (padleft, padright, padtop, padbottom))

            # transform
            return self.encoder(img), (padleft, padright, padtop, padbottom)

        # case numpy uint16
        assert isinstance(img, np.ndarray), img.__class__.__name__
        assert img.ndim == 2, img.shape
        assert img.dtype == np.uint16
        img_torch = torch.from_numpy(img.astype(np.float32)).unsqueeze(0).unsqueeze(0) / 65535
        lat_torch, pad = self.encode(img_torch)
        lat_np = (lat_torch*255 + 0.5).numpy(force=True).astype(np.uint8)
        lat_torch_quant = torch.from_numpy(lat_np).to(torch.float32) / 255
        pred_torch = self.decode((lat_torch_quant, pad))
        pred_np = (pred_torch*65535 + 0.5).squeeze(0).squeeze(0).numpy(force=True).astype(np.int32)
        residu = img.astype(np.int32) - pred_np
        residu = (np.abs(residu) &lt;&lt; 1) + (residu &lt; 0).astype(np.int32)
        data = pickle.dumps((lat_np, residu, pad))
        data = bz2.compress(data, compresslevel=9)
        return data

    def forward(self, img: typing.Union[torch.Tensor, np.ndarray, bytes]) -&gt; torch.Tensor:
        &#34;&#34;&#34;Encode then decode the image.

        Parameters
        ----------
        img : torch.Tensor
            The 1 channel image to encode.

        Returns
        -------
        predicted : torch.Tensor
            The predicted image of same shape.

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
        &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;)
        &gt;&gt;&gt; codec.forward(torch.rand((1000, 1000))).shape
        torch.Size([1000, 1000])
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(img, torch.Tensor), img.__class__.__name__
        assert 2 &lt;= img.ndim &lt;= 4, img.shape
        if img.ndim &lt; 4:
            return torch.squeeze(self.forward(torch.unsqueeze(img, 0)), 0)
        return self.decode(self.encode(img))

    def overfit(self, diagrams: list[Diagram]):
        &#34;&#34;&#34;Overfit the model on the given diagrams.

        Parameters
        ----------
        diagrams : list[Diagram]
            All the diagrams we want to compress.
        &#34;&#34;&#34;
        self.train()
        optim = torch.optim.RAdam(self.parameters(), lr=5e-5)
        loss = torch.nn.MSELoss()

        self.to(&#34;cuda&#34;)

        for epoch in range(100):
            tot_loss_val = 0
            for i, diagram in enumerate(tqdm(diagrams, unit=&#34;img&#34;, desc=f&#34;epoch {epoch+1}&#34;)):
                if i % 8 == 0:
                    optim.zero_grad(set_to_none=True)
                img = diagram.image.clone().to(&#34;cuda&#34;)
                pred = self.forward(img)
                loss_val = loss(pred, img)
                tot_loss_val += loss_val.item()
                loss_val.backward()
                if (i+1) % 8 == 0:
                    optim.step()
            print(tot_loss_val/len(diagrams))
        torch.save(
            {&#34;encoder&#34;: self.encoder.state_dict(), &#34;decoder&#34;: self.decoder.state_dict()},
            self._weights,
        )
        no_comp = 0
        comp = 0
        self.eval()
        self.to(&#34;cpu&#34;)
        for diagram in tqdm(diagrams):
            img = (diagram.image*65535 + 0.5).numpy(force=True).astype(np.uint16)
            no_comp += len(img.tobytes())
            comp += len(self.encode(img))
        print(f&#34;total no compressed {no_comp} bytes&#34;)
        print(f&#34;total compressed {comp} bytes&#34;)
        print(f&#34;compression factor {no_comp/comp:.2f}&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="laueimproc.io.comp_lm.Decoder"><code class="flex name class">
<span>class <span class="ident">Decoder</span></span>
</code></dt>
<dd>
<div class="desc"><p>Unfold the projected encoded images into the color space.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Decoder(torch.nn.Module):
    &#34;&#34;&#34;Unfold the projected encoded images into the color space.&#34;&#34;&#34;

    def __init__(self):
        super().__init__()

        eta = 1.32

        self.pre = torch.nn.Sequential(
            torch.nn.ConstantPad2d(2, 0.5),
            torch.nn.Conv2d(66, 64, kernel_size=1),
            torch.nn.ELU(inplace=True),
        )
        self.decoder = torch.nn.Sequential(
            *(
                torch.nn.Sequential(
                    torch.nn.ConvTranspose2d(
                        round(16*eta**layer),
                        round(16*eta**(layer-1)),
                        kernel_size=4,
                        stride=2,
                        padding=1,
                        bias=False,
                    ),
                    torch.nn.BatchNorm2d(round(16*eta**(layer-1))),
                    torch.nn.ELU(inplace=True),
                    torch.nn.Conv2d(
                        round(16*eta**(layer-1)),
                        round(16*eta**(layer-1)),
                        kernel_size=3,
                        padding=1,
                    ),
                    torch.nn.ELU(inplace=True),
                )
                for layer in range(5, 0, -1)
            ),
        )
        self.post = torch.nn.Sequential(
            torch.nn.Conv2d(16, 12, kernel_size=5, padding=2),
            torch.nn.ELU(inplace=True),
            torch.nn.Conv2d(12, 1, kernel_size=5, padding=2),
            torch.nn.Sigmoid(),
        )

    def forward(self, lat: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply the function on the latent images.

        Parameters
        ----------
        lat : torch.Tensor
            The projected image in the latent space of shape (n, 256, hl, wl).

        Returns
        -------
        img : torch.Tensor
            A close image in colorspace to the input image.
            It is as mutch bijective as possible than VariationalEncoder.
            New shape is (n, 256, 160+hl*32, 160+wl*32) with value in [0, 1].

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import Decoder
        &gt;&gt;&gt; decoder = Decoder()
        &gt;&gt;&gt; decoder(torch.rand((5, 64, 1, 3))).shape
        torch.Size([5, 1, 160, 224])
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(lat, torch.Tensor), lat.__class__.__name__
        assert lat.ndim == 4, lat.shape
        assert lat.shape[1] == 64, lat.shape
        assert lat.shape[2:] &gt;= (1, 1), lat.shape
        assert lat.dtype.is_floating_point, lat.dtype

        pos_h = torch.linspace(-1, 1, lat.shape[2], dtype=lat.dtype, device=lat.device)
        pos_w = torch.linspace(-1, 1, lat.shape[3], dtype=lat.dtype, device=lat.device)
        pos_h, pos_w = pos_h.reshape(1, 1, -1, 1), pos_w.reshape(1, 1, 1, -1)
        pos_h, pos_w = (
            pos_h.expand(len(lat), 1, *lat.shape[2:]),
            pos_w.expand(len(lat), 1, *lat.shape[2:]),
        )

        x = self.pre(torch.cat((lat, pos_h, pos_w), dim=1))
        x = self.decoder(x)
        out = self.post(x)
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.io.comp_lm.Decoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, lat: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Apply the function on the latent images.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lat</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The projected image in the latent space of shape (n, 256, hl, wl).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>A close image in colorspace to the input image.
It is as mutch bijective as possible than VariationalEncoder.
New shape is (n, 256, 160+hl<em>32, 160+wl</em>32) with value in [0, 1].</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from laueimproc.io.comp_lm import Decoder
&gt;&gt;&gt; decoder = Decoder()
&gt;&gt;&gt; decoder(torch.rand((5, 64, 1, 3))).shape
torch.Size([5, 1, 160, 224])
&gt;&gt;&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, lat: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Apply the function on the latent images.

    Parameters
    ----------
    lat : torch.Tensor
        The projected image in the latent space of shape (n, 256, hl, wl).

    Returns
    -------
    img : torch.Tensor
        A close image in colorspace to the input image.
        It is as mutch bijective as possible than VariationalEncoder.
        New shape is (n, 256, 160+hl*32, 160+wl*32) with value in [0, 1].

    Examples
    --------
    &gt;&gt;&gt; import torch
    &gt;&gt;&gt; from laueimproc.io.comp_lm import Decoder
    &gt;&gt;&gt; decoder = Decoder()
    &gt;&gt;&gt; decoder(torch.rand((5, 64, 1, 3))).shape
    torch.Size([5, 1, 160, 224])
    &gt;&gt;&gt;
    &#34;&#34;&#34;
    assert isinstance(lat, torch.Tensor), lat.__class__.__name__
    assert lat.ndim == 4, lat.shape
    assert lat.shape[1] == 64, lat.shape
    assert lat.shape[2:] &gt;= (1, 1), lat.shape
    assert lat.dtype.is_floating_point, lat.dtype

    pos_h = torch.linspace(-1, 1, lat.shape[2], dtype=lat.dtype, device=lat.device)
    pos_w = torch.linspace(-1, 1, lat.shape[3], dtype=lat.dtype, device=lat.device)
    pos_h, pos_w = pos_h.reshape(1, 1, -1, 1), pos_w.reshape(1, 1, 1, -1)
    pos_h, pos_w = (
        pos_h.expand(len(lat), 1, *lat.shape[2:]),
        pos_w.expand(len(lat), 1, *lat.shape[2:]),
    )

    x = self.pre(torch.cat((lat, pos_h, pos_w), dim=1))
    x = self.decoder(x)
    out = self.post(x)
    return out</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="laueimproc.io.comp_lm.LMCodec"><code class="flex name class">
<span>class <span class="ident">LMCodec</span></span>
<span>(</span><span>weights: Union[str, pathlib.Path])</span>
</code></dt>
<dd>
<div class="desc"><p>Encode and Decode Laue Max images.</p>
<p>Initialise the codec.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>weights</code></strong> :&ensp;<code>pathlike</code></dt>
<dd>The filename of the model data.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LMCodec(torch.nn.Module):
    &#34;&#34;&#34;Encode and Decode Laue Max images.&#34;&#34;&#34;

    def __init__(self, weights: typing.Union[str, pathlib.Path]):
        &#34;&#34;&#34;Initialise the codec.

        Parameters
        ----------
        weights : pathlike
            The filename of the model data.
        &#34;&#34;&#34;
        assert isinstance(weights, (str, pathlib.Path)), weights.__class__.__name__
        weights = pathlib.Path(weights).expanduser().resolve()
        assert not weights.is_dir(), weights

        super().__init__()
        self.encoder = VariationalEncoder()
        self.decoder = Decoder()
        self._weights = weights

        if weights.is_file():
            checkpoint = torch.load(weights)
            self.encoder.load_state_dict(checkpoint[&#34;encoder&#34;])
            self.decoder.load_state_dict(checkpoint[&#34;decoder&#34;])
        else:
            torch.save(
                {&#34;encoder&#34;: self.encoder.state_dict(), &#34;decoder&#34;: self.decoder.state_dict()},
                weights,
            )

    def decode(
        self, data: typing.Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]
    ) -&gt; typing.Union[torch.Tensor, np.ndarray]:
        &#34;&#34;&#34;Decode le compressed content.

        Parameters
        ----------
        data
            The compact data representation.

        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
        &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;).eval()
        &gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
        &gt;&gt;&gt; data = codec.encode(img)
        &gt;&gt;&gt; decoded = codec.decode(data)
        &gt;&gt;&gt; (img == decoded).all()
        True
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        if isinstance(data, tuple):
            assert len(data) == 2, len(data)
            encoded, pad = data
            assert isinstance(encoded, torch.Tensor), encoded.__class__.__name__
            assert encoded.ndim == 4, encoded.shape
            assert encoded.shape[1] == 64, encoded.shape
            assert len(pad) == 4, len(pad)
            assert all(isinstance(p, int) for p in pad), pad
            padleft, padright, padtop, padbottom = pad
            img = self.decoder(encoded)
            img = img[:, :, padtop:img.shape[2]-padbottom, padleft:img.shape[3]-padright]
            return img

        assert isinstance(data, bytes), data.__class__.__name__
        data = bz2.decompress(data)
        lat_np, residu, (padleft, padright, padtop, padbottom) = pickle.loads(data)
        residu = (residu &gt;&gt; 1) * np.where(residu % 2, -1, 1)
        lat_torch = torch.from_numpy(lat_np).to(torch.float32) / 255
        pred_torch = self.decoder(lat_torch)
        pred_torch = (
            pred_torch
            [:, :, padtop:pred_torch.shape[2]-padbottom, padleft:pred_torch.shape[3]-padright]
        )
        pred_np = (pred_torch*65535 + 0.5).squeeze(0).squeeze(0).numpy(force=True).astype(np.int32)
        img_np = (residu + pred_np).astype(np.uint16)
        return img_np

    def encode(
        self, img: typing.Union[torch.Tensor, np.ndarray]
    ) -&gt; typing.Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]:
        &#34;&#34;&#34;Encode the image.

        Parameters
        ----------
        img : torch.Tensor or np.ndarray
            The 1 channel image to encode.

        Examples
        --------
        &gt;&gt;&gt; import numpy as np
        &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
        &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;).eval()
        &gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
        &gt;&gt;&gt; encoded = codec.encode(img)
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        if isinstance(img, torch.Tensor):
            assert img.ndim == 4, img.shape
            assert img.shape[1] == 1, img.shape

            # padding
            padtop = max(0, 128-img.shape[2])
            padtop += 32 - ((img.shape[2] + padtop) % 32)
            padleft = max(0, 1280-img.shape[3])
            padleft += 32 - ((img.shape[3] + padleft) % 32)
            padtop, padbottom = padtop//2, padtop - padtop//2
            padleft, padright = padleft//2, padleft - padleft//2
            img = torch.nn.functional.pad(img, (padleft, padright, padtop, padbottom))

            # transform
            return self.encoder(img), (padleft, padright, padtop, padbottom)

        # case numpy uint16
        assert isinstance(img, np.ndarray), img.__class__.__name__
        assert img.ndim == 2, img.shape
        assert img.dtype == np.uint16
        img_torch = torch.from_numpy(img.astype(np.float32)).unsqueeze(0).unsqueeze(0) / 65535
        lat_torch, pad = self.encode(img_torch)
        lat_np = (lat_torch*255 + 0.5).numpy(force=True).astype(np.uint8)
        lat_torch_quant = torch.from_numpy(lat_np).to(torch.float32) / 255
        pred_torch = self.decode((lat_torch_quant, pad))
        pred_np = (pred_torch*65535 + 0.5).squeeze(0).squeeze(0).numpy(force=True).astype(np.int32)
        residu = img.astype(np.int32) - pred_np
        residu = (np.abs(residu) &lt;&lt; 1) + (residu &lt; 0).astype(np.int32)
        data = pickle.dumps((lat_np, residu, pad))
        data = bz2.compress(data, compresslevel=9)
        return data

    def forward(self, img: typing.Union[torch.Tensor, np.ndarray, bytes]) -&gt; torch.Tensor:
        &#34;&#34;&#34;Encode then decode the image.

        Parameters
        ----------
        img : torch.Tensor
            The 1 channel image to encode.

        Returns
        -------
        predicted : torch.Tensor
            The predicted image of same shape.

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
        &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;)
        &gt;&gt;&gt; codec.forward(torch.rand((1000, 1000))).shape
        torch.Size([1000, 1000])
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(img, torch.Tensor), img.__class__.__name__
        assert 2 &lt;= img.ndim &lt;= 4, img.shape
        if img.ndim &lt; 4:
            return torch.squeeze(self.forward(torch.unsqueeze(img, 0)), 0)
        return self.decode(self.encode(img))

    def overfit(self, diagrams: list[Diagram]):
        &#34;&#34;&#34;Overfit the model on the given diagrams.

        Parameters
        ----------
        diagrams : list[Diagram]
            All the diagrams we want to compress.
        &#34;&#34;&#34;
        self.train()
        optim = torch.optim.RAdam(self.parameters(), lr=5e-5)
        loss = torch.nn.MSELoss()

        self.to(&#34;cuda&#34;)

        for epoch in range(100):
            tot_loss_val = 0
            for i, diagram in enumerate(tqdm(diagrams, unit=&#34;img&#34;, desc=f&#34;epoch {epoch+1}&#34;)):
                if i % 8 == 0:
                    optim.zero_grad(set_to_none=True)
                img = diagram.image.clone().to(&#34;cuda&#34;)
                pred = self.forward(img)
                loss_val = loss(pred, img)
                tot_loss_val += loss_val.item()
                loss_val.backward()
                if (i+1) % 8 == 0:
                    optim.step()
            print(tot_loss_val/len(diagrams))
        torch.save(
            {&#34;encoder&#34;: self.encoder.state_dict(), &#34;decoder&#34;: self.decoder.state_dict()},
            self._weights,
        )
        no_comp = 0
        comp = 0
        self.eval()
        self.to(&#34;cpu&#34;)
        for diagram in tqdm(diagrams):
            img = (diagram.image*65535 + 0.5).numpy(force=True).astype(np.uint16)
            no_comp += len(img.tobytes())
            comp += len(self.encode(img))
        print(f&#34;total no compressed {no_comp} bytes&#34;)
        print(f&#34;total compressed {comp} bytes&#34;)
        print(f&#34;compression factor {no_comp/comp:.2f}&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.io.comp_lm.LMCodec.decode"><code class="name flex">
<span>def <span class="ident">decode</span></span>(<span>self, data: Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]) ‑> Union[torch.Tensor, numpy.ndarray]</span>
</code></dt>
<dd>
<div class="desc"><p>Decode le compressed content.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>data</code></strong></dt>
<dd>The compact data representation.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
&gt;&gt;&gt; codec = LMCodec(&quot;/tmp/lmweights.tar&quot;).eval()
&gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
&gt;&gt;&gt; data = codec.encode(img)
&gt;&gt;&gt; decoded = codec.decode(data)
&gt;&gt;&gt; (img == decoded).all()
True
&gt;&gt;&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decode(
    self, data: typing.Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]
) -&gt; typing.Union[torch.Tensor, np.ndarray]:
    &#34;&#34;&#34;Decode le compressed content.

    Parameters
    ----------
    data
        The compact data representation.

    Examples
    --------
    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
    &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;).eval()
    &gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
    &gt;&gt;&gt; data = codec.encode(img)
    &gt;&gt;&gt; decoded = codec.decode(data)
    &gt;&gt;&gt; (img == decoded).all()
    True
    &gt;&gt;&gt;
    &#34;&#34;&#34;
    if isinstance(data, tuple):
        assert len(data) == 2, len(data)
        encoded, pad = data
        assert isinstance(encoded, torch.Tensor), encoded.__class__.__name__
        assert encoded.ndim == 4, encoded.shape
        assert encoded.shape[1] == 64, encoded.shape
        assert len(pad) == 4, len(pad)
        assert all(isinstance(p, int) for p in pad), pad
        padleft, padright, padtop, padbottom = pad
        img = self.decoder(encoded)
        img = img[:, :, padtop:img.shape[2]-padbottom, padleft:img.shape[3]-padright]
        return img

    assert isinstance(data, bytes), data.__class__.__name__
    data = bz2.decompress(data)
    lat_np, residu, (padleft, padright, padtop, padbottom) = pickle.loads(data)
    residu = (residu &gt;&gt; 1) * np.where(residu % 2, -1, 1)
    lat_torch = torch.from_numpy(lat_np).to(torch.float32) / 255
    pred_torch = self.decoder(lat_torch)
    pred_torch = (
        pred_torch
        [:, :, padtop:pred_torch.shape[2]-padbottom, padleft:pred_torch.shape[3]-padright]
    )
    pred_np = (pred_torch*65535 + 0.5).squeeze(0).squeeze(0).numpy(force=True).astype(np.int32)
    img_np = (residu + pred_np).astype(np.uint16)
    return img_np</code></pre>
</details>
</dd>
<dt id="laueimproc.io.comp_lm.LMCodec.encode"><code class="name flex">
<span>def <span class="ident">encode</span></span>(<span>self, img: Union[torch.Tensor, numpy.ndarray]) ‑> Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]</span>
</code></dt>
<dd>
<div class="desc"><p>Encode the image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>torch.Tensor</code> or <code>np.ndarray</code></dt>
<dd>The 1 channel image to encode.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import numpy as np
&gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
&gt;&gt;&gt; codec = LMCodec(&quot;/tmp/lmweights.tar&quot;).eval()
&gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
&gt;&gt;&gt; encoded = codec.encode(img)
&gt;&gt;&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def encode(
    self, img: typing.Union[torch.Tensor, np.ndarray]
) -&gt; typing.Union[tuple[torch.Tensor, tuple[int, int, int, int]], bytes]:
    &#34;&#34;&#34;Encode the image.

    Parameters
    ----------
    img : torch.Tensor or np.ndarray
        The 1 channel image to encode.

    Examples
    --------
    &gt;&gt;&gt; import numpy as np
    &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
    &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;).eval()
    &gt;&gt;&gt; img = np.random.randint(0, 65536, (1000, 1000), dtype=np.uint16)
    &gt;&gt;&gt; encoded = codec.encode(img)
    &gt;&gt;&gt;
    &#34;&#34;&#34;
    if isinstance(img, torch.Tensor):
        assert img.ndim == 4, img.shape
        assert img.shape[1] == 1, img.shape

        # padding
        padtop = max(0, 128-img.shape[2])
        padtop += 32 - ((img.shape[2] + padtop) % 32)
        padleft = max(0, 1280-img.shape[3])
        padleft += 32 - ((img.shape[3] + padleft) % 32)
        padtop, padbottom = padtop//2, padtop - padtop//2
        padleft, padright = padleft//2, padleft - padleft//2
        img = torch.nn.functional.pad(img, (padleft, padright, padtop, padbottom))

        # transform
        return self.encoder(img), (padleft, padright, padtop, padbottom)

    # case numpy uint16
    assert isinstance(img, np.ndarray), img.__class__.__name__
    assert img.ndim == 2, img.shape
    assert img.dtype == np.uint16
    img_torch = torch.from_numpy(img.astype(np.float32)).unsqueeze(0).unsqueeze(0) / 65535
    lat_torch, pad = self.encode(img_torch)
    lat_np = (lat_torch*255 + 0.5).numpy(force=True).astype(np.uint8)
    lat_torch_quant = torch.from_numpy(lat_np).to(torch.float32) / 255
    pred_torch = self.decode((lat_torch_quant, pad))
    pred_np = (pred_torch*65535 + 0.5).squeeze(0).squeeze(0).numpy(force=True).astype(np.int32)
    residu = img.astype(np.int32) - pred_np
    residu = (np.abs(residu) &lt;&lt; 1) + (residu &lt; 0).astype(np.int32)
    data = pickle.dumps((lat_np, residu, pad))
    data = bz2.compress(data, compresslevel=9)
    return data</code></pre>
</details>
</dd>
<dt id="laueimproc.io.comp_lm.LMCodec.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, img: Union[torch.Tensor, numpy.ndarray, bytes]) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Encode then decode the image.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The 1 channel image to encode.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>predicted</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The predicted image of same shape.</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
&gt;&gt;&gt; codec = LMCodec(&quot;/tmp/lmweights.tar&quot;)
&gt;&gt;&gt; codec.forward(torch.rand((1000, 1000))).shape
torch.Size([1000, 1000])
&gt;&gt;&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, img: typing.Union[torch.Tensor, np.ndarray, bytes]) -&gt; torch.Tensor:
    &#34;&#34;&#34;Encode then decode the image.

    Parameters
    ----------
    img : torch.Tensor
        The 1 channel image to encode.

    Returns
    -------
    predicted : torch.Tensor
        The predicted image of same shape.

    Examples
    --------
    &gt;&gt;&gt; import torch
    &gt;&gt;&gt; from laueimproc.io.comp_lm import LMCodec
    &gt;&gt;&gt; codec = LMCodec(&#34;/tmp/lmweights.tar&#34;)
    &gt;&gt;&gt; codec.forward(torch.rand((1000, 1000))).shape
    torch.Size([1000, 1000])
    &gt;&gt;&gt;
    &#34;&#34;&#34;
    assert isinstance(img, torch.Tensor), img.__class__.__name__
    assert 2 &lt;= img.ndim &lt;= 4, img.shape
    if img.ndim &lt; 4:
        return torch.squeeze(self.forward(torch.unsqueeze(img, 0)), 0)
    return self.decode(self.encode(img))</code></pre>
</details>
</dd>
<dt id="laueimproc.io.comp_lm.LMCodec.overfit"><code class="name flex">
<span>def <span class="ident">overfit</span></span>(<span>self, diagrams: list[<a title="laueimproc.classes.diagram.Diagram" href="../classes/diagram.html#laueimproc.classes.diagram.Diagram">Diagram</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>Overfit the model on the given diagrams.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>diagrams</code></strong> :&ensp;<code>list[Diagram]</code></dt>
<dd>All the diagrams we want to compress.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def overfit(self, diagrams: list[Diagram]):
    &#34;&#34;&#34;Overfit the model on the given diagrams.

    Parameters
    ----------
    diagrams : list[Diagram]
        All the diagrams we want to compress.
    &#34;&#34;&#34;
    self.train()
    optim = torch.optim.RAdam(self.parameters(), lr=5e-5)
    loss = torch.nn.MSELoss()

    self.to(&#34;cuda&#34;)

    for epoch in range(100):
        tot_loss_val = 0
        for i, diagram in enumerate(tqdm(diagrams, unit=&#34;img&#34;, desc=f&#34;epoch {epoch+1}&#34;)):
            if i % 8 == 0:
                optim.zero_grad(set_to_none=True)
            img = diagram.image.clone().to(&#34;cuda&#34;)
            pred = self.forward(img)
            loss_val = loss(pred, img)
            tot_loss_val += loss_val.item()
            loss_val.backward()
            if (i+1) % 8 == 0:
                optim.step()
        print(tot_loss_val/len(diagrams))
    torch.save(
        {&#34;encoder&#34;: self.encoder.state_dict(), &#34;decoder&#34;: self.decoder.state_dict()},
        self._weights,
    )
    no_comp = 0
    comp = 0
    self.eval()
    self.to(&#34;cpu&#34;)
    for diagram in tqdm(diagrams):
        img = (diagram.image*65535 + 0.5).numpy(force=True).astype(np.uint16)
        no_comp += len(img.tobytes())
        comp += len(self.encode(img))
    print(f&#34;total no compressed {no_comp} bytes&#34;)
    print(f&#34;total compressed {comp} bytes&#34;)
    print(f&#34;compression factor {no_comp/comp:.2f}&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="laueimproc.io.comp_lm.VariationalEncoder"><code class="flex name class">
<span>class <span class="ident">VariationalEncoder</span></span>
</code></dt>
<dd>
<div class="desc"><p>Projects images into a more compact space.</p>
<p>Each patch of 320x320 pixels with a stride of 64 pixels
is projected into a space of dimension 64, quantizable with 8 bits per component.</p>
<p>Initialize internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class VariationalEncoder(torch.nn.Module):
    &#34;&#34;&#34;Projects images into a more compact space.

    Each patch of 320x320 pixels with a stride of 64 pixels
    is projected into a space of dimension 64, quantizable with 8 bits per component.
    &#34;&#34;&#34;

    def __init__(self):
        super().__init__()

        eta = 1.32  # (lat_dim/first_dim)**(1/nb_layers)

        self.pre = torch.nn.Sequential(
            torch.nn.Conv2d(1, 16, kernel_size=3, stride=1),
            torch.nn.ELU(),
        )
        self.encoder = torch.nn.Sequential(
            *(
                torch.nn.Sequential(
                    torch.nn.Conv2d(
                        round(16*eta**layer),
                        round(16*eta**(layer+1)),
                        kernel_size=6,
                        stride=2,
                        padding=2,
                        bias=False,
                    ),
                    torch.nn.BatchNorm2d(round(16*eta**(layer+1))),
                    torch.nn.ELU(),
                    torch.nn.Conv2d(
                        round(16*eta**(layer+1)),
                        round(16*eta**(layer+1)),
                        kernel_size=3,
                    ),
                    torch.nn.ELU(),
                )
                for layer in range(5)
            ),
        )
        self.post = torch.nn.Sequential(
            torch.nn.Conv2d(65, 64, kernel_size=3, padding=1),
            torch.nn.Sigmoid(),
        )

    def forward(self, img: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Apply the function on the images.

        Parameters
        ----------
        img : torch.Tensor
            The float image batch of shape (n, 1, h, w).
            With h and w &gt;= 160 + k*32, k &gt;= 0 integer.

        Returns
        -------
        lat : torch.Tensor
            The projection of the image in the latent space.
            New shape is (n, 256, h/32-4, w/32-4) with value in [0, 1].

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
        &gt;&gt;&gt; encoder = VariationalEncoder()
        &gt;&gt;&gt; encoder(torch.rand((5, 1, 160, 224))).shape
        torch.Size([5, 64, 1, 3])
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(img, torch.Tensor), img.__class__.__name__
        assert img.ndim == 4, img.shape
        assert img.shape[1] == 1, img.shape
        assert img.shape[2:] &gt;= (160, 160), img.shape
        assert img.shape[2] % 32 == 0, img.shape
        assert img.shape[3] % 32 == 0, img.shape
        assert img.dtype.is_floating_point, img.dtype

        mean = (
            torch.mean(img, dim=(2, 3), keepdim=True)
            .expand(-1, 1, img.shape[2]//32-4, img.shape[3]//32-4)
        )
        x = self.pre(img)
        x = self.encoder(x)
        lat = self.post(torch.cat((x, mean), dim=1))
        if self.training:
            lat = self.add_quantization_noise(lat)
        return lat

    @staticmethod
    def add_quantization_noise(lat: torch.Tensor) -&gt; torch.Tensor:
        &#34;&#34;&#34;Add a uniform noise in order to simulate the quantization into uint8.

        Parameters
        ----------
        lat : torch.Tensor
            The float lattent space of shape (n, 64, a, b) with value in range ]0, 1[.

        Returns
        -------
        noised_lat : torch.Tensor
            The input tensor with a aditive uniform noise U(-.5/255, .5/255).
            The finals values are clamped to stay in the range [0, 1].

        Examples
        --------
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
        &gt;&gt;&gt; lat = torch.rand((5, 64, 1, 3))
        &gt;&gt;&gt; q_lat = VariationalEncoder.add_quantization_noise(lat)
        &gt;&gt;&gt; torch.all(abs(q_lat - lat) &lt;= 0.5/255)
        tensor(True)
        &gt;&gt;&gt; abs((q_lat - lat).mean().round(decimals=3))
        tensor(0.)
        &gt;&gt;&gt;
        &#34;&#34;&#34;
        assert isinstance(lat, torch.Tensor), lat.__class__.__name__
        assert lat.ndim == 4, lat.shape
        assert lat.shape[1] == 64, lat.shape
        assert lat.dtype.is_floating_point, lat.dtype

        noise = torch.rand_like(lat)/255
        noise -= 0.5/255
        out = lat + noise
        out = torch.clamp(out, min=0, max=1)
        return out</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="laueimproc.io.comp_lm.VariationalEncoder.add_quantization_noise"><code class="name flex">
<span>def <span class="ident">add_quantization_noise</span></span>(<span>lat: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Add a uniform noise in order to simulate the quantization into uint8.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lat</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The float lattent space of shape (n, 64, a, b) with value in range ]0, 1[.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>noised_lat</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The input tensor with a aditive uniform noise U(-.5/255, .5/255).
The finals values are clamped to stay in the range [0, 1].</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
&gt;&gt;&gt; lat = torch.rand((5, 64, 1, 3))
&gt;&gt;&gt; q_lat = VariationalEncoder.add_quantization_noise(lat)
&gt;&gt;&gt; torch.all(abs(q_lat - lat) &lt;= 0.5/255)
tensor(True)
&gt;&gt;&gt; abs((q_lat - lat).mean().round(decimals=3))
tensor(0.)
&gt;&gt;&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def add_quantization_noise(lat: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Add a uniform noise in order to simulate the quantization into uint8.

    Parameters
    ----------
    lat : torch.Tensor
        The float lattent space of shape (n, 64, a, b) with value in range ]0, 1[.

    Returns
    -------
    noised_lat : torch.Tensor
        The input tensor with a aditive uniform noise U(-.5/255, .5/255).
        The finals values are clamped to stay in the range [0, 1].

    Examples
    --------
    &gt;&gt;&gt; import torch
    &gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
    &gt;&gt;&gt; lat = torch.rand((5, 64, 1, 3))
    &gt;&gt;&gt; q_lat = VariationalEncoder.add_quantization_noise(lat)
    &gt;&gt;&gt; torch.all(abs(q_lat - lat) &lt;= 0.5/255)
    tensor(True)
    &gt;&gt;&gt; abs((q_lat - lat).mean().round(decimals=3))
    tensor(0.)
    &gt;&gt;&gt;
    &#34;&#34;&#34;
    assert isinstance(lat, torch.Tensor), lat.__class__.__name__
    assert lat.ndim == 4, lat.shape
    assert lat.shape[1] == 64, lat.shape
    assert lat.dtype.is_floating_point, lat.dtype

    noise = torch.rand_like(lat)/255
    noise -= 0.5/255
    out = lat + noise
    out = torch.clamp(out, min=0, max=1)
    return out</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="laueimproc.io.comp_lm.VariationalEncoder.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, img: torch.Tensor) ‑> torch.Tensor</span>
</code></dt>
<dd>
<div class="desc"><p>Apply the function on the images.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>img</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The float image batch of shape (n, 1, h, w).
With h and w &gt;= 160 + k*32, k &gt;= 0 integer.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>lat</code></strong> :&ensp;<code>torch.Tensor</code></dt>
<dd>The projection of the image in the latent space.
New shape is (n, 256, h/32-4, w/32-4) with value in [0, 1].</dd>
</dl>
<h2 id="examples">Examples</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; import torch
&gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
&gt;&gt;&gt; encoder = VariationalEncoder()
&gt;&gt;&gt; encoder(torch.rand((5, 1, 160, 224))).shape
torch.Size([5, 64, 1, 3])
&gt;&gt;&gt;
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, img: torch.Tensor) -&gt; torch.Tensor:
    &#34;&#34;&#34;Apply the function on the images.

    Parameters
    ----------
    img : torch.Tensor
        The float image batch of shape (n, 1, h, w).
        With h and w &gt;= 160 + k*32, k &gt;= 0 integer.

    Returns
    -------
    lat : torch.Tensor
        The projection of the image in the latent space.
        New shape is (n, 256, h/32-4, w/32-4) with value in [0, 1].

    Examples
    --------
    &gt;&gt;&gt; import torch
    &gt;&gt;&gt; from laueimproc.io.comp_lm import VariationalEncoder
    &gt;&gt;&gt; encoder = VariationalEncoder()
    &gt;&gt;&gt; encoder(torch.rand((5, 1, 160, 224))).shape
    torch.Size([5, 64, 1, 3])
    &gt;&gt;&gt;
    &#34;&#34;&#34;
    assert isinstance(img, torch.Tensor), img.__class__.__name__
    assert img.ndim == 4, img.shape
    assert img.shape[1] == 1, img.shape
    assert img.shape[2:] &gt;= (160, 160), img.shape
    assert img.shape[2] % 32 == 0, img.shape
    assert img.shape[3] % 32 == 0, img.shape
    assert img.dtype.is_floating_point, img.dtype

    mean = (
        torch.mean(img, dim=(2, 3), keepdim=True)
        .expand(-1, 1, img.shape[2]//32-4, img.shape[3]//32-4)
    )
    x = self.pre(img)
    x = self.encoder(x)
    lat = self.post(torch.cat((x, mean), dim=1))
    if self.training:
        lat = self.add_quantization_noise(lat)
    return lat</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="laueimproc.io" href="index.html">laueimproc.io</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="laueimproc.io.comp_lm.Decoder" href="#laueimproc.io.comp_lm.Decoder">Decoder</a></code></h4>
<ul class="">
<li><code><a title="laueimproc.io.comp_lm.Decoder.forward" href="#laueimproc.io.comp_lm.Decoder.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laueimproc.io.comp_lm.LMCodec" href="#laueimproc.io.comp_lm.LMCodec">LMCodec</a></code></h4>
<ul class="">
<li><code><a title="laueimproc.io.comp_lm.LMCodec.decode" href="#laueimproc.io.comp_lm.LMCodec.decode">decode</a></code></li>
<li><code><a title="laueimproc.io.comp_lm.LMCodec.encode" href="#laueimproc.io.comp_lm.LMCodec.encode">encode</a></code></li>
<li><code><a title="laueimproc.io.comp_lm.LMCodec.forward" href="#laueimproc.io.comp_lm.LMCodec.forward">forward</a></code></li>
<li><code><a title="laueimproc.io.comp_lm.LMCodec.overfit" href="#laueimproc.io.comp_lm.LMCodec.overfit">overfit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="laueimproc.io.comp_lm.VariationalEncoder" href="#laueimproc.io.comp_lm.VariationalEncoder">VariationalEncoder</a></code></h4>
<ul class="">
<li><code><a title="laueimproc.io.comp_lm.VariationalEncoder.add_quantization_noise" href="#laueimproc.io.comp_lm.VariationalEncoder.add_quantization_noise">add_quantization_noise</a></code></li>
<li><code><a title="laueimproc.io.comp_lm.VariationalEncoder.forward" href="#laueimproc.io.comp_lm.VariationalEncoder.forward">forward</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>